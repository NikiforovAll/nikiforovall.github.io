[
  
    {
      "id": "0",
      "title": "Building RAG with .NET AI Building Blocks - Microsoft.Extensions.AI",
      "url": "/dotnet/ai/2026/02/28/building-rag-with-dotnet-ai-building-blocks.html",
      "date": "February 28, 2026",
      "categories": ["dotnet","ai"],
      "tags": ["dotnet","aspire","meai","rag","qdrant","ollama","vector-data","ai-evaluation"],
      "shortinfo": "How to build a provider-agnostic RAG system in .NET using Microsoft.Extensions.AI, VectorData, and Aspire for orchestration, testing, and evaluation.",
      "content": "TL;DRI built a RAG-based document intelligence assistant in .NET using IChatClient and IEmbeddingGenerator from Microsoft.Extensions.AI, VectorStoreCollection from Microsoft.Extensions.VectorData, Qdrant for vector search, Ollama for local inference, and .NET Aspire to wire it all together. The result is a provider-agnostic RAG app where switching from Ollama to OpenAI means changing one line of DI registration. This post walks through the abstractions, the pipeline, and how Aspire enables LLM-as-Judge evaluation in integration tests.  TL;DR  Introduction  The building blocks          IChatClient and IEmbeddingGenerator      DelegatingChatClient: middleware for AI      VectorData with auto-embedding        The RAG pipeline          Ingestion      Query        Aspire orchestration  RAG evaluation with LLM-as-Judge  Demo  Summary  ReferencesSource code: https://github.com/NikiforovAll/company-intel-dotnetIntroductionMost AI integrations are tightly coupled to a specific provider. Your code imports the OpenAI SDK, or the Ollama client, or the Azure AI library. Switching providers means rewriting application code, not just configuration.Microsoft.Extensions.AI (MEAI) gives you IChatClient and IEmbeddingGenerator. You register an implementation at startup. The rest of your code uses the abstraction. Change one line in DI. Your RAG pipeline, your agent, your evaluation tests ‚Äî none of them care.Microsoft.Extensions.VectorData does the same for vector stores. Define your data model with attributes, and the framework handles embedding and search.This post walks through building a complete RAG application using these abstractions, with Aspire for orchestration and LLM-as-Judge for quality evaluation.The building blocks.NET has four AI building blocks. This post focuses on the first two.            Block      Package      What it gives you                  MEAI      Microsoft.Extensions.AI      IChatClient, IEmbeddingGenerator ‚Äî provider-agnostic interfaces              VectorData      Microsoft.Extensions.VectorData      VectorStoreCollection, attribute-driven data models, auto-embedding              Agent Framework      Microsoft.Agents.AI      IAIAgent, AG-UI protocol for streaming to frontends      IChatClient and IEmbeddingGeneratorHere‚Äôs the entire AI registration for the app:builder.AddOllamaApiClient(\"ollama-llama3-1\").AddChatClient();builder.AddOllamaApiClient(\"ollama-all-minilm\").AddEmbeddingGenerator();After this, any service can inject IChatClient or IEmbeddingGenerator&lt;string, Embedding&lt;float&gt;&gt; and use them without knowing what‚Äôs behind them.The chat client usage looks like this:IChatClient chatClient = sp.GetRequiredService&lt;IChatClient&gt;();var response = await chatClient.GetResponseAsync(messages);The same applies to embeddings:var embedder = sp.GetRequiredService&lt;IEmbeddingGenerator&lt;string, Embedding&lt;float&gt;&gt;&gt;();var embeddings = await embedder.GenerateAsync([\"some text\"]);384-dimensional vectors from all-minilm, but your code doesn‚Äôt know or care about dimensions.DelegatingChatClient: middleware for AIMEAI borrows the middleware pattern from ASP.NET Core. DelegatingChatClient wraps an IChatClient and intercepts calls. You can add logging, caching, telemetry, retry logic ‚Äî all as composable layers.Here‚Äôs a real example from the project. OllamaSharp doesn‚Äôt set MessageId on streaming updates, but CopilotKit‚Äôs Zod schema rejects null values. The fix is a thin wrapper:internal sealed class EnsureMessageIdChatClient(IChatClient inner) : DelegatingChatClient(inner){    public override async IAsyncEnumerable&lt;ChatResponseUpdate&gt; GetStreamingResponseAsync(        IEnumerable&lt;ChatMessage&gt; messages,        ChatOptions? options = null,        CancellationToken cancellationToken = default)    {        string? messageId = null;        await foreach (var update in base.GetStreamingResponseAsync(            messages, options, cancellationToken))        {            if (update.MessageId is null)            {                messageId ??= Guid.NewGuid().ToString(\"N\");                update.MessageId = messageId;            }            yield return update;        }    }}This is a compatibility shim, but the pattern is general. MEAI ships with built-in delegating clients for logging, OpenTelemetry, caching, and automatic function invocation. You compose them in DI like middleware:builder.AddOllamaApiClient(\"ollama-llama3-1\")    .AddChatClient(pipeline =&gt; pipeline        .UseLogging()        .UseOpenTelemetry()        .UseFunctionInvocation());Each .Use*() call wraps the client in another layer. The outermost layer runs first, just like ASP.NET middleware.VectorData with auto-embeddingThis is the data model for chunks stored in Qdrant:public sealed class DocumentRecord{    [VectorStoreKey]    public Guid Id { get; set; } = Guid.NewGuid();    [VectorStoreData]    public string Text { get; set; } = \"\";    [VectorStoreData]    public string FileName { get; set; } = \"\";    [VectorStoreData]    public int ChunkIndex { get; set; }    [VectorStoreData(IsIndexed = true)]    public string Source { get; set; } = \"\";    [VectorStoreVector(Dimensions: 384)]    public string Embedding =&gt; Text;}The interesting part is the last property. [VectorStoreVector(Dimensions: 384)] on a string property means ‚Äúembed this text automatically when upserting.‚Äù The Embedding property points to Text, so VectorData calls your registered IEmbeddingGenerator, gets the 384-dim vector, and stores it in Qdrant. No manual embedding calls anywhere in the ingestion code.Registration ties it together:builder.Services.AddSingleton&lt;VectorStoreCollection&lt;Guid, DocumentRecord&gt;&gt;(sp =&gt;{    var embeddingGenerator = sp.GetRequiredService&lt;IEmbeddingGenerator&lt;string, Embedding&lt;float&gt;&gt;&gt;();    var vectorStore = new QdrantVectorStore(        sp.GetRequiredService&lt;QdrantClient&gt;(),        ownsClient: false,        new QdrantVectorStoreOptions { EmbeddingGenerator = embeddingGenerator }    );    return vectorStore.GetCollection&lt;Guid, DocumentRecord&gt;(CollectionName);});Pass the embedding generator to the vector store options. From that point, every UpsertAsync call automatically embeds text before storage. Every SearchAsync call automatically embeds the query before searching.The RAG pipelineTwo phases:Phase 1: INGEST                      Phase 2: QUERY‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄUpload PDF                           \"What does the report say about X?\"  ‚Üí Extract text (PdfPig)              ‚Üí Embed query (all-minilm)  ‚Üí Chunk (recursive, 128 tokens)      ‚Üí Vector search (Qdrant, top-5)  ‚Üí Auto-embed + upsert (Qdrant)       ‚Üí LLM generates grounded answerIngestionThe ingestion service is straightforward:public async Task&lt;IngestionRecord&gt; IngestPdfAsync(    Stream pdfStream, string fileName, string source,    long fileSizeBytes, CancellationToken ct = default){    await collection.EnsureCollectionExistsAsync(ct);    var extraction = PdfTextExtractor.Extract(pdfStream);    var chunks = TextChunker.Chunk(extraction.Text);    var records = chunks        .Select((chunk, index) =&gt; new DocumentRecord        {            Text = chunk,            FileName = fileName,            ChunkIndex = index,            Source = source,        })        .ToList();    await collection.UpsertAsync(records, ct);}Notice what‚Äôs missing: no embedding calls. collection.UpsertAsync handles it because we configured auto-embedding on the DocumentRecord. PdfPig extracts text (pure .NET, no native dependencies), TextChunker splits it into chunks, and the vector store takes care of the rest.The chunker is a recursive splitter targeting 128 tokens (~512 chars) with 25-token overlap. It tries paragraph boundaries first (\\n\\n), then lines (\\n), then sentences (. ), then words.QueryThe search adapter creates a function that retrieves relevant chunks:public static Func&lt;string, CancellationToken, Task&lt;IEnumerable&lt;TextSearchResult&gt;&gt;&gt; Create(    VectorStoreCollection&lt;Guid, DocumentRecord&gt; collection, int top = 5) =&gt;    async (query, ct) =&gt;    {        var results = new List&lt;TextSearchResult&gt;();        await foreach (var result in collection.SearchAsync(            query, top: top, cancellationToken: ct))        {            var sourceName = FormatSourceName(                result.Record.FileName, result.Record.Text);            results.Add(new TextSearchResult            {                Text = result.Record.Text,                SourceName = sourceName,                SourceLink = result.Record.Source,            });        }        return results;    };collection.SearchAsync takes a plain string query. The vector store auto-embeds it, runs similarity search against Qdrant, and returns matching records. Page numbers are extracted from chunk text via regex and formatted into the source name (e.g., ‚Äúreport.pdf (Page 5)‚Äù).The agent then passes these chunks as context to the LLM with instructions to answer only from the provided context and cite sources.graph LR    UI[\"Next.js(CopilotKit)\"] --&gt;|AG-UI SSE| API[\"ASP.NET Core(MEAI Agent)\"]    API --&gt;|embed query| Ollama_E[\"Ollamaall-minilm\"]    API --&gt;|vector search| Qdrant[\"Qdrant\"]    API --&gt;|generate answer| Ollama_L[\"OllamaLlama 3.1\"]    Qdrant --&gt;|top-5 chunks| API    Ollama_E --&gt;|384-dim vector| APIAspire orchestrationHere‚Äôs the AppHost. This defines the entire system:var builder = DistributedApplication.CreateBuilder(args);var ollama = builder    .AddOllama(\"ollama\")    .WithImageTag(\"latest\")    .WithDataVolume()    .WithLifetime(ContainerLifetime.Persistent);var llama = ollama.AddModel(\"llama3.1\");var embedModel = ollama.AddModel(\"all-minilm\");var qdrant = builder    .AddQdrant(\"qdrant\", apiKey: builder.AddParameter(\"qdrant-apikey\", \"localdev\"))    .WithImageTag(\"latest\")    .WithDataVolume()    .WithLifetime(ContainerLifetime.Persistent);var sqlite = builder    .AddSqlite(\"ingestion-db\", databasePath: sqlitePath, databaseFileName: \"ingestion.db\")    .WithSqliteWeb();var api = builder    .AddProject&lt;Projects.CompanyIntel_Api&gt;(\"api\")    .WithReference(llama)    .WithReference(embedModel)    .WithReference(qdrant)    .WithReference(sqlite);builder    .AddJavaScriptApp(\"ui\", \"../CompanyIntel.UI\", \"dev\")    .WithPnpm()    .WithHttpEndpoint(port: 3000, env: \"PORT\")    .WithEnvironment(\"AGENT_URL\", api.GetEndpoint(\"http\"))    .WithOtlpExporter();await builder.Build().RunAsync();dotnet aspire run starts Ollama (pulls models if needed), Qdrant, SQLite with a web UI, the ASP.NET Core API, and the Next.js frontend. WithLifetime(ContainerLifetime.Persistent) means Ollama and Qdrant survive between runs. WithReference injects connection strings automatically ‚Äî the API project never hardcodes a URL.RAG evaluation with LLM-as-JudgeThis is where Aspire really pays off. The evaluation tests spin up the entire stack ‚Äî Ollama, Qdrant, API ‚Äî in ephemeral containers, ingest a test PDF, ask questions, and score the answers using LLM-as-Judge.The test fixture:public class AspireAppFixture : IAsyncLifetime{    public async ValueTask InitializeAsync()    {        var appHost = await DistributedApplicationTestingBuilder.CreateAsync&lt;Projects.CompanyIntel_AppHost&gt;();        _app = await appHost.BuildAsync(cts.Token);        await _app.StartAsync(cts.Token);        await _app.ResourceNotifications.WaitForResourceHealthyAsync(\"api\", cts.Token);        ApiClient = _app.CreateHttpClient(\"api\", \"http\");    }}The evaluation itself uses Microsoft.Extensions.AI.Evaluation.Quality, which provides four evaluators:            Metric      What it measures                  Relevance      Does the answer address the question?              Coherence      Is the answer well-structured and logical?              Groundedness      Is every claim backed by the retrieved context?              Retrieval      Did the search find the right chunks?      Each evaluator uses the same Ollama llama3.1 as a judge and scores on a 1-5 scale:[Theory][MemberData(nameof(EvalScenarios))]public async Task EvaluateRagResponseQuality(string question){    var chatResponse = await fixture.ApiClient.PostAsJsonAsync(\"/api/chat\", new { message = question });    var result = await chatResponse.Content.ReadFromJsonAsync&lt;ChatApiResponse&gt;();    IChatClient judgeChatClient = new OllamaApiClient(fixture.OllamaEndpoint, \"llama3.1\");    var chatConfig = new ChatConfiguration(judgeChatClient);    IEvaluator[] evaluators =    [        new RelevanceEvaluator(),        new CoherenceEvaluator(),        new GroundednessEvaluator(),        new RetrievalEvaluator(),    ];    foreach (var evaluator in evaluators)    {        var evalResult = await evaluator.EvaluateAsync(            messages, modelResponse, chatConfig, additionalContext, cts.Token);        foreach (var metricName in evaluator.EvaluationMetricNames)        {            var metric = evalResult.Get&lt;NumericMetric&gt;(metricName);            Assert.True(metric.Value &gt;= 3,                $\"{metricName} score too low: {metric.Value}\");        }    }}graph LR    Aspire[\"AspireTest Runner\"] --&gt;|start stack| Stack[\"Ollama + Qdrant + API(ephemeral containers)\"]    Stack --&gt;|ready| Ingest[\"Ingesttest PDF\"]    Ingest --&gt;|for each scenario| ChatAPI[\"Chat API/api/chat\"]    ChatAPI --&gt;|answer + context| Judge[\"Judge LLM(llama3.1)\"]    Judge --&gt;|scores 1-5| Metrics[\"Relevance ¬∑ CoherenceGroundedness ¬∑ Retrieval\"]    Metrics --&gt;|all &gt;= 3?| Assert[\"xUnit Assert\"]Groundedness is the most interesting metric here. It checks whether every claim in the answer can be traced back to the retrieved context. A score below 3 means the model is hallucinating ‚Äî making claims the documents don‚Äôt support.DemoThe chat tab. Ask a question, get a grounded answer with citations:    The documents tab. Upload PDFs, see ingestion stats (pages, chunks, size):    SummaryThe .NET AI building blocks let you write RAG applications without coupling to a specific provider.What worked well:  VectorData‚Äôs auto-embedding removed all manual embedding calls from the codebase. Define [VectorStoreVector] on a string property, and upsert/search just works.  DelegatingChatClient handled a real compatibility issue (missing MessageId in OllamaSharp streaming) without touching the rest of the code.  Aspire‚Äôs DistributedApplicationTestingBuilder made RAG evaluation reproducible: ephemeral containers, random ports, clean state per run.  Microsoft.Extensions.AI.Evaluation.Quality gave us four evaluation metrics out of the box, using the same local LLM as judge.References  Microsoft.Extensions.AI documentation  Microsoft.Extensions.VectorData documentation  Microsoft.Extensions.AI.Evaluation  AG-UI protocol"
    },
  
    {
      "id": "1",
      "title": "Building RAG with .NET Aspire and Python",
      "url": "/dotnet/ai/2026/02/22/building-rag-with-aspire-and-python.html",
      "date": "February 22, 2026",
      "categories": ["dotnet","ai"],
      "tags": ["dotnet","aspire","python","pydantic-ai","rag","opentelemetry","qdrant","ollama"],
      "shortinfo": "How .NET Aspire orchestrates a polyglot RAG system with Python agents, Qdrant vector store, and Ollama LLM, with one command and full observability.",
      "content": "TL;DRI built a RAG-based company research assistant where .NET Aspire orchestrates a Python backend (Pydantic AI + FastAPI), a Next.js frontend (CopilotKit), Qdrant for hybrid vector search, and Ollama for local LLM inference. One dotnet run starts everything. This post walks through the architecture, the RAG pipeline, and how Aspire makes polyglot orchestration and observability surprisingly painless.Source code: https://github.com/NikiforovAll/company_intel  TL;DR  Introduction  Architecture overview  Aspire as the orchestrator          Connection strings flow to Python        The RAG pipeline          Ingestion      Hybrid retrieval      Streaming to the frontend        Observability  RAG evaluation with Aspire  Demo  Summary  ReferencesIntroductionMost RAG tutorials show you the happy path: embed some text, throw it into a vector store, query it. Real systems are messier. You need a scraper, a chunking pipeline, embeddings, a vector store with hybrid search, an LLM, a frontend, and something to wire it all together. Each piece has its own runtime, its own configuration, its own way of failing.I wanted to build a company research assistant, where you ingest data about companies (websites, Wikipedia, news) and then ask questions with grounded, cited answers. The interesting part isn‚Äôt any single component. It‚Äôs how they fit together..NET Aspire turned out to be the right tool for this. Not because it‚Äôs .NET (the backend is Python, the frontend is Node.js), but because it handles the boring-but-hard parts: service discovery, connection string injection, health checks, and OpenTelemetry collection. You define the topology once, run it, and everything talks to everything.Architecture overviewThe system has two phases.  Phase 1Ô∏è‚É£: a backoffice agent scrapes company data, chunks it, embeds it (dense + sparse vectors), and stores everything in Qdrant.  Phase 2Ô∏è‚É£: a chat agent takes user questions, runs hybrid retrieval against Qdrant, and generates grounded answers with citations. No internet access required during query time.Phase 1: INGEST (online)          Phase 2: QUERY (offline)‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄUser: \"ingest Figma\"              User: \"Who are competitors?\"  ‚Üí Scrape websites (Crawl4AI)      ‚Üí Hybrid retrieval (Qdrant)  ‚Üí Clean ‚Üí Chunk ‚Üí Embed           ‚Üí RRF fusion (dense + BM25)  ‚Üí Store in Qdrant                 ‚Üí LLM generates grounded answerThe data flow for queries: UI (CopilotKit) ‚Üí AG-UI protocol ‚Üí FastAPI ‚Üí Pydantic AI agent ‚Üí Ollama (Qwen3 8B). The agent calls a search_knowledge_base tool that embeds the query, runs hybrid search in Qdrant, applies a context budget, and returns chunks to the LLM.graph LR    UI[\"Next.js(CopilotKit)\"] --&gt;|AG-UI SSE| API[\"FastAPI(Pydantic AI)\"]    API --&gt;|embed query| Ollama_E[\"Ollamaarctic-embed\"]    API --&gt;|hybrid search| Qdrant[\"Qdrant(dense + BM25)\"]    API --&gt;|generate answer| Ollama_L[\"OllamaQwen3 8B\"]    Qdrant --&gt;|top-k chunks| API    Ollama_E --&gt;|384-dim vector| APIAspire as the orchestratorHere‚Äôs the entire AppHost/Program.cs. This is the only .NET code in the project, and it defines everything:var builder = DistributedApplication.CreateBuilder(args);var ollama = builder    .AddOllama(\"ollama\")    .WithImageTag(\"latest\")    .WithDataVolume()    .WithLifetime(ContainerLifetime.Persistent);var qwen3 = ollama.AddModel(\"qwen3:latest\");var embedModel = ollama.AddModel(\"snowflake-arctic-embed:33m\");var qdrant = builder    .AddQdrant(\"qdrant\", apiKey: builder.AddParameter(\"qdrant-apikey\", \"localdev\"))    .WithDataVolume()    .WithLifetime(ContainerLifetime.Persistent);var agent = builder    .AddUvicornApp(\"company-intel-agent\", \"../agent\", \"main:app\")    .WithUv()    .WithHttpHealthCheck(\"/health\")    .WithReference(qwen3)    .WithReference(embedModel)    .WithReference(qdrant)    .WithOtlpExporter();var ui = builder    .AddNpmApp(\"ui\", \"../ui\", \"dev\")    .WithPnpmPackageInstallation()    .WithHttpEndpoint(port: 3000, env: \"PORT\")    .WithEnvironment(\"AGENT_URL\", agent.GetEndpoint(\"http\"))    .WithOtlpExporter();await builder.Build().RunAsync();A few things worth noting:AddOllama and AddModel pull and run Ollama as a container, then register specific models. The Python agent gets connection strings for both qwen3 (the LLM) and snowflake-arctic-embed (the embedding model) automatically through WithReference. Qdrant is the same story. WithLifetime(ContainerLifetime.Persistent) means Ollama and Qdrant survive between runs, so you don‚Äôt re-download models every time.AddUvicornApp is a community extension that runs a Python FastAPI app via uvicorn, managed by uv for dependency resolution. Aspire treats it like any other resource: health checks, logs, traces, all piped into the dashboard.AddNpmApp does the same for the Next.js frontend. WithPnpmPackageInstallation runs pnpm install on startup. The AGENT_URL environment variable points the frontend at the Python backend, and Aspire resolves the endpoint dynamically.Connection strings flow to PythonWhen Aspire starts the Python agent, it injects environment variables like ConnectionStrings__ollama-qwen3 with the format Endpoint=http://localhost:XXXX;Model=qwen3:latest. The Python side parses these in settings.pyNo hardcoded URLs. No .env files to manage. Aspire assigns ports dynamically, and the Python code just reads them. If you add a second Qdrant instance or swap Ollama for a remote endpoint, you change the AppHost, not the Python code.The RAG pipelineIngestionThe ingestion pipeline in pipeline.py is straightforward: load raw documents, chunk them, embed them, upsert to Qdrant.async def ingest_company(company: str, data_dir: Path) -&gt; IngestionResult:    with logfire.span(\"ingest_company {company}\", company=company):        store = get_vectorstore()        store.delete_company(company)  # idempotent: wipe and re-ingest        docs = load_raw_documents(company, data_dir)        chunks = chunk_documents(docs)        embedder = get_embedder()        texts = [c.text for c in chunks]        dense, sparse = await embedder.embed_texts(texts)        total = store.upsert_chunks(chunks, dense, sparse)        return IngestionResult(            company=company,            documents_loaded=len(docs),            chunks_produced=len(chunks),            vectors_stored=total,        )The chunking is semantic: split by headings, then paragraphs, then sentences, targeting 256-512 tokens with 50-token overlap. Each chunk gets both a dense vector (from snowflake-arctic-embed-s, 384 dimensions) and a sparse BM25 vector via fastembed.Hybrid retrievalThe query side uses Qdrant‚Äôs prefetch + fusion to combine dense and sparse search:def search(self, dense_vector, sparse_vector, company=None, limit=SEARCH_FUSION_LIMIT):    query_filter = None    if company:        query_filter = Filter(must=[            FieldCondition(key=\"company\", match=MatchValue(value=company.strip().lower()))        ])    response = self._client.query_points(        collection_name=COLLECTION_NAME,        prefetch=[            Prefetch(                query=dense_vector,                using=DENSE_VECTOR_NAME,                score_threshold=DENSE_SCORE_THRESHOLD,                limit=SEARCH_DENSE_LIMIT,            ),            Prefetch(                query=SparseVector(indices=sparse_vector.indices, values=sparse_vector.values),                using=SPARSE_VECTOR_NAME,                limit=SEARCH_SPARSE_LIMIT,            ),        ],        query=FusionQuery(fusion=Fusion.RRF),        query_filter=query_filter,        limit=limit,        with_payload=True,    )    # ... return resultsTwo prefetch branches run in parallel inside Qdrant: top-10 dense results and top-10 sparse results. Reciprocal Rank Fusion (RRF) merges them into a single ranked list. Dense search catches semantic similarity; sparse search catches exact keyword matches. The combination is more robust than either alone.The agent then generates an answer with inline citations from the retrieved chunks based on agent‚Äôs system prompt.Streaming to the frontendThe FastAPI backend uses the AG-UI protocol (via pydantic_ai.ui.ag_ui.AGUIAdapter) to stream agent responses as server-sent events. The frontend uses CopilotKit, which connects to the Python backend through an HttpAgent:@app.post(\"/\")async def run_agent(request: Request) -&gt; Response:    with logfire.span(\"agent request\"):        response = await AGUIAdapter.dispatch_request(request, agent=agent)        return responseOne line dispatches the request. CopilotKit on the React side handles message rendering, tool call visualization, and streaming updates.Observabilityü§î Getting OpenTelemetry from Python into the Aspire dashboard has a catch: Python‚Äôs OTLP exporter defaults to gRPC, but Aspire‚Äôs gRPC endpoint requires HTTP/2 with ALPN, which Python‚Äôs grpcio doesn‚Äôt support. The fix is to use HTTP/protobuf instead:def _configure_aspire_otlp() -&gt; None:    http_endpoint = os.environ.get(\"DOTNET_DASHBOARD_OTLP_HTTP_ENDPOINT_URL\")    if http_endpoint:        os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = http_endpoint        os.environ[\"OTEL_EXPORTER_OTLP_PROTOCOL\"] = \"http/protobuf\"Aspire sets DOTNET_DASHBOARD_OTLP_HTTP_ENDPOINT_URL automatically via WithOtlpExporter(). The Python code picks it up and redirects telemetry to the HTTP endpoint.ü§î Another gotcha: the Aspire dashboard can‚Äôt render exponential histograms, so you need explicit bucket histograms. Logfire‚Äôs defaults use exponential, so the code overrides them with ExplicitBucketHistogramAggregation.Once configured, you get distributed traces across the entire request path: HTTP request ‚Üí agent execution ‚Üí embedding ‚Üí Qdrant search ‚Üí LLM generation. All visible in the Aspire dashboard alongside the .NET services.Logfire instruments Pydantic AI and HTTPX automatically:logfire.configure(service_name=\"company-intel-agent\", send_to_logfire=False)logfire.instrument_pydantic_ai() # adds Semantic Conventions for Generative AIlogfire.instrument_httpx()This gives you GenAI semantic conventions (token usage per LLM call) and HTTP client spans for free.Here‚Äôs what a distributed trace looks like for a backoffice ingestion request. You can see every span: the agent run, scraping (Wikipedia, website, search), and the final ingestion into Qdrant:    And the metrics view, showing scraper page content size broken down by source type:    RAG evaluation with AspireHere‚Äôs where Aspire really pays off. The eval test uses DistributedApplicationTestingBuilder to spin up the entire stack, a fresh Qdrant, Ollama, and the Python agent, run ingestion against a golden dataset, then check retrieval quality:[Fact]public async Task RetrievalQualityMeetsThresholds(){    var appHost = await DistributedApplicationTestingBuilder.CreateAsync&lt;Projects.AppHost&gt;(        [\"UseVolumes=false\"],  // fresh Qdrant, no stale data        (appOptions, _) =&gt; appOptions.DisableDashboard = true,        cts.Token    );    await using var app = await appHost.BuildAsync(cts.Token);    await app.StartAsync(cts.Token);    await app.ResourceNotifications.WaitForResourceHealthyAsync(        \"company-intel-agent\", cts.Token    );    using var http = app.CreateHttpClient(\"company-intel-agent\");    // Start eval, poll until done, assert thresholds    var startResp = await http.PostAsJsonAsync(\"/eval/run\", new { company = \"paypal\" }, cts.Token);    // ... poll status ...    Assert.True(status.Metrics!.HitRate &gt;= 0.50);    Assert.True(status.Metrics!.ContextRecall &gt;= 0.50);}UseVolumes=false ensures a clean Qdrant on every run. No leftover data from previous tests. Aspire assigns random ports, so there are no conflicts if you run tests in parallel. The test exercises the real ingestion and retrieval pipelines, not a mock.The eval itself uses substring matching: each query in the golden dataset has expected facts, and the test checks whether retrieved chunks contain those facts. No LLM-as-judge needed for this; deterministic substring matching gives reliable signal in seconds instead of minutes.ü§î We could use LLM-as-judge for a more semantic evaluation, but that would require a fast local model. Qwen3 8B is good for generation but slow for evaluation, so substring matching was a pragmatic choice here.Two metrics: Hit Rate (did at least one expected fact get retrieved?) and Context Recall (what fraction of expected facts were found?). Both must exceed 50% or CI fails.DemoThe chat tab. Ask a question, get a grounded answer with citations from the knowledge base:        Multi-company research with detailed, structured answers:    The backoffice tab. Tell it to gather data for a company, and it scrapes, chunks, embeds, and stores everything in the background:    SummaryThe main takeaway: Aspire isn‚Äôt just for .NET microservices. It‚Äôs a polyglot orchestrator. If your system has containers, Python apps, and Node.js frontends that need to discover each other and report telemetry to the same place, Aspire handles that with less configuration than Docker Compose.What worked well:  Connection string injection removed all hardcoded URLs from Python code  WithOtlpExporter() gave the Python agent observability for free (once you work around the HTTP/2 issue)  DistributedApplicationTestingBuilder made RAG evaluation reproducible: fresh stack per test, no manual setupWhat I‚Äôd do differently next time: use a remote LLM for evaluation instead of local Qwen3. Substring matching works, but semantic matching would catch paraphrased facts. That needs a fast model though, not a 8B one running on CPU.References  .NET Aspire documentation  Pydantic AI  AG-UI protocol  CopilotKit  Qdrant hybrid search  Crawl4AI"
    },
  
    {
      "id": "2",
      "title": "Risks of AI-Assisted Development",
      "url": "/ai/2026/02/15/risks-of-ai-assisted-development.html",
      "date": "February 15, 2026",
      "categories": ["ai"],
      "tags": ["ai","productivity","software-engineering"],
      "shortinfo": "AI coding tools are useful, but understanding their failure modes ‚Äî from prompt injection to skill degradation ‚Äî is essential for using them responsibly.",
      "content": "TL;DRAI coding tools are powerful but come with real risks: unsolved prompt injection vulnerabilities, subtle bugs that pass review because they ‚Äúlook right,‚Äù cognitive shifts from writing to reading code, measurable skill degradation, and unclear ROI. None of these mean you should stop using AI ‚Äî they mean you should use it with open eyes. Review AI-generated code harder, write code by hand regularly, and make sure fundamentals come before delegation.  TL;DR  Introduction  1Ô∏è‚É£ Security: prompt injection is unsolved  2Ô∏è‚É£ Subtle bugs: the ‚Äúlooks right‚Äù problem  3Ô∏è‚É£ Cognitive shift: reading vs. writing code  4Ô∏è‚É£ Skill degradation: knowing when to learn  5Ô∏è‚É£ Cost: the enterprise pricing question  What to do about all thisIntroductionAI coding tools are useful. They‚Äôre also new enough that we‚Äôre still figuring out what goes wrong when you lean on them too hard. This post covers the real risks ‚Äî not to scare anyone off, but because using a power tool without understanding its failure modes is how you lose a finger.AI is a tool. Treat it like one. A table saw doesn‚Äôt care about your fingers, and a language model doesn‚Äôt care about your codebase.1Ô∏è‚É£ Security: prompt injection is unsolvedEvery AI tool that processes external input is vulnerable to prompt injection. This is not a theoretical concern ‚Äî it‚Äôs a known, actively exploited class of vulnerability with no general solution today.The core problem: language models can‚Äôt reliably distinguish between instructions and data. If your application feeds user input into a prompt, an attacker can hijack the model‚Äôs behavior. Filters and guardrails help, but they‚Äôre mitigations, not fixes. Researchers keep finding bypasses.What this means in practice:  Code that calls AI APIs with user-supplied content needs the same scrutiny as code that builds SQL queries from user input  AI-generated code may introduce vulnerabilities the model doesn‚Äôt flag ‚Äî it optimizes for ‚Äúlooks correct,‚Äù not ‚Äúis secure‚Äù  Supply chain concerns multiply when AI suggests dependencies it was trained on but hasn‚Äôt vettedUntil prompt injection has a real solution (and there‚Äôs no timeline for that), treat AI-touching-external-input as a threat surface.2Ô∏è‚É£ Subtle bugs: the ‚Äúlooks right‚Äù problemAI-generated bugs are different. The code reads clean. Variable names are reasonable. The structure follows conventions. Everything looks right. And buried inside is a logic error that won‚Äôt surface until production ‚Äî a race condition, an off-by-one in business logic, a null check that misses an edge case.This is genuinely dangerous. Code review relies partly on pattern recognition ‚Äî reviewers notice when something ‚Äúfeels off.‚Äù AI-generated code doesn‚Äôt trigger that instinct because it‚Äôs optimized for readability, not correctness. The model produces text that looks like working code based on statistical patterns. Sometimes that‚Äôs working code. Sometimes it‚Äôs convincingly wrong.The defense is old-fashioned: tests, assertions, thorough reviews, and healthy skepticism. Don‚Äôt trust code more because it reads well. Read it harder because it reads well.3Ô∏è‚É£ Cognitive shift: reading vs. writing codeWriting code and reading code use different parts of your brain. Writing forces you to think through logic step by step ‚Äî you‚Äôre constructing. Reading is pattern matching ‚Äî you‚Äôre recognizing.Neuroscience calls this the ‚Äúgeneration effect‚Äù (documented since 1978): you need to do something to truly know it. Flash card experiments showed that generating a word from a partial cue created dramatically better recall than just reading the complete word. Brain scans confirm it ‚Äî generating thoughts from scratch lights up multiple brain regions simultaneously. Reading or hearing information barely registers by comparison.When AI writes the code and you review it, you‚Äôre always in reading mode. You lose the constructive thinking that comes from writing. Over months and years, that changes how you relate to code. You get better at skimming and approving, worse at building from scratch.An MIT study put numbers on this. Researchers monitored brain activity (EEG) while students wrote essays under three conditions: unassisted, AI-assisted, and AI-after-outlining. The AI-assisted group showed significantly lower brain activity and neural connectivity while working. And here‚Äôs the telling part: 83% of the ChatGPT group couldn‚Äôt recall a single sentence from their own essay minutes after finishing. The essays were technically fine ‚Äî grammatically correct, well-structured ‚Äî but evaluators consistently called them ‚Äúhollow‚Äù and ‚Äúsoulless.‚Äù The students produced text without thinking. It showed.This isn‚Äôt hypothetical. Developers who rely heavily on autocomplete and generation tools report feeling less confident writing code without them. The skill atrophies like any unused muscle.The risk isn‚Äôt that AI makes you a worse developer tomorrow. It‚Äôs that two years from now, you can‚Äôt write a moderately complex function without reaching for a tool. And when the tool is wrong (see previous section), you can‚Äôt tell.Deliberate practice matters. Write code by hand regularly. Solve problems without AI assistance. Keep the muscle working.4Ô∏è‚É£ Skill degradation: knowing when to learnThere‚Äôs a difference between delegating a task you understand and delegating a task you‚Äôve never learned. AI blurs this line.If a senior developer uses AI to generate boilerplate they‚Äôve written hundreds of times ‚Äî fine. They know what correct looks like. They‚Äôll spot problems. But if a junior developer uses AI to skip learning how authentication works, or how database transactions behave, they‚Äôre building on a foundation they don‚Äôt have.The evidence from other fields is blunt:  London taxi drivers ‚Äî who navigate from memory ‚Äî had physically larger brain regions for spatial reasoning than average. Drivers who switched to GPS experienced measurable gray matter shrinkage in those same regions.  Doctors who used AI diagnostic assistance for just four months showed weakened ability to spot cancer independently afterward.  In a study by Christof Van Nimwegen, participants who solved logic puzzles with software assistance collapsed when the software was taken away ‚Äî they ‚Äúaimlessly clicked around‚Äù while the unassisted group kept improving.The pattern repeats: outsource a cognitive function to a tool, and the brain physically downsizes for that function. There‚Äôs no reason to think coding is exempt.The uncomfortable truth: AI makes it easy to produce code you don‚Äôt understand. And for a while, that works. You ship features, hit deadlines, look productive. The gap shows up when something breaks in an unfamiliar way, or when you need to design something the model hasn‚Äôt seen, or when you‚Äôre in a meeting and can‚Äôt explain your own system.Know the difference between ‚ÄúI‚Äôm using AI to go faster at something I know‚Äù and ‚ÄúI‚Äôm using AI to avoid learning something I should know.‚Äù The second one has a cost, and you pay it later.5Ô∏è‚É£ Cost: the enterprise pricing questionAI tooling isn‚Äôt cheap. Enterprise tiers, additional API usage for agentic workflows, premium models ‚Äî the bill adds up. For a team of 100 developers, you‚Äôre looking at $25-50K/year just for Copilot licenses, potentially much more with heavy agentic usage.The ROI question remains genuinely open. Vendors cite productivity studies, but most of those measure speed on isolated tasks, not long-term codebase quality or maintenance costs. Nobody has strong longitudinal data yet.Meanwhile, pricing models keep changing. What costs $19/month today might cost $39 tomorrow if vendors decide the market will bear it. Or it might get cheaper as competition increases. Nobody knows, and that uncertainty makes long-term budgeting harder than anyone admits.Organizations should treat AI tool spend like any other infrastructure cost: measure what you can, set budgets, and be honest about what you don‚Äôt know yet. ‚ÄúEveryone else is paying for it‚Äù is not a strategy.What to do about all thisNone of these risks mean you should avoid AI tools. They mean you should use them with open eyes.  Review AI-generated code with more scrutiny, not less  Maintain security practices around any AI-facing surface  Write code by hand regularly to keep your skills sharp  Make sure junior developers learn fundamentals before leaning on generation  Think first, then delegate ‚Äî outline your approach before handing work to AI (the MIT study found this group had higher brain connectivity than even the unassisted group)  Track your AI tool costs and measure outcomes honestly  Stay current on prompt injection research ‚Äî the landscape changes fastThe developers who‚Äôll get the most from AI tools long-term are the ones who understand both the capabilities and the failure modes. Blind trust and blind rejection are equally wasteful."
    },
  
    {
      "id": "3",
      "title": " Observing Claude Code Task Orchestration in Real Time using 'claude-code-kanban'",
      "url": "/ai/productivity/2026/02/07/claude-code-kanban.html",
      "date": "February 07, 2026",
      "categories": ["ai","productivity"],
      "tags": ["claude","ai","agents","developer-tools","cli","coding-stories"],
      "shortinfo": "A real-time Kanban dashboard for observing Claude Code agent tasks",
      "content": "TL;DR: claude-code-kanban is a real-time Kanban dashboard for Claude Code teams. Tasks flow through Pending ‚Üí In Progress ‚Üí Completed in your browser.One command: npx claude-code-kanban.Source code: github.com/NikiforovAll/claude-task-viewerWhy I built thisI was running a Claude Code team, a lead agent coordinating three specialists to implement an auth module. Around minute five I realized I had no idea what was happening. One agent was blocked, another had finished, the lead was assigning new work.Claude Code has a built-in task list, but it‚Äôs limited. You see a flat list of tasks in the terminal. No visual grouping by status, no dependency graph, no way to track multiple agents at a glance. When you have four agents working in parallel, that‚Äôs not enough.I wanted to glance at something and immediately know: what‚Äôs done, what‚Äôs stuck, what‚Äôs next. So I built a Kanban board that watches Claude Code‚Äôs task files and streams updates to a browser.What it looks likeThree columns: Pending, In Progress, Completed. Each task card shows its assigned agent (color-coded), a short description, and which tasks it‚Äôs waiting on.  The sidebar lists sessions with progress bars. A live feed shows what agents are doing right now. You can filter by project, search by name, or toggle between active and all sessions.Clicking a task opens the detail panel: full description, dependencies, and a note field. The notes are readable by Claude too, so you can leave instructions mid-session.  What you actually getThe dashboard auto-detects team sessions. Each agent gets a consistent color across the board. You can filter by owner to isolate one agent‚Äôs work or see everything at once. Session cards show team member count and names.Blocked tasks show ‚ÄúWaiting on #X, #Y‚Äù labels. If you try to delete a task that blocks others, the dashboard stops you and shows which tasks would break. This is how you find bottlenecks: three tasks all waiting on one means that‚Äôs your critical path.Press P to view the session plan with syntax-highlighted code blocks. Press I for session info: team members, project path, git branch. I check these first when coming back to a session.The whole thing is keyboard-driven. Arrow keys navigate tasks, Tab switches between sidebar and board, Enter opens details, Esc closes panels. ? shows all shortcuts.Dark and light themes. The dark one has a ‚ÄúTerminal Luxe‚Äù aesthetic. Preference sticks across sessions.Getting startednpx claude-code-kanbanThat‚Äôs it. The server starts on port 3456 and opens your browser. Start a Claude Code session with teams and the dashboard picks it up automatically, no configuration needed.What‚Äôs nextThe tool is observation-only by design. Claude Code owns the task state and I want to keep that boundary clean. But the observation side has room to grow: timeline views, session comparison, maybe export for sharing with people who aren‚Äôt staring at the terminal.References  Source Code ‚Äî GitHub  npm Package ‚Äî claude-code-kanban  Claude Code Documentation"
    },
  
    {
      "id": "4",
      "title": "LazyClaude 0.12.0: Marketplace Plugin Management",
      "url": "/ai/2025/12/21/lazyclaude-0-12-0.html",
      "date": "December 21, 2025",
      "categories": ["ai"],
      "tags": ["ai","claude","python","productivity","developer-tools","tui"],
      "shortinfo": "New marketplace browser for discovering and managing Claude Code plugins directly from the terminal",
      "content": "TL;DRLazyClaude is a keyboard-driven TUI for managing Claude Code customizations (slash commands, subagents, skills, MCPs, hooks). Version 0.12.0 introduces a built-in marketplace browser that lets you discover, preview, and install plugins without leaving the terminal. If you‚Äôre new to LazyClaude, check out the original announcement for a full introduction.Source: github.com/NikiforovAll/lazyclaudeInstall: uvx lazyclaudeWhy LazyClaude?Managing Claude Code customizations means navigating multiple configuration directories (~/.claude/, ./.claude/, ~/.claude/plugins/) and understanding what‚Äôs defined at each level. LazyClaude gives you a visual, keyboard-driven interface that shows everything in one place.Key Benefits:  Visual Exploration: See all customizations organized by type (slash commands, subagents, skills, memory files, MCPs, hooks)  Multi-Level Management: Understand and manage configurations across user, project, and plugin levels  Keyboard-Driven Workflow: Navigate with vim-style keys (j/k), no mouse required  Copy/Move Between Levels: Easily share personal configs with your team or customize plugin content  Integrated Marketplace: Discover and install plugins without leaving the terminal (new in 0.12.0!)Getting StartedLaunch with uvx lazyclaude and you‚Äôll see a multi-panel interface inspired by lazygit:  Interface Overview:  Left Sidebar: Panels for each customization type ([1] Slash Commands, [2] Subagents, [3] Skills, [4] Memory, [5] MCPs, [6] Hooks)  Right Pane: Content and metadata view for the selected item  Status Bar: Shows current path and active filter  Footer: Keyboard shortcuts referenceQuick Navigation:  1-6 or Tab - Switch between panels  j/k or arrow keys - Navigate items  [ / ] - Toggle between content and metadata views  ? - Show help, q - QuitCore WorkflowsFiltering and Viewing ConfigurationsLazyClaude helps you understand where customizations are defined. Use filters to narrow your view:  a - Show All levels  u - Show only User level (~/.claude/)  p - Show only Project level (./.claude/)  P - Show only Plugin level (~/.claude/plugins/)  D - Toggle visibility of disabled plugins  This makes it easy to:  See what‚Äôs personal vs. shared with your team  Understand what comes from installed plugins  Find where a specific customization is definedEditing and Managing Configurations  e - Open in your $EDITOR  c - Copy to another level (e.g., copy plugin config to user level to customize)  m - Move to another level  C - Copy file path to clipboardCommon Patterns:Customize a Plugin:[P]lugin ‚Üí navigate ‚Üí [c]opy ‚Üí User ‚Üí [e]ditShare with Team:[u]ser ‚Üí navigate ‚Üí [c]opy ‚Üí Project ‚Üí git commitWhat‚Äôs New in 0.12.0: Marketplace BrowserVersion 0.12.0 adds integrated marketplace support. Press M to open the marketplace browser:Browse and Install PluginsNavigate available plugins with vim-style keys (j/k), expand marketplace trees with l, and install plugins with i.  Marketplace Actions:  i - Install plugin (or enable if disabled)  d - Uninstall plugin  e - Open plugin folder in file manager  / - Search within marketplacePreview Before InstallingPress p on any plugin to enter preview mode. The marketplace closes and the main interface shows you all the plugin‚Äôs contents (slash commands, skills, etc.) in read-only mode. Explore what you‚Äôre getting before you install it.  Preview Mode Benefits:  View all customizations included in a plugin  Read content and metadata without installing  Understand plugin structure and capabilities  Navigate as if the plugin were installed (read-only)Press Esc to exit preview mode and return to the normal view.ConclusionLazyClaude 0.12.0 makes managing Claude Code customizations visual, keyboard-driven, and efficient. The new marketplace browser brings plugin discovery and installation directly into the terminal, completing the workflow. Whether you‚Äôre exploring your existing setup, customizing plugins, or sharing configurations with your team, LazyClaude provides the tools you need without leaving the command line.Get Started:uvx lazyclaudeFor complete documentation on all features and workflows, check out the user guide."
    },
  
    {
      "id": "5",
      "title": "LazyClaude: Explore Your Claude Code Setup Without Breaking Flow",
      "url": "/ai/2025/12/07/lazyclaude.html",
      "date": "December 07, 2025",
      "categories": ["ai"],
      "tags": ["ai","claude","python","productivity","developer-tools","tui"],
      "shortinfo": "A lazygit-inspired TUI to browse all your Claude Code customizations",
      "content": "TL;DRClaude Code‚Äôs customization system is powerful but scattered across multiple files and configuration levels. LazyClaude is a lazygit-inspired terminal UI that lets you explore all your customizations‚Äîslash commands, agents, skills, MCPs, hooks, and memory files‚Äîin one place. Filter by level (user/project/plugin), search by name, and navigate with vim-style keybindings.Source: github.com/NikiforovAll/lazyclaudeInstall: uvx lazyclaudeIntroductionClaude Code has a rich customization system. You can define custom slash commands, create specialized agents, configure MCP servers, set up hooks, and write memory files that shape how Claude understands your project. The problem? These customizations live in different places:  ~/.claude/ for user-level configuration  .claude/ for project-specific settings  Plugin directories for third-party extensionsWhen you need to find a specific command or understand what‚Äôs available, you‚Äôre left running /help, /mcp, /agents, or opening files one by one. Each lookup interrupts your flow.LazyClaude solves this by providing a dedicated terminal UI where you can browse all your Claude Code customizations without leaving your workflow.What is LazyClaude?LazyClaude is a terminal user interface that follows the design philosophy of lazygit: keyboard-driven, panel-based, and focused on getting information quickly.See README for more details.Getting StartedInstall with uv:uvx lazyclaudeConclusionLazyClaude brings visibility to Claude Code‚Äôs configuration system. Instead of hunting through files or interrupting your workflow with help commands, you get a dedicated window showing everything at a glance.It‚Äôs keyboard-first, fast, and stays out of your way‚Äîexactly what a developer tool should be.Source code: https://github.com/NikiforovAll/lazyclaudeContributions welcome. Try it, break it, and let me know what customizations you discover.References  LazyClaude - GitHub Repository  Textual - Python TUI Framework  lazygit - Terminal UI for git  Claude Code Documentation"
    },
  
    {
      "id": "6",
      "title": "VSCode Morph Agent: Let AI Control Your Workspace",
      "url": "/ai/2025/12/01/vscode-morph-agent.html",
      "date": "December 01, 2025",
      "categories": ["ai"],
      "tags": ["vscode","ai","agents","productivity","copilot","developer-tools"],
      "shortinfo": "Explore how AI agents can manipulate your IDE workspace to enhance agentic coding workflows",
      "content": "TL;DRWhat if AI agents could not only write code but also organize your workspace? Morph Agent is a VSCode extension that lets AI control your editor layout through natural language. It‚Äôs a proof of concept demonstrating a powerful principle: agents should be able to manipulate the ambient environment to help us focus on what matters.Source: github.com/NikiforovAll/vscode-morphInstall: VSCode MarketplaceIntroductionModern software development increasingly involves working with multiple artifacts simultaneously‚Äîspecifications, implementation plans, task lists, code files, tests, and documentation. When using AI coding assistants, this complexity multiplies: you‚Äôre juggling context windows, chat panels, terminal output, and reference materials.The problem isn‚Äôt just having the right files open‚Äîit‚Äôs having them arranged in a way that supports your current workflow. Manually reorganizing your workspace breaks your flow. What if your AI assistant could do this for you?Morph Agent explores this idea. It‚Äôs a VSCode extension that gives AI the ability to control your editor layout. Through GitHub Copilot Chat, you can describe how you want your workspace arranged, and Morph makes it happen.The idea is based on a simple principle: AI should control the ambient workspace, not just the code.Furthermore, an agent should be able to produce additional tools or UI elements to facilitate better interaction with the user.Find this Principle in the Other ToolsFor example, in Claude Code, the user is presented with a special tool to clarify questions - AskUserQuestion. This tool opens a special input inside the terminal so that the user can provide input back to the agent. This is a great example of an agent manipulating the ambient environment to facilitate better interaction.What is Morph Agent?Morph Agent bridges the gap between natural language intent and editor configuration. It provides:      Natural Language Layout Control ‚Äî Use @morph-layout in Copilot Chat to request arrangements like ‚Äúsplit my files side by side‚Äù or ‚Äúopen the tests next to the implementation‚Äù        KDL Layout DSL ‚Äî A declarative configuration language (KDL) for precise, reproducible layouts        Bidirectional Conversion ‚Äî Capture your current layout as KDL code, or apply KDL to transform your workspace  The key insight is that layouts become first-class artifacts. You can version control them, share them with your team, and let AI generate them contextually.The KDL FormatKDL (pronounced ‚Äúcuddle‚Äù) is a minimal document language perfect for configuration. Here‚Äôs a quick taste:Side-by-side:layout {  cols {    group { file \"src/main.ts\" }    group { file \"src/test.ts\" }  }}Stacked:layout {  rows {    group { file \"README.md\" }    group { file \"CHANGELOG.md\" }  }}2x2 Grid:layout {  rows {    cols {      group { file \"a.ts\" }      group { file \"b.ts\" }    }    cols {      group { file \"c.ts\" }      group { file \"d.ts\" }    }  }}Here is a demo showing different layout configurations being applied:Scenario 1: Agentic Coding LayoutWhen working with AI coding assistants, I find myself constantly arranging windows: specification on one side, implementation plan visible, chat ready for questions, terminal for running tests. Setting this up manually each session is tedious.With Morph, I can define a persistent layout configuration:window {  layout {    cols {      rows size=\"60\" {        cols size=\"50\" {          group size=\"50\" { file \"spec.md\" }          group size=\"50\" { file \"plan.md\" }        }        group size=\"50\" { file \"extension.ts\" }      }      rows size=\"40\" {        group size=\"80\" { chat }        group size=\"20\" { terminal }      }    }  }}This layout gives me:  Top-left: Spec and plan files side by side (reference materials)  Bottom-left: The code I‚Äôm working on  Right side: AI chat (80%) and terminal (20%)Save this as .layout.kdl in your project, and you can restore it instantly next time you return to work on the same project. Your workspace becomes reproducible.Scenario 2: Spec-Driven DevelopmentSpec-driven development follows a natural progression:spec.md ‚Üí plan.md ‚Üí tasks.md ‚Üí implementationAt each stage, different files are relevant. Instead of manually opening and closing files, imagine an agent that understands where you are in the workflow and adjusts the workspace accordingly.When you‚Äôre in the planning phase:layout {  cols {    group size=\"50\" { file \"spec.md\" }    group size=\"50\" { file \"plan.md\" }  }}When you move to implementation:layout {  cols {    group size=\"30\" { file \"plan.md\" file \"tasks.md\" }    group size=\"70\" { file \"src/feature.ts\" }  }}The workspace morphs to match your mental context. This is the kind of ambient intelligence that makes AI assistants truly helpful.Scenario 3: Context-Aware File OpeningSometimes the agent just needs to show you the right files. During a conversation about testing, the agent might realize you need to see both the implementation and its tests:layout {  file \"extension.ts\" \"test/parser.test.ts\"}Or when debugging an issue across multiple files:layout {  cols {    group { file \"src/parser.ts\" }    group { file \"src/compiler.ts\" }    group { file \"src/types.ts\" }  }}The agent opens exactly what‚Äôs relevant based on the conversation context, providing better assistance through relevant file visibility.The Future VisionMorph Agent is just a proof of concept, but it demonstrates a principle I believe will become fundamental to agentic coding:Agents will manipulate the ambient workspace to help us focus.Today‚Äôs coding assistants are constrained to text‚Äîthey can read and write code, but they can‚Äôt:  Open relevant files when discussing a topic  Arrange windows to match your workflow  Adjust the environment based on the task at handThis will change. Future agents will:  Understand workflow phases and adapt the workspace automatically  Learn your preferences and suggest layouts based on past behavior  Coordinate multiple tools ‚Äî editor, browser, documentation, terminal ‚Äî as a unified environment  Preserve context across sessions, restoring not just files but the entire cognitive workspaceThe interface between human and AI isn‚Äôt just about generating code. It‚Äôs about shaping the environment where that code gets written.Getting StartedInstall Morph Agent from the VSCode Marketplace.Try it out:  Open Copilot Chat  Type @morph-layout generate a grid layout and apply from opened files  Watch your workspace transformOr create a .layout.kdl file in your project and use the ‚ÄúApply Layout‚Äù CodeLens to restore your preferred arrangement.ConclusionMorph Agent is an experiment in giving AI control over the workspace‚Äînot just the code. It‚Äôs a small step toward a future where agents understand not just what we‚Äôre building, but how we work.The source code is available on GitHub. Try it, break it, and imagine what ambient AI assistance could look like.References  Morph Agent - VSCode Marketplace  Morph Agent - GitHub Repository  KDL - The KDL Document Language"
    },
  
    {
      "id": "7",
      "title": "Transform .NET Diagnostics into a Specialized AI Agent with Claude Agent SDK",
      "url": "/dotnet/ai/2025/11/11/dotnet-format-agent.html",
      "date": "November 11, 2025",
      "categories": ["dotnet","ai"],
      "tags": ["dotnet","ai","agents","claude","cli","code-quality","mcp","developer-tools"],
      "shortinfo": "Learn how to build specialized AI agents that transform CLI tools into conversational interfaces using Claude Agent SDK and dotnet format as a practical example.",
      "content": "TL;DRTransform dotnet format from a CLI tool into a conversational AI agent that helps you explore and manage technical debt interactively. Learn the pattern by building with Claude Agent SDK. This blog post demonstrates how to use claude-agent-sdk to wrap dotnet format commands, expose them via MCP, and create a conversational interface for code quality analysis.  Note: You could use a general-purpose AI agent to analyze dotnet format output by copying and pasting JSON reports. However, a specialized agent offers significant advantages: it automates tool execution, maintains context across multiple queries, provides domain-specific insights, and creates a streamlined workflow. This post shows you how to build one. Also, it is kind of fun! üéâ  TL;DR  Why Build Specialized Agents?  Architecture Overview  Building with Claude Agent SDK          Tool Layer: Wrapping dotnet format      Agent Layer: MCP Integration      Making It Conversational        Demo: Interactive Code Quality  The Pattern: Reusable Building Blocks  Conclusion  ReferencesSource code: https://github.com/NikiforovAll/dotnet-format-agentWhy Build Specialized Agents?.NET projects often accumulate hundreds of diagnostics - formatting issues, analyzer warnings, code style violations. These can come from your IDE, dotnet format, or code analysis tools. The real challenge isn‚Äôt finding issues - it‚Äôs dealing with the overwhelming volume:# Run format check$ dotnet format --verify-no-changes --severity warn# Result: 247 diagnostics across 45 files# IDE0055, CA1031, IDE1006, CA2007, IDE0290...# Which ones matter? Where do I start? What's safe to auto-fix?You‚Äôre faced with questions:  ü§î Which issues should I tackle first?  ü§î What‚Äôs safe to auto-fix vs. needs manual review?  ü§î Which diagnostics indicate real problems vs. style preferences?  ü§î How do I prioritize across multiple files and rule types?Manual triage is exhausting: you need to look up diagnostic IDs, assess their impact, decide on priorities, and then repeat this process for every single issue. But here‚Äôs the reality - you probably don‚Äôt have time for this. You‚Äôve got features to ship, bugs to fix, and meetings to attend. Code quality is important, but it‚Äôs rarely urgent.So what happens? You see 247 warnings and think: ‚ÄúI‚Äôll deal with this later.‚Äù Except later never comes. Or worse - you start ignoring warnings entirely. When everything is highlighted as a problem, nothing feels like a problem. The warnings lose their meaning, and suddenly you‚Äôve defeated the whole purpose of having code quality tools in the first place.Codebase maintainability matters, but it needs to fit into your actual workflow. You need a way to quickly understand what‚Äôs important, what can wait, and what‚Äôs safe to fix right now - without spending hours analyzing diagnostics.What if you could just have a conversation? Instead of parsing diagnostics yourself, ask: ‚ÄúWhat are the most important issues to fix first?‚Äù or ‚ÄúWhat can I safely auto-fix without breaking anything?‚ÄùThat‚Äôs what specialized agents enable - they transform CLI tools into conversational interfaces that help you explore, triage, and prioritize diagnostic output in minutes.Architecture OverviewA specialized agent has three layers:graph TD    A[Conversational Interface] --&gt;|Natural language| B[Agent + Harness]    B --&gt;|Tool calls| C[Tool Layer]    C --&gt;|dotnet format| D[.NET Project]        B -.-&gt;|Uses| E[Claude Agent SDK]    C -.-&gt;|Wraps| F[CLI Tools]        style A fill:#e1f5ff    style B fill:#fff4e1    style C fill:#f0e1ffLayer 1: Conversational InterfaceWhat users interact with - plain English queries like ‚ÄúShow me formatting issues‚ÄùLayer 2: Agent Harness (Claude Agent SDK)Understands intent, calls appropriate tools, formats responses intelligentlyLayer 3: Tool LayerWraps dotnet format commands, parses output, returns structured dataThe key insight: Keep your tool layer independent. The agent is just a conversational wrapper around existing functionality.Building with Claude Agent SDKLet‚Äôs build the agent piece by piece using the Claude Agent SDK.Tool Layer: Wrapping dotnet formatStart with a clean abstraction over dotnet format:class DotnetFormatRunner:    \"\"\"Wraps dotnet format CLI with structured input/output.\"\"\"        def run_style_check(self, project_path: str,severity: str | None = None) -&gt; dict:        \"\"\"Run dotnet format style check.\"\"\"        # Build command        cmd = [\"dotnet\", \"format\", \"--verify-no-changes\", \"--report\", \"report.json\"]        if severity:            cmd.extend([\"--severity\", severity])        # Execute        result = subprocess.run(cmd, capture_output=True, cwd=project_path)        # Parse JSON report        with open(\"report.json\") as f:            diagnostics = json.load(f)        # Return structured data        return {            \"diagnostics\": diagnostics,            \"summary\": self._generate_summary(diagnostics)        }Key principles:  ‚úÖ Accept structured inputs (paths, options)  ‚úÖ Return structured outputs (dicts with diagnostics)  ‚úÖ Handle errors gracefully  ‚úÖ Keep it testable without AIAgent Layer: MCP IntegrationThe Model Context Protocol (MCP) is how Claude discovers and calls your tools. Using the Claude Agent SDK, expose your runner as MCP tools:from claude_agent_sdk import tool@tool(    \"extract_style_diagnostics\",    \"Extract code style/formatting diagnostics from dotnet format\",    {\"severity\": str, \"exclude\": list[str]})async def extract_style_diagnostics(args: dict) -&gt; dict:    # Normalize inputs (AI might send unexpected formats)    severity = args.get(\"severity\")    if severity in [\"all\", \"None\", \"\", None]:        severity = None        # Call your existing tool    runner = DotnetFormatRunner()    result = runner.run_style_check(project_path=os.getcwd(),severity=severity)        # Format response using TOON (Terminal Object Notation)    # This reduces token usage by 30-60% compared to JSON    formatted = format_diagnostics_toon(result[\"diagnostics\"])        return {        \"content\": [{            \"type\": \"text\",            \"text\": formatted        }]    }Why TOON format? Instead of verbose JSON:[  {\"file\": \"Program.cs\", \"line\": 10, \"rule\": \"IDE0055\", \"message\": \"Fix formatting\"},  {\"file\": \"Program.cs\", \"line\": 15, \"rule\": \"IDE0055\", \"message\": \"Fix formatting\"}]Use compact format:IDE0055 (count: 2):  Program.cs:10  Program.cs:15This is easier to read and uses fewer tokens = lower costs.Bundle tools into an MCP server:from claude_agent_sdk import create_sdk_mcp_server# Create MCP server with all your toolsdotnet_format_server = create_sdk_mcp_server(    name=\"dotnet-format\",    version=\"1.0.0\",    tools=[        extract_style_diagnostics,        extract_analyzers_diagnostics,        # ... more tools    ])Making It ConversationalConfigure the agent with your MCP server and system prompt:from claude_agent_sdk import ClaudeAgentOptions, ClaudeSDKClientoptions = ClaudeAgentOptions(    # Connect MCP servers    mcp_servers={\"dotnet-format\": dotnet_format_server},    # Which tools to enable    allowed_tools=[        \"mcp__techdebt__extract_style_diagnostics\",        \"mcp__techdebt__extract_analyzers_diagnostics\",        \"Read\",        \"Write\",        \"Edit\",        \"Glob\",        \"Grep\",        \"WebFetch\"    ],    # Working directory    cwd=os.getcwd(),    # Auto-approve tool calls    permission_mode=\"bypassPermissions\",    # System prompt    system_prompt={        \"type\": \"preset\",        \"preset\": \"claude_code\",  # Base Claude Code behavior        \"append\": \"\"\"You are a .NET code quality expert specializing in technical debt analysis.When analyzing codebases:1. Use 'warn' severity by default2. Group similar issues together3. Prioritize by impact and fix difficulty4. Always include \"Next Steps\" section\"\"\"    })# Run the agentasync with ClaudeSDKClient(options=options) as client:    await client.query(\"What are the most important code quality issues?\")        async for message in client.receive_response():        # Display response (markdown formatted)        print(message.text)The system prompt is crucial - it teaches the agent:  ‚úÖ Domain expertise (.NET, diagnostics)  ‚úÖ Tool usage patterns (default to ‚Äòwarn‚Äô severity)  ‚úÖ Response formatting (Markdown, sections)  ‚úÖ Output structure (always include Next Steps)Demo: Interactive Code Quality$ uv run --package tda tda \"List  grouped analyzers diagnostics\" --cwd ../path/to/project --interactiveTechnical Debt AgentWorking directory: ~/dev/path/to/projectModel: claude-sonnet-4-5Type 'exit' or 'quit' to end the session.You&gt; List analyzers diagnostics grouped by diagnostic*Agent Response Truncated*Would you like me to drill down into any specific diagnostic code or analyze particular files where these issues occur?Cost: $0.0886                The Pattern: Reusable Building BlocksEvery specialized agent follows this recipe:1. Tool Layer (what you already have)class YourToolRunner:    def run_analysis(self, path: str, options: dict) -&gt; dict:        # Call external tool        # Parse output          # Return structured data2. Agent Layer (MCP wrapper)@tool(\"your_tool\", \"description\", {schema})async def your_tool(args: dict) -&gt; dict:    result = runner.run(...)    return {\"content\": [{\"type\": \"text\", \"text\": formatted_result}]}server = create_sdk_mcp_server(tools=[your_tool])3. Conversational Interface (configure agent)options = ClaudeAgentOptions(    mcp_servers={\"your\": server},    system_prompt={\"append\": \"You are a {DOMAIN} expert...\"})async with ClaudeSDKClient(options=options) as client:    await client.query(\"Natural language question\")ConclusionBuilding specialized agents transforms how we interact with CLI tools:Before:dotnet format --verify-no-changes --report report.jsoncat report.json | jq '...'# manually analyze, look up docs, decide...After:tda \"What should I fix first?\"# Get prioritized recommendations with explanationsKey takeaways:  ‚úÖ Keep tool layer independent - agent is just a wrapper  ‚úÖ Use MCP to expose tools to Claude  ‚úÖ System prompt teaches domain expertise  ‚úÖ Optimize output format (TOON over JSON)  ‚úÖ Make it conversational - multi-turn exploration beats one-shot commandsThe pattern is reusable. Take your existing CLI tools, wrap them with Claude Agent SDK, and give them a conversational interface.For more details, check out the full source code. The code I provided here is a simplified version to illustrate the pattern.References  dotnet-format-agent - Full source code  Claude Agent SDK - Python SDK  Context7 - Claude SDK Docs - API reference  dotnet format - Official docs"
    },
  
    {
      "id": "8",
      "title": "Claude Code Handbook: Best Practices and Recommendations",
      "url": "/productivity/2025/10/05/claude-code-handbook.html",
      "date": "October 05, 2025",
      "categories": ["productivity"],
      "tags": ["productivity","ai","claude","agents"],
      "shortinfo": "In this post, I share my collection of practical recommendations and principles for using Claude Code.",
      "content": "Previously, I‚Äôve shared my Claude Code Usage Best Practices and Recommendations.Since than, I‚Äôve been using Claude Code extensively and have refined my practices further. In this post, I present an updated handbook of best practices and recommendations for using Claude Code for daily use.See: Claude Code Handbook"
    },
  
    {
      "id": "9",
      "title": "MCP Landscape for .NET Developers",
      "url": "/ai/mcp/2025/10/03/mcp-landscape.html",
      "date": "October 03, 2025",
      "categories": ["ai","mcp"],
      "tags": ["ai","mcp","resources","landscape"],
      "shortinfo": "Explore the MCP ecosystem landscape",
      "content": "MCP LandscapeThis presentation provides an overview of the Model Context Protocol (MCP) ecosystem.Visit the full screen presentation at: nikiforovall.blog/mcp-landscape.Source code: https://github.com/NikiforovAll/mcp-landscapeAgenda:  Introduction to MCP  MCP Registry: The App Store for Servers  Writing Effective MCP Tools: Production Best Practices  Building MCP Servers with .NET  MCP in Agentic SystemsReferences  https://github.com/nikiforovall/mcp-template-dotnet  https://www.anthropic.com/engineering/writing-tools-for-agents  https://modelcontextprotocol.io/docs/learn/architecture  https://github.blog/ai-and-ml/generative-ai/how-to-build-secure-and-scalable-remote-mcp-servers/"
    },
  
    {
      "id": "10",
      "title": "Testing with Playwright and Claude Code",
      "url": "/ai/2025/09/06/playwright-claude-code-testing.html",
      "date": "September 06, 2025",
      "categories": ["ai"],
      "tags": ["playwright","testing","claude","mcp","automation","ai"],
      "shortinfo": "Discover how Playwright MCP Server and Claude Code can help you perform manual and exploratory testing of web applications.",
      "content": "TL;DRLearn how to use Playwright MCP servers with Claude Code slash commands to perform manual and exploratory testing of web applications. This combination gives Claude Code ‚Äúeyes‚Äù to interact with browsers and helps bridge the gap between manual testing and automation.Source code: https://github.com/NikiforovAll/playwright-claude-code-demoIntroductionTesting web applications often requires a mix of manual and automated testing. While automated tests catch regressions, manual testing helps us discover unexpected behaviors and usability issues. But what if we could combine the best of both worlds?When you add a Playwright MCP server, Claude can directly interact with web browsers - navigate pages, click buttons, fill forms, and observe results just like a human tester would.What Makes This Combination PowerfulThe magic happens when you combine three things:  Playwright MCP Server - Gives Claude browser automation capabilities  Claude Code Slash Commands - Create reusable testing workflows  Manual Testing Approach with AI Assistance - Let Claude explore and report findings. So instead of writing detailed test scripts, you can describe what you want to test in plain English, and Claude will handle the rest.This setup is perfect for exploratory testing where you want to understand how an application behaves without writing formal test scripts first.Getting StartedFirst, you need to set up a Playwright MCP server and add following .mcp.json file to your project:{  \"mcpServers\": {    \"playwright\": {      \"type\": \"stdio\",      \"command\": \"npx\",      \"args\": [\"@playwright/mcp@latest\"],      \"env\": {}    }  }}Then configure it in your Claude Code MCP settings to connect the server. Once connected, Claude gets access to browser automation tools.Here is my .claude/settings.json file:{  \"permissions\": {    \"allow\": [      \"Read(*)\",      \"Search(*)\",      \"Edit(*)\",      \"Write(*)\",      \"mcp__playwright__browser_navigate\",      \"mcp__playwright__browser_evaluate\",      \"mcp__playwright__browser_click\",      \"mcp__playwright__browser_navigate_back\",      \"mcp__playwright__browser_take_screenshot\",      \"mcp__playwright__browser_close\"    ]  },  \"enableAllProjectMcpServers\": true,  \"enabledMcpjsonServers\": [    \"playwright\"  ]}Creating Custom Slash CommandsYou can create custom slash commands to standardize your testing workflows. Here‚Äôs an example of a manual testing command:---description: Manually test a site and create a report---### Manual Testing Instructions1. Use the Playwright MCP Server to manually test the scenario provided by the user. If no scenario is provided, ask the user to provide one.2. Navigate to the url provided by the user and perform the described interactions. If no url is provided, ask the user to provide one.3. Observe and verify the expected behavior, focusing on accessibility, UI structure, and user experience.4. Report back in clear, natural language:   - What steps you performed (navigation, interactions, assertions).   - What you observed (outcomes, UI changes, accessibility results).   - Any issues, unexpected behaviors, or accessibility concerns found.5. Reference URLs, element roles, and relevant details to support your findings.Example report format:- **Scenario:** [Brief description]- **Steps Taken:** [List of actions performed]- **Outcome:** [What happened, including any assertions or accessibility checks]- **Issues Found:** [List any problems or unexpected results]Generate a .md file with the report in the `manual-tests` directory and include any relevant screenshots or snapshots.Take screenshots or snapshots of the page if necessary to illustrate issues or confirm expected behavior.Close the browser after completing the manual test.Save this as .claude/commands/pw-manual-testing.md in your project.Real-World ExampleLet‚Äôs say you want to test a blog website. You can ask Claude:/pw-manual-testingPlease test the blog navigation on https://nikiforovall.blog/ - specifically:1. Load the homepage2. Find and click on the first blog post3. Verify the post loads correctly with all content sectionsClaude will then use the Playwright MCP server to:  Navigate to the website  Identify page elements and structure  Click through the navigation  Analyze the results  Create a detailed test reportHere‚Äôs what a typical report looks like:## Outcome Analysis### ‚úÖ Successful Results1. **Page Navigation:** Successfully navigated from homepage to specific blog post2. **URL Change:** Properly updated from homepage to post URL3. **Content Loading:** Full blog post content loaded properly including:   - Article header with publication date   - Main content sections (TL;DR, Introduction, Getting Started, etc.)   - Code examples and technical details   - Related articles sidebar### üéØ Accessibility &amp; UX Observations- **Semantic HTML:** Proper use of headings, articles, and navigation elements- **Link Accessibility:** Clear, descriptive link text for navigation- **Content Organization:** Well-structured content with clear hierarchy&lt;!-- And so on... --&gt;üí° Check https://github.com/NikiforovAll/playwright-claude-code-demo/blob/main/manual-tests/blog-post-navigation-test.md for the generated test report.From Manual Testing to AutomationOnce Claude has explored your application manually, you can ask it to generate Playwright tests based on the findings using .claude/commands/pw-generate-tests.md command:---description: Generate Playwright tests based on user scenarios---# ContextYour goal is to generate a Playwright test based on the provided scenario after completing all prescribed steps.## Your task- You are given a scenario and you need to generate a playwright test for it. If the user does not provide a scenario, you will ask them to provide one.- DO NOT generate test code based on the scenario alone.- DO run steps one by one using the tools provided by the Playwright MCP.- Only after all steps are completed, emit a Playwright TypeScript test that uses `@playwright/test` based on message history- Save generated test file in the tests directory- Execute the test file and iterate until the test passesIn our case, Claude might generate a test like this:import { test, expect } from \"@playwright/test\";test.describe(\"Blog Navigation Tests\", () =&gt; {  test(\"should navigate to blog post from homepage\", async ({ page }) =&gt; {    await page.goto(\"/\");        // Click on the first blog post title    await page.getByRole(\"link\").first().click();    // Verify blog post content is loaded    await expect(page.getByRole(\"heading\", { level: 1 })).not.toBeEmpty();  });});üí° See https://github.com/NikiforovAll/playwright-claude-code-demo/tree/main/tests for more examples of AI-generated Playwright tests.Why This Approach WorksThis combination is particularly effective because:  üëÅÔ∏è Claude can see the page - Unlike traditional testing tools, Claude can understand the visual layout and content  üå± Natural language testing - You describe what you want to test in plain English  üîé Exploratory by nature - Claude can discover unexpected issues during testing  üìñ  Documentation built-in - Every test session generates detailed reports  ü§ñ Easy transition to automation - Manual findings easily become automated tests  üìù Spec-driven - You can use tests as specifications so Claude Code can validate the code against the spec. This can dramatically improve the correctness of the generated code because Claude Code can verify the code it generates.Practical Use CasesThis setup shines in several scenarios:  New feature exploration - Test features before writing automated tests  Accessibility audits - Claude can identify accessibility issues during exploration  Cross-browser testing - Quickly test the same flows across different browsers  Regression testing - Manually verify fixes before committing to automation  User experience validation - Test actual user workflows and identify pain points  End-to-end testing - Validate complete user journeys through the applicationConclusionCombining Playwright MCP Server with Claude Code creates a powerful testing environment. You get the flexibility of manual testing with the consistency of automation, all while building documentation and formal tests along the way.The key benefit is that Claude Code gets ‚Äúeyes‚Äù to see and interact with your web applications just like a human tester would, but with the consistency and reporting capabilities that manual testing usually lacks.Try it out on your next web application - you might be surprised at what Claude discovers during its explorations!References  https://github.com/NikiforovAll/playwright-claude-code-demo  https://docs.anthropic.com/en/docs/claude-code/slash-commands  https://github.com/debs-obrien/debbie.codes/tree/main/.github/prompts  CommitQuality - Playwright Test  Playwright MCP Server by Debbie O‚ÄôBrien"
    },
  
    {
      "id": "11",
      "title": "Add Authentication to MCP Servers using Microsoft Entra ID",
      "url": "/dotnet/2025/09/02/mcp-auth.html",
      "date": "September 02, 2025",
      "categories": ["dotnet"],
      "tags": ["dotnet","ai","mcp","mcp-server"],
      "shortinfo": "Learn how to add authentication to MCP servers using Microsoft Entra ID with a ready-to-use .NET template.",
      "content": "TL;DRLearn how to add authentication to MCP servers using Microsoft Entra ID with a ready-to-use .NET template.Source code: https://github.com/NikiforovAll/mcp-template-dotnetIntroductionWithout proper authentication, sensitive data and critical business operations become vulnerable to unauthorized access, potentially leading to security breaches, data leaks, and compliance violations. This fundamental security principle applies not only to traditional Web Applications and APIs but also extends to emerging technologies and protocols such as the Model Context Protocol (MCP). Recently, the MCP Specification has been extended to include OAuth 2.1 support. This specification defines the authorization flow for HTTP-based transports.The Model Context Protocol provides authorization capabilities at the transport level, enabling MCP clients to make requests to restricted MCP servers on behalf of resource owners.üí° It is a good idea to read the specification before you proceed with this blog post. But if you are short on time, take a look at the diagram below. It shows a sequence diagram of the authorization flow between an MCP client, an MCP server (resource server), and an authorization server.sequenceDiagram    participant C as Client    participant M as MCP Server (Resource Server)    participant A as Authorization Server    C-&gt;&gt;M: MCP request without token    M--&gt;&gt;C: HTTP 401 Unauthorized with WWW-Authenticate header    Note over C: Extract resource_metadatafrom WWW-Authenticate    C-&gt;&gt;M: GET /.well-known/oauth-protected-resource    M--&gt;&gt;C: Resource metadata with authorization server URL    Note over C: Validate RS metadata,build AS metadata URL    C-&gt;&gt;A: GET Authorization server metadata endpoint    Note over C,A: Try OAuth 2.0 and OpenID Connectdiscovery endpoints in priority order    A--&gt;&gt;C: Authorization server metadata    Note over C,A: OAuth 2.1 authorization flow happens here    C-&gt;&gt;A: Token request    A--&gt;&gt;C: Access token    C-&gt;&gt;M: MCP request with access token    M--&gt;&gt;C: MCP response    Note over C,M: MCP communication continues with valid tokenWhat this means in practice is that every MCP server will now be able to provide this document in a well-known location: https://my-mcp.io/.well-known/oauth-protected-resourceMCP clients are supposed to check for well-known OAuth 2.0 Protected Resource Metadata (RFC9728) before making requests to MCP servers. This metadata will include information about the authorization servers that the MCP server trusts.{  \"resource\": \"https://localhost:7000\",  \"authorization_servers\": [    \"https://login.microsoftonline.com/9763c9ca-4551-4032-b438-43a1397592e2/v2.0\"  ],  \"bearer_methods_supported\": [    \"header\"  ],  \"scopes_supported\": [    \"api://6ae92ebf-ab73-4722-afd8-042259671aee/Mcp.User\"  ]}Getting StartedI‚Äôve prepared a dotnet new template that will help you quickly scaffold a new MCP Server project.dotnet new install Nall.ModelContextProtocol.Template# Template Name      Short Name         Language  Tags# -----------------  -----------------  --------  -------------# Template Name         Short Name            Language  Tags# --------------------  --------------------  --------  -------------# MCP Server            mcp-server            [C#]      dotnet/ai/mcp# MCP Server HTTP       mcp-server-http       [C#]      dotnet/ai/mcp# MCP Server HTTP Auth  mcp-server-http-auth  [C#]      dotnet/ai/mcp# MCP Server Hybrid     mcp-server-hybrid     [C#]      dotnet/ai/mcpNow, we can create a new MCP Server project using the template.dotnet new mcp-server-http-auth -n McpAuth -o McpAuth# The template \"MCP Server HTTP Auth\" was created successfully.Let‚Äôs take a look at Program.cs:var builder = WebApplication.CreateBuilder(args);builder.Services.AddHttpContextAccessor();builder.Services.AddScoped&lt;UserService&gt;();builder    .Services.AddAuthentication(options =&gt;    {        options.DefaultAuthenticateScheme = JwtBearerDefaults.AuthenticationScheme;        options.DefaultChallengeScheme = McpAuthenticationDefaults.AuthenticationScheme;    })    .AddMcp(options =&gt;    {        var identityOptions = builder            .Configuration.GetSection(\"AzureAd\")            .Get&lt;MicrosoftIdentityOptions&gt;()!;        options.ResourceMetadata = new ProtectedResourceMetadata        {            Resource = GetMcpServerUrl(),            AuthorizationServers = [GetAuthorizationServerUrl(identityOptions)],            ScopesSupported = [$\"api://{identityOptions.ClientId}/Mcp.User\"],        };    })    .AddMicrosoftIdentityWebApi(builder.Configuration.GetSection(\"AzureAd\"));builder.Services.AddMcpServer().WithToolsFromAssembly().WithHttpTransport();var app = builder.Build();app.UseAuthentication();app.UseAuthorization();app.MapMcp().RequireAuthorization();// Run the web serverapp.Run();// Helper method to get authorization server URLstatic Uri GetAuthorizationServerUrl(MicrosoftIdentityOptions o) =&gt; new($\"{o.Instance?.TrimEnd('/')}/{o.TenantId}/v2.0\");Uri GetMcpServerUrl() =&gt; builder.Configuration.GetValue&lt;Uri&gt;(\"McpServerUrl\");As you can see, this project template uses Microsoft.Identity.Web to authenticate users with Microsoft Entra ID (formerly Microsoft Active Directory). The interesting part here is the use of McpAuthenticationDefaults to configure the DefaultChallengeScheme. This is a special scheme that is used to challenge a user and MCP Client for credentials.‚òùÔ∏è As a developer, you don‚Äôt even need to know how this works under the hood. All you need to do is to carefully configure all components:  McpAuthenticationHandler : AuthenticationHandler&lt;McpAuthenticationOptions&gt;, IAuthenticationRequestHandler by using Microsoft.AspNetCore.Authentication.AuthenticationBuilder.AddMcp method.  ProtectedResourceMetadata is used to describe the protected resource that the MCP server exposes, including its URL, the authorization servers it trusts, and the scopes it supports.  DefaultChallengeScheme as McpAuthenticationDefaults.AuthenticationScheme. Thus, the ‚Äúdance‚Äù of OAuth 2.1 begins.Configure Microsoft Entra IDAs you may have noticed, the configuration relies on MicrosoftIdentityOptions. When you call AddMicrosoftIdentityWebApi, it automatically adds them to the DI based on configuration.{  \"AzureAd\": {    \"Instance\": \"https://login.microsoftonline.com/\",    \"Domain\": \"t4mwx.onmicrosoft.com\",    \"TenantId\": \"9763c9ca-4551-4032-b438-43a1397592e2\",    \"ClientId\": \"6ae92ebf-ab73-4722-afd8-042259671aee\",    \"ClientSecret\": \"&lt;my_very_secure_secret&gt;\"  }}To get them, you will need to create App Registrations in Azure AD.  Go to Azure and click ‚Äú+New Registration‚Äù  Specify ‚ÄúName‚Äù and ‚ÄúSupported account types‚Äù.  Add Single-page redirect URLs: https://localhost:7000/ and create a new app.  Generate a secret by using the ‚ÄúCertificates &amp; secrets‚Äù tab.  Click ‚ÄúExpose an API‚Äù and add a new scope called Mcp.Read.  Add Pre-Authorized Applications. This is the well-known ID of VSCode - ‚Äúea5a67f6-b6f3-4338-b240-c655ddc3cc8e‚Äù. Note: if you want to support additional applications, you will need to add their client IDs here.DemoCreate a .vscode/mcp.json with the following content and run the ‚ÄúEcho‚Äù MCP Server:    As you can see, we were able to authenticate to Microsoft Entra ID and obtain an access token for the MCP server. The user‚Äôs identity was successfully retrieved from the access token using UserService.public class UserService(IHttpContextAccessor httpContextAccessor){    public string? UserName =&gt; httpContextAccessor.HttpContext?.User.Identity?.Name;}And the EchoTool with injected UserService:/// &lt;summary&gt;/// This is a simple tool that echoes the message back to the client./// &lt;/summary&gt;[McpServerToolType]public class EchoTool(UserService userService){    [McpServerTool(        Name = \"Echo\",        Title = \"Echoes the message back to the client.\",        UseStructuredContent = true    )]    [Description(\"This tool echoes the message back to the client.\")]    public EchoResponse Echo(string message) =&gt;        new($\"hello {message} from {userService.UserName}\", userService.UserName!);}public record EchoResponse(string Message, string UserName);References  https://github.com/NikiforovAll/mcp-template-dotnet  https://modelcontextprotocol.io/specification/draft/basic/authorization  https://den.dev/blog/mcp-prm-auth/  https://den.dev/blog/mcp-csharp-sdk-authorization/  https://github.com/modelcontextprotocol/csharp-sdk/blob/main/samples/ProtectedMcpServer/Program.cs  https://www.youtube.com/watch?v=EXxIeOfJsqA&amp;ab_channel=MicrosoftDeveloper"
    },
  
    {
      "id": "12",
      "title": "Introducing Technical Debt Master: AI-Powered Code Analysis with Local LLMs",
      "url": "/ai/2025/08/09/tech-debt-master.html",
      "date": "August 09, 2025",
      "categories": ["ai"],
      "tags": ["ai","cli","agents","aspire","otel","developer-tools"],
      "shortinfo": "An AI-based CLI tool that automates technical debt discovery, triage, and resolution. It transforms technical debt management into a proactive and automated process",
      "content": "TL;DRIntroducing tdm, a CLI tool that automates technical debt discovery, triage, and resolution. tdm transforms technical debt management into a proactive, automated process that works seamlessly with your existing development tools through MCP integration.Source code: https://github.com/NikiforovAll/tech-debt-master  TL;DR  Motivation  tdm workflow  Let‚Äôs Get Started          Installation      Configuration        Technical Debt Discovery  Triaging Technical Debt  Technical Debt Resolution  ReferencesMotivationAs software development continues to evolve, one thing remains constant: technical debt accumulates faster than we can manage it.This is where Technical Debt Master (hereafter  tdm) comes in. With the emergence of local/open-weights LLMs like DeepSeek R1 and GPT-OSS models that can run through tools like Ollama or LM Studio, we now have an unprecedented opportunity to bring AI-powered code analysis directly to our development environments.But the advantages of using local LLMs for code analysis are compelling:  Privacy and Security: Your code never leaves your environment  Cost Efficiency: No API costs for analysis runs  Offline Capability: Work without internet connectivity  Control: Full control over model versions and behaviorAnother compelling reason to use tdm in automation. Instead of relying on developers to remember which parts of the codebase need attention or hoping that code reviews will catch accumulating problems, tdm transforms this reactive approach into a proactive, automated system that works alongside your existing development workflow. More importantly, you catch architectural problems and code quality issues before they become expensive to fix, fundamentally shifting the cost curve in your favor.tdm workflowtdm implements a structured, phase-based approach to technical debt management:  Phase1Ô∏è‚É£ - Discovery. Continuous scanning identifies new technical debt as it‚Äôs introduced and AI-powered analysis provides context and severity assessment.  Phase2Ô∏è‚É£ - Triage. This stage offers two workflows: interactive CLI for individual developers to review and prioritize issues directly in the terminal, or HTML reports for team-based collaborative sessions where developers can discuss priorities together.  Phase3Ô∏è‚É£ - Resolution. The resolution stage leverages MCP server to enable automated technical debt remediation. Once triaged, identified technical debt items are exposed through the MCP server, effectively creating a structured backlog that AI coding agents can consume and act upon. This approach provides great flexibility - you can integrate tdm with your existing AI tools such as GitHub Copilot, Cursor, Claude Code CLI, Gemini CLI, etc.Let‚Äôs Get StartedInstallationInstall tdm from source code:dotnet cake --target packdotnet tool install --global --add-source ./Artefacts TechDebtMaster.CliYou can use tdm help to get a helpful reference for all available commands.tdm help# Available Commands:# Repository Management# repo                      Repository management and indexing operations#   ‚îú‚îÄ index [path]         Index repository content#   ‚îî‚îÄ status [path]        Show status of previous analysis and repository changes# Debt Analysis# debt                      Technical debt analysis and reporting#   ‚îú‚îÄ analyze [path]       Perform debt analysis on all indexed files#   ‚îú‚îÄ show [path]          Show technical debt statistics in a tree structure grouped by tags#   ‚îú‚îÄ view [path]          View detailed content of specific technical debt items#   ‚îú‚îÄ report [path]        Generate an interactive HTML report of technical debt#   ‚îî‚îÄ import [report-file] Import modified HTML report to update analysis data# System Management# init                      Initialize tdm in the current repository# config                    Manage configuration settings#   ‚îú‚îÄ show                 Display current configuration#   ‚îî‚îÄ set [key] [value]    Set a configuration value# prompts                   Manage prompt templates#   ‚îú‚îÄ edit                 Edit a prompt template#   ‚îú‚îÄ restore              Restore prompt templates to default state#   ‚îî‚îÄ set-default          Set the default prompt template# mcp                       Start Model Context Protocol server# clean                     Remove the .tdm folder from the current directory# help                      Show this detailed help informationConfigurationYou can manage tdm configuration settings using the tdm config command. This allows you to customize various aspects of the tool to better fit your needs.tdm integrates with OpenTelemetry to export detailed telemetry data including token consumption, model inference times, and per-analysis cost breakdowns, providing precise visibility into codebase analysis costs. In this blog post I will use it in conjunction with the .NET Aspire Dashboard. Also, we will use Aspire to work with Ollama to deploy local LLM models.var builder = DistributedApplication.CreateBuilder(args);var ollama = builder    .AddOllama(\"ollama\")    .WithImageTag(\"0.6.0\")    .WithOpenWebUI(ui =&gt; ui.WithImageTag(\"0.5.20\"))    .WithDataVolume()    .WithLifetime(ContainerLifetime.Persistent);    var r1 = ollama.AddModel(\"deepseek-r1\", \"deepseek-r1:1.5b\");// (alternatively) if you have free 16GB of RAM you can try something likevar gpt = ollama.AddModel(\"gpt-oss\", \"gpt-oss:20b\");// (optional) we use it to get a connecting string and otel configurationbuilder    .AddProject&lt;Projects.TechDebtMaster_Cli&gt;(\"tdm\")    .WithArgs(\"--\", \"help\")    .WithReference(r1);builder.Build().Run();Now, we can run it using Aspire CLI:aspire runNow we can configure tdm:OTEL_EXPORTER_OTLP_ENDPOINT=https://localhost:21050OTEL_EXPORTER_OTLP_HEADERS=x-otlp-api-key=ddc6b5e2c7f7ef486697b3a60a9aee52OTEL_EXPORTER_OTLP_PROTOCOL=grpctdm config set ai.provider ollamatdm config set ai.url http://localhost:62604tdm config set ai.model 'deepseek-r1:1.5b'tdm config set default.include '\\.cs$' # we will use it with C# code base, so it makes sense to analyze only C# filesThe beautiful part about tdm is that you can customize analysis prompts to better fit your project‚Äôs needs. Run tdm prompts edit to open the prompt in your default system editor.üí° For example, you can configure tdm to identify specifically security-related issues by creating dedicated security.prompty file. tdm leverages Prompty for prompt management.Technical Debt DiscoveryInitialize tdm and index the repository with technical debt. In our case it is called tdm-testproject-monkeymcp (some project with technical debt)tdm init# ‚àö Updated .gitignore to include .tdm folder# ‚àö TechDebtMaster initialization complete!tdm repo index# Analyzing repository: .# Include pattern (from default.include): \\.cs$ (only files matching this pattern will be analyzed)# ‚àö Repository analyzed successfully!tdm repo status# Analysis Status for: .# Last analyzed: 2025-08-09 15:53:45 UTC# Recent Changes:# New files (6):#   + MonkeyMCP/Program.cs#   + MonkeyMCPShared/MonkeyPrompts.cs#   + MonkeyMCPShared/MonkeyResources.cs#   + MonkeyMCPShared/MonkeyTools.cs#   + MonkeyMCPSSE/Program.cs#   ... and 1 moreNow we are ready to perform technical debt analysis for the configured prompt using.tdm debt analyzeHere is an example of data we can find in the Aspire Dashboard, each command has a separate trace:üí° Each command has a separate traceüí° You can drill down to see individual traces for find per-file analysis traces.üí° Metrics give you insights into the overall LLM usage.Once we have a backlog of technical debt identified, we can prioritize and address it effectively.Triaging Technical DebtTriaging technical debt means prioritizing identified issues based on business impact and remediation effort. tdm enables effective team triaging through structured reports where developers can collaboratively assess technical complexity against business priorities.Use tdm debt show to get a tree view of technical debt items:tdm debt showYou can work with items using tdm debt view --interactive command:tdm debt view --interactive    üí° In the demo above, I showcased how to interactively view, manage, and export backlog items in various formats. For example, using --xml can be useful if you prefer not to use MCP and instead want to export backlog items in a more portable format. Similarly, a command like tdm debt show --json would also work.If you are not a CLI-type of person, you can use tdm debt report to open a web-based interface for managing your technical debt. The neat trick here is that you can actually save the report once you are done and import it back to tdm.tdm debt report -o report.html --open    üí° Use tdm debt import report.html --apply to export modified report back to tdm.Technical Debt ResolutionNow, we can use MCP integration to work on technical debt resolution. tdm provides out of the box integration/prompts for popular tools like GitHub Copilot, Claude Code, Gemini CLI.tdm init --profile vscode --force# ‚àö Created .vscode/mcp.json configuration# ‚àö Created .github/prompts/tdm-work-on-debt.prompt.md# ‚àö Updated .gitignore to include .tdm folder# ‚àö TechDebtMaster initialization complete!# You can now start the MCP server with: tdm mcpThis initialization command produces the necessary configuration files for integrating with the MCP server.tdm offers complete control over agent behavior through customizable prompt files (e.g.: .github/prompts/tdm-work-on-debt.prompt.md). These files define how AI coding agents interact with your technical debt backlog via the MCP interface. You can tailor prompts to align with your team‚Äôs coding standards, preferences, and risk tolerance. This flexibility allows you to decide whether agents should automatically resolve straightforward issues or require human approval for more complex changes. The prompt-based configuration enables you to adjust the level of automation to suit different scenarios. For those feeling adventurous, you can even run a coding agent like Claude Code in ‚ÄúYOLO mode‚Äù with a simple prompt such as ‚Äúresolve technical debt.‚Äù. The benefit of using tdm in this case is that coding agent can focus on resolving exactly technical debt you triaged previously.For example, here is a default prompt generated during vscode profile initialization:---mode: agenttools: ['changes', 'codebase', 'editFiles', 'fetch', 'findTestFiles', 'problems', 'runCommands', 'runTasks', 'search', 'searchResults', 'terminalLastCommand', 'terminalSelection', 'testFailure', 'usages', 'tdm-get-item', 'tdm-list-items', 'tdm-remove-item', 'tdm-show-repo-stats']description: 'An autonomous workflow for identifying, analyzing, and resolving technical debt in a codebase to improve maintainability and efficiency.'---## WorkflowExecute the following workflow to systematically address technical debt:### 1. Assessment Phase- Use `tdm-show-repo-stats` to gather repository-wide technical debt metrics### 2. Prioritization Phase- Use `tdm-list-items` to retrieve first page of technical debt items### 3. Verification Phase- For each item in the list:- Use `tdm-get-item` to fetch detailed information about the item- Present the item to the user for review- Ask user for confirmation to proceed with the item### 4. Resolution Phase- Use `tdm-get-item` to fetch detailed item information- Present user with the item- Analyze item validity:- Review related code- Verify if debt is still relevant- Document investigation findings- For each valid item:- Implement necessary fixes- Remove resolved items using `tdm-remove-item`### 5. Validation Requirements- Ensure all changes maintain existing functionality- Document any architectural decisions- Request human review for complex changes- Once an item is resolved or is no longer relevant, remove it from the list using 'tdm-remove-item'.ü§ñLet‚Äôs see it in practice:tdm mcp# Starting MCP server for repository: .# Server will listen on: http://localhost:3001# Press Ctrl+C to stop the server# ‚àö MCP server started successfullyAnd run /tdm-work-on-debt  prompt for GitHub Copilot chat window:    Source code: github.com/NikiforovAll/tech-debt-masterIf you have feedback, questions, or want to contribute, feel free to open an issue or pull request on GitHub.References  https://github.com/NikiforovAll/tech-debt-master  https://openai.com/open-models/  https://huggingface.co/openai/gpt-oss-20b"
    },
  
    {
      "id": "13",
      "title": "Monitoring Claude Code with .NET Aspire Dashboard",
      "url": "/productivity/2025/06/14/claude-code-telemetry-aspire.html",
      "date": "June 14, 2025",
      "categories": ["productivity"],
      "tags": ["productivity","ai","claude","agents","aspire","telemetry","monitoring"],
      "shortinfo": "Learn how to monitor your Claude Code usage and performance with .NET Aspire dashboard.",
      "content": "IntroductionClaude Code is a powerful AI coding assistant that can help you with various development tasks. It support telemetry monitoring, so I thought it would be fun to set up a .NET Aspire dashboard to visualize my usage and see how claude operates under the hood.Source code: github.com/NikiforovAll/claude-code-rulesSetting Up .NET Aspire DashboardThe first step is to run the Aspire dashboard. You can do this easily with Podman or Docker:podman run --rm -it -d \\    -p 18888:18888 \\    -p 4317:18889 \\    --name aspire-dashboard \\    mcr.microsoft.com/dotnet/aspire-dashboardThis command starts the Aspire dashboard with the following configuration:  Dashboard UI accessible at port 18888  OpenTelemetry collector at port 4317Configuring Claude Code TelemetryNext, we need to configure Claude Code to send telemetry data to our Aspire dashboard. This is done through environment variables.Create an .env file with the following content:export CLAUDE_CODE_ENABLE_TELEMETRY=1export OTEL_LOG_USER_PROMPTS=1export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317export OTEL_EXPORTER_OTLP_PROTOCOL=grpcexport OTEL_LOGS_EXPORTER=otlpexport OTEL_LOGS_EXPORT_INTERVAL=5000export OTEL_EXPORTER_OTLP_METRICS_PROTOCOL=grpcexport OTEL_METRICS_EXPORTER=otlpexport OTEL_METRIC_EXPORT_INTERVAL=10000export OTEL_SERVICE_NAME=claude-codeexport OTEL_RESOURCE_ATTRIBUTES=service.instance.id=nikiforovallThen load these environment variables in your terminal:export $(grep -v '^#' .env | xargs)See Monitoring Usage for more details on how to configure telemetry for Claude Code.Using the DashboardWith everything set up, you can now:  Open your browser and navigate to http://localhost:18888  Use Claude Code as you normally would  Watch as telemetry data appears in the Aspire dashboardThe dashboard will show various metrics and logs that can help you understand:  What kind of models and tools are being used  How often Claude Code is invoked  What prompts are being sent  Token usage patterns  Any errors or issues that occur    Benefits of MonitoringSetting up telemetry monitoring for Claude Code offers several advantages:  Usage Insights: Understand how Claude Code performs in real-world scenarios.  Resource Management: Monitor token usage and adjust your subscription if necessary."
    },
  
    {
      "id": "14",
      "title": "My Claude Code Usage Best Practices and Recommendations",
      "url": "/productivity/2025/06/13/claude-code-rules.html",
      "date": "June 13, 2025",
      "categories": ["productivity"],
      "tags": ["productivity","ai","claude","agents"],
      "shortinfo": "In this post, I share my collection of practical recommendations and principles for using Claude Code.",
      "content": "This post shares my collection of practical recommendations and principles for using Claude Code. For more details and the full source code, check out my repository:Source code: github.com/NikiforovAll/claude-code-rulesPractical RecommendationsHere is my list of practical recommendations for using Claude Code:Planning  Ask Claude to brainstorm ideas and iterate on them. Later, these ideas can be used as grounding context for your prompts.  plan mode vs auto-accept mode vs edit mode:          Verify what is about to be performed using plan mode.      Once verified, proceed with auto-accept mode.      Step-by-step mode is the default mode with no auto-accept.        Workflows:          a. Explore, plan, code, commit.      b. Write tests, commit; code, iterate, commit.      c. Write code, screenshot results, iterate.        Ask ‚Äúthink hard‚Äù to trigger deep thinking:          ‚Äúthink‚Äù &lt; ‚Äúthink hard‚Äù &lt; ‚Äúthink harder‚Äù &lt; ‚Äúultrathink‚Äù      AI Task-Based Development  Write a plan to an external source (e.g., file - plan.md) and use it as a checklist.  plan.prompt.md - use an external file as memory for task management and planning.I‚Äôve created a set of commands to help with AI task-based development:  Use /create-prd to create a Product Requirements Document (PRD) based on user input.  Use /generate-tasks to create a task list from the PRD.  Use /process-task-list to manage and track task progress.Project structure looks like this:tree -a# .# ‚îú‚îÄ‚îÄ .claude# ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ commands# ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ create-prd.md# ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ generate-tasks.md# ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ process-task-list.md# ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ settings.local.json# ‚îú‚îÄ‚îÄ .mcp.json# ‚îî‚îÄ‚îÄ README.mdFor more details, please refer to source code.Knowledge Mining / Grounding  Use the /init command to initialize the CLAUDE.md file. There‚Äôs no required format for CLAUDE.md files. I recommend keeping it concise and human-readable. You can use it to store important information about your project, such as architecture, design decisions, and other relevant details that can help Claude understand your codebase better.  CLAUDE.md can open other files like this: @path/to/import. Be careful, as this file is attached every time you submit a prompt.  There are two different types:          Project memory ./CLAUDE.md - share it with your team.      User memory ~/.claude/CLAUDE.md - personal preferences.        You can use # &lt;text&gt; to add particular memory to the CLAUDE.md file.      Use /memory to edit the memory file directly. It will open the file in your default editor.    Codebase Q&amp;A: Use Claude Code to ask questions about your codebase. It can help during onboarding or when you need to understand how something works.  Use hints, reference files, provide examples, mention documentation, and provide links.Miscellaneous  Use in ‚Äúpipe‚Äù mode, as Unix philosophy utils: claude -p \"\" or echo '' | claude -p \"\".  üóëÔ∏è /clear and /compact &lt;specific prompt for aggregation&gt; can be very helpful.  üß† If you don‚Äôt know something about Claude Code, ask it! It‚Äôs self-aware.          E.g., What kind of tools do you have? Can you perform a web search?      MCP ServersYou can use MCP servers. See claude-code/mcps.Here is an example of how to setup MCP servers, just create a .mcp.json file in your project root:{  \"mcpServers\": {    \"microsoft.docs.mcp\": {      \"type\": \"http\",      \"url\": \"https://learn.microsoft.com/api/mcp\"    },    \"context7\": {      \"type\": \"stdio\",      \"command\": \"npx\",      \"args\": [        \"-y\",        \"@upstash/context7-mcp@latest\"      ]    }  }}üéÅ Bonus: Turn Claude Code into an Interactive Tutor with Microsoft Docs &amp; Context7You can supercharge Claude Code by integrating it with Microsoft Docs and Context7. It can be useful for learning and development tasks.    Useful Links  ‚≠ê Learn best practices for Claude Code: engineering/claude-code-best-practices.  ‚≠ê Tutorials - https://docs.anthropic.com/en/docs/claude-code/tutorials  Explore common use cases for Claude Code: claude-code/common-tasks.  CLI Usage - https://docs.anthropic.com/en/docs/claude-code/cli-usage  Claude Code Memory - https://docs.anthropic.com/en/docs/claude-code/memory  General Prompt Engineering with Claude Models - https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview  Interactive Tutorial for Prompt Engineering - https://github.com/anthropics/prompt-eng-interactive-tutorialIf you have questions or want to see more, check out the GitHub repository or leave a comment below!"
    },
  
    {
      "id": "15",
      "title": "Hangfire MCP Server in Standalone Mode",
      "url": "/dotnet/2025/05/29/hangfire-mcp-standalone.html",
      "date": "May 29, 2025",
      "categories": ["dotnet"],
      "tags": ["dotnet","ai","hangfire","mcp","mcp-server"],
      "shortinfo": "Enqueue background jobs using the Standalone Hangfire MCP server.",
      "content": "TL;DREnqueue background jobs using the Hangfire MCP server in ‚Äústandalone‚Äù mode using the .NET global tool.Source code: https://github.com/NikiforovAll/hangfire-mcp    In my previous blog post, I showed you how to set up a Hangfire MCP server in a .NET application. We built Hangfire MCP from scratch, and as a prerequisite, you need to reference an assembly that contains Hangfire jobs directly in the Hangfire MCP project. This is a great approach if you want to extend the Hangfire MCP server‚Äôs capabilities, but it is not very convenient if you just want to use the Hangfire MCP server to enqueue background jobs.In this post, I will show you how to use the Hangfire MCP server in ‚Äústandalone‚Äù mode using the .NET global tool. Here is a NuGet package that I created for this purpose: Nall.HangfireMCP.Here is how to setup it as an MCP server in VSCode:dotnet tool install --global Nall.HangfireMCP# You can invoke the tool using the following command: HangfireMCP# Tool 'nall.hangfiremcp' (version '1.0.0') was successfully installed.All we need to do is configure the mcp.json file to add the Hangfire MCP server and set the environment variables to specify the Hangfire jobs assembly, the job discovery expression, and the connection string for Hangfire.{  \"servers\": {    \"hangfire-mcp-standalone\": {      \"type\": \"stdio\",      \"command\": \"HangfireMCP\",      \"args\": [        \"--stdio\"      ],      \"env\": {        \"HANGFIRE_JOBS_ASSEMBLY\": \"path/to/Jobs.dll\",        \"HANGFIRE_JOBS_MATCH_EXPRESSION\": \"[?IsInterface &amp;&amp; contains(Name, 'Job')]\",        \"HANGFIRE_CONNECTION_STRING\": \"Host=localhost;Port=5432;Username=postgres;Password=postgres;Database=hangfire\"      }    }  }}As a result, the jobs are dynamically loaded from the specified assembly and can be enqueued using the MCP protocol. The rules for matching job names can be specified using the HANGFIRE_JOBS_MATCH_EXPRESSION environment variable. For example, the expression [?IsInterface &amp;&amp; contains(Name, 'Job')] will match all interfaces that contain ‚ÄúJob‚Äù in their name. It is a JMESPath expression, so you can define how to match job names according to your needs.Aspire (Bonus)Here is how to use the Hangfire MCP server in Standalone Mode using Aspire:var builder = DistributedApplication.CreateBuilder(args);var postgresServer = builder    .AddPostgres(\"postgres-server\")    .WithDataVolume()    .WithLifetime(ContainerLifetime.Persistent);var postgresDatabase = postgresServer.AddDatabase(\"hangfire\");builder.AddProject&lt;Projects.Web&gt;(\"server\")    .WithReference(postgresDatabase)    .WaitFor(postgresDatabase);var mcp = builder    .AddProject&lt;Projects.HangfireMCP_Standalone&gt;(\"hangfire-mcp\")    .WithEnvironment(\"HANGFIRE_JOBS_ASSEMBLY\", \"path/to/Jobs.dll\")    .WithEnvironment(\"HANGFIRE_JOBS_MATCH_EXPRESSION\", \"[?IsInterface &amp;&amp; contains(Name, 'Job')]\")    .WithReference(postgresDatabase)    .WaitFor(postgresDatabase);builder    .AddMCPInspector()    .WithSSE(mcp)    .WaitFor(mcp);builder.Build().Run();ConclusionThe Hangfire MCP server can be used in standalone mode to enqueue background jobs without the need to write custom code. This provides greater flexibility and ease of use, especially when you want to quickly set up a Hangfire MCP server for job processing.References  https://nikiforovall.blog/dotnet/2025/05/25/hangfire-mcp.html  https://github.com/NikiforovAll/hangfire-mcp  modelcontextprotocol/csharp-sdk"
    },
  
    {
      "id": "16",
      "title": "Background Job Scheduling Using Hangfire MCP Server",
      "url": "/dotnet/2025/05/25/hangfire-mcp.html",
      "date": "May 25, 2025",
      "categories": ["dotnet"],
      "tags": ["dotnet","ai","hangfire","mcp","mcp-server"],
      "shortinfo": "Enqueue background jobs using Hangfire MCP server.",
      "content": "TL;DREnqueue background jobs using the Hangfire MCP server. Source code: https://github.com/NikiforovAll/hangfire-mcp    MotivationI like Hangfire for background job processing in .NET applications. It has a simple and intuitive API, a powerful dashboard, and supports various storage options. Essentially, it provides everything I need for real-world applications that require background processing. However, sometimes I need to run a job with parameters in the background, and there isn‚Äôt an easy way to do it directly from the Hangfire dashboard.ü§î So, I had an idea to schedule Hangfire jobs through an MCP server. This is exactly the use case for an MCP server.  MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.Setup MCP ServerIf you want to set up your own Hangfire MCP Server, you can use the Nall.ModelContextProtocol.Template template that I shared with you in the previous post.At a high level, you need to do the following:  Set up the MCP server.  Register McpServerTool.  Configure job discovery.  Connect the MCP server to Hangfire using a connection string.  Configure the MCP client to consume the Hangfire MCP server. For example, in VS Code, you can configure the mcp.json file to add your server.Code Deep DiveLet‚Äôs see how this whole thing is composed by reviewing the AppHost/Program.cs file:var builder = DistributedApplication.CreateBuilder(args);var postgresServer = builder.AddPostgres(\"postgres-server\").WithDataVolume();var postgresDatabase = postgresServer.AddDatabase(\"hangfire\");builder.AddProject&lt;Projects.Web&gt;(\"server\")    .WithReference(postgresDatabase)    .WaitFor(postgresDatabase);var mcp = builder.AddProject&lt;Projects.HangfireMCP&gt;(\"hangfire-mcp\")    .WithReference(postgresDatabase)    .WaitFor(postgresDatabase);builder.AddMCPInspector().WithSSE(mcp);builder.Build().Run();In the above code, we are adding the following components:  Postgres Server: This is where Hangfire will store its data.  Web Project (aka Server): This is the main web application that will perform background job processing.  Hangfire MCP Project: This is the MCP server that will expose Hangfire jobs as MCP commands.  MCP Inspector: This is the MCP inspector that will allow us to interact with the Hangfire MCP server. This is very useful for debugging and testing purposes.üöÄ You can clone the repository (https://github.com/NikiforovAll/hangfire-mcp) and run the project to see how it works.aspire run --project ./samples/AppHost/AppHost.csprojHere is how the Aspire Dashboard looks:From a high-level perspective, here is how it works:sequenceDiagram    participant User as User    participant MCPHangfire as MCP Hangfire    participant IBackgroundJobClient as IBackgroundJobClient    participant Database as Database    participant HangfireServer as Hangfire Server    User-&gt;&gt;MCPHangfire: Enqueue Job (via MCP Client)    MCPHangfire-&gt;&gt;IBackgroundJobClient: Send Job Message    IBackgroundJobClient-&gt;&gt;Database: Store Job Message    HangfireServer-&gt;&gt;Database: Fetch Job Message    HangfireServer-&gt;&gt;HangfireServer: Process JobHangfire MCP ServerThis project is quite straightforward. We just need to map the MCP tool to a Hangfire job and call it a day. Here is how it looks:[McpServerToolType]public class HangfireTool(IHangfireDynamicScheduler scheduler){    [McpServerTool(Name = \"RunJob\")]    public string Run(        [Required] string jobName,        [Required] string methodName,        Dictionary&lt;string, object&gt;? parameters = null    )    {        var assembly = typeof(ITimeJob).Assembly; // &lt;-- it should point to the assembly where your Hangfire jobs are defined.        return scheduler.Enqueue(new(jobName, methodName, parameters), assembly);    }}This code defines a Hangfire MCP tool that allows us to enqueue jobs using the MCP protocol. The Run method takes the job name, method name, and optional parameters, and enqueues the job using the IHangfireDynamicScheduler. The IHangfireDynamicScheduler is a custom scheduler that uses reflection to find the job method in the specified assembly and enqueue it.üí° You can find the implementation of IHangfireDynamicScheduler in the source code of the project.Now, you can ask GitHub Copilot to enqueue a job using something like this:Please run the following job: #RunJob{    \"jobName\": \"HangfireJobs.ISendMessageJob\",    \"methodName\": \"ExecuteAsync\",    \"parameters\": {        \"text\": \"Hello, MCP!\"    }}Note that in this case, we need to specify the exact job name and method name, as well as the parameters. Wouldn‚Äôt it be great if we could discover the jobs by asking Copilot? Let‚Äôs see how we can do that.[McpServerTool(Name = \"ListJobs\"), Description(\"Lists all jobs\")][return: Description(\"An array of job descriptors in JSON format\")]public string ListJobs(){    var jobs = scheduler.DiscoverJobs(        type =&gt; type.IsInterface &amp;&amp; type.Name.EndsWith(\"Job\", StringComparison.OrdinalIgnoreCase),        typeof(ITimeJob).Assembly // &lt;-- it should point to the assembly where your Hangfire jobs are defined.    );    return JsonSerializer.Serialize(jobs);}In the code above, we define a ListJobs method that returns a list of all jobs. The DiscoverJobs method uses reflection to find all jobs in the specified assembly.The idea here is that you can define rules for job discovery that are specific to your application. In this case, we are matching interfaces that end with Job. In my demo application, I have two jobs defined: ITimeJob and ISendMessageJob. Here is how they look:public interface ITimeJob{    public Task ExecuteAsync();}public interface ISendMessageJob{    public Task ExecuteAsync(string text); // You can specify parameters    public Task ExecuteAsync(Message message); // Works with complex types as well}Let‚Äôs take a look at the MCP server from the MCP Inspector:As you can see, we have a bunch of tools available in the MCP Inspector. We can list jobs, enqueue jobs, get job status by job ID, and requeue jobs. The MCP Inspector allows us to interact with the Hangfire MCP server, making it a nice way to test and debug the MCP server.ConclusionNow you can easily enqueue Hangfire jobs using MCP. You can use it to schedule jobs from any MCP client, such as VS Code, or even from your own custom application. If you want to see the end-to-end demo, you can check out the video at the beginning of this post.References  modelcontextprotocol/csharp-sdk"
    },
  
    {
      "id": "17",
      "title": "Code Review with GitHub Copilot in Visual Studio Code",
      "url": "/productivity/2025/05/03/github-copilot-prompt-engineering-code-review.html",
      "date": "May 03, 2025",
      "categories": ["productivity"],
      "tags": ["productivity","ai","copilot","agents"],
      "shortinfo": "In this blog post, I will show you how to leverage GitHub Copilot's code review capabilities in Visual Studio Code. In addition to the built-in features, I'll introduce my own...",
      "content": "TL;DR  TL;DR  Introduction  Code Review Custom Instructions  Code Review in Agent Mode          Crafting the Review Prompt      Demo        Conclusion  ReferencesIn this blog post, I will show you how to leverage GitHub Copilot‚Äôs code review capabilities in Visual Studio Code. In addition to the built-in features, I‚Äôll introduce my own agent-based code review workflow using a custom prompt. This approach leverages Copilot‚Äôs flexibility to tailor the review process to your team‚Äôs needs.Source code: https://github.com/NikiforovAll/dotnet-copilot-rulesIntroductionCode review has always been a cornerstone of high-quality software development, but its importance has only grown with the advent of LLM-based development tools like GitHub Copilot. As AI-generated code becomes more prevalent, careful reading and thorough review are essential to ensure correctness, maintainability, and security.ü§ñ With the latest updates, Copilot code review is seamlessly integrated into Visual Studio Code, providing instant, AI-powered feedback directly in your editor.Some of the key features include:  Review Selection: Highlight any section of code and request an initial review. Copilot will analyze the selected code and provide feedback, including suggestions for improvements or fixes, which you can apply with a single click.  Review Changes: Request a deeper review of all your staged or unstaged changes directly from the Source Control tab. Copilot will review your modifications and surface comments or suggestions inline and in the Problems tab.üìÉ Find out more about these features in the GitHub Copilot documentation.Code Review Custom InstructionsI already described how to ground Copilot Code Review with custom instructions in my previous post - Enforcing .NET Coding Guidelines with GitHub Copilot Custom Instructions. Basically, it boils down to something like this:{    \"github.copilot.chat.reviewSelection.enabled\": true,    \"github.copilot.chat.reviewSelection.instructions\": [        {            \"file\": \".vscode/rules/csharp/coding-guidelines.md\"        }    ]}Code Review in Agent ModeBeyond the built-in features, I‚Äôve crafted a custom code review prompt designed for agent mode. This approach aims to replicate the functionality of the Pull Request Review from Copilot.While the standard Copilot Pull Request Review is excellent, it has certain limitations:  It‚Äôs restricted to the Web UI, requiring users to leave their IDE.  It cannot be used outside the GitHub environment.  It offers less capability compared to the Agent mode, which allows for the use of MCPs, model switching, etc.  üéØ To address these limitations, I suggest you to try out my custom code review prompt. This prompt is designed to be used in the agent mode, allowing you to leverage the full power of Copilot while keeping your workflow within Visual Studio Code.Crafting the Review PromptHere is the prompt I use for code reviews:üöÄ https://github.com/NikiforovAll/dotnet-copilot-rules/blob/main/.vscode/prompts/code-review.prompt.mdThe prompt has two logical sections:  Description: This section describes the purpose of the prompt and sets the context for the review.     --- description: Perform a code review --- ## Code Review Expert: Detailed Analysis and Best Practices As a senior software engineer with expertise in code quality, security, and performance optimization, perform a code review of the provided git diff.  Focus on delivering actionable feedback in the following areas: (Skipped for brevity) Format your review using clear sections and bullet points. Include inline code references where applicable. Note: This review should comply with the project's established coding standards and architectural guidelines.        Constraints: This section outlines the specific constraints and guidelines that the code should adhere to.     ## Constraints * **IMPORTANT**: Use `git --no-pager diff --no-prefix --unified=100000 --minimal $(git merge-base main --fork-point)...head` to get the diff for code review. * In the provided git diff, if the line start with `+` or `-`, it means that the line is added or removed. If the line starts with a space, it means that the line is unchanged. If the line starts with `@@`, it means that the line is a hunk header. * Use markdown for each suggestion, like     ```     # Code Review for ${feature_description}     Overview of the code changes, including the purpose of the feature, any relevant context, and the files involved.     # Suggestions     ## ${code_review_emoji} ${Summary of the suggestion, include necessary context to understand suggestion}     * **Priority**: ${priority: (üî•/‚ö†Ô∏è/üü°/üü¢)}     * **File**: ${relative/path/to/file}     * **Details**: ...     * **Example** (if applicable): ...     * **Suggested Change** (if applicable): (code snippet...)             ## (other suggestions...)     ...     # Summary     ``` * Use the following emojis to indicate the priority of the suggestions:     * üî• Critical     * ‚ö†Ô∏è High     * üü° Medium     * üü¢ Low * Each suggestion should be prefixed with an emoji to indicate the type of suggestion:     * üîß Change request     * ‚ùì Question     * ‚õèÔ∏è Nitpick     * ‚ôªÔ∏è Refactor suggestion     * üí≠ Thought process or concern     * üëç Positive feedback     * üìù Explanatory note or fun fact     * üå± Observation for future consideration      Basically, we ask Copilot to get the diff by running the git diff command and perform a code review based on the output from the terminal.git --no-pager diff --no-prefix --unified=100000 --minimal $(git merge-base main --fork-point)...headDemoLet‚Äôs say we have a simple program that counts the number of capital letters in each line of a text file. We can use the code-review.prompt.md file to analyze the proposed solution.string filePath = \"path/to/your/file.txt\";FileStream fileStream = new FileStream(filePath, FileMode.Open, FileAccess.Read);StreamReader reader = new StreamReader(fileStream);List&lt;string&gt; lines = new List&lt;string&gt;();while (!reader.EndOfStream){    string line = await reader.ReadLineAsync();    lines.Add($\"{new Guid()} - {line}[{CountCapitalLetters(line)}]\");}foreach (string line in lines){    Console.WriteLine(line);}// Counts the number of capital latters in a string, the typo in word 'latters' is intentionalint CountCapitalLetters(string input, string param1 = default, int param2 = 0, bool param3 = false, object param4 = null){    int count = 0;    foreach (char c in input)    {        if (char.IsUpper(c))        {            count++;        }    }    return count;}We can attach the prompt by CTRL + ALT + / and select the code-review.prompt.md file. Then, we can ask Copilot to follow the instructions in the prompt and perform a code review.    ‚úÖ And here is the result of the code review:  I would say that the review is quite good. It is a good starting point to keep chatting with Copilot and ask for more details, or even to ask it to rewrite the code based on the suggestions.üí° The great thing about this is that you can tailor the prompt to your specific needs. You can add or remove sections, change the wording, reference the project‚Äôs coding standards, etc.üí° Treat Copilot as a coworker: start ‚ÄúVoice Chat‚Äù mode and engage in a conversation just as you would with a teammate. Ask questions, discuss suggestions, and iterate on solutions together to get the most out of your code review.Conclusion  üôå I hope you found it helpful. If you have any questions, please feel free to reach out. If you‚Äôd like to support my work, a star on GitHub would be greatly appreciated! üôèReferences  Prompt engineering for Copilot Chat  GitHub Copilot Customization  Prompt Engineering Guide  Using GitHub Copilot code review"
    },
  
    {
      "id": "18",
      "title": "Prompt Engineering with GitHub Copilot in Visual Studio Code",
      "url": "/productivity/2025/04/19/github-copilot-prompt-engineering.html",
      "date": "April 19, 2025",
      "categories": ["productivity"],
      "tags": ["productivity","ai","copilot","agents"],
      "shortinfo": "Learn how to leverage reusable prompts in GitHub Copilot to enhance your productivity in Visual Studio Code.",
      "content": "TL;DRLearn how to leverage reusable prompts in GitHub Copilot to enhance your productivity in Visual Studio Code.Source code: https://github.com/NikiforovAll/dotnet-copilot-rulesIntroductionPrompt engineering in the right context can be a very powerful technique when working with LLMs.In this blog post, I will not dive into the details of prompt engineering. For more information on prompt engineering, check out the Prompt Engineering Guide.Why Prompt Engineering?In the context of Visual Studio Code and GitHub Copilot, prompt engineering allows developers to create reusable instructions that streamline workflows, enforce coding standards, and improve collaboration. By leveraging reusable prompts, you can enhance Copilot‚Äôs ability to produce meaningful suggestions tailored to your specific needs.Reusable Prompt Files in GitHub CopilotGitHub Copilot introduces the concept of reusable prompt files. These files allow you to define task-specific instructions in a structured and reusable way.You can define prompts at the workspace level, making them accessible to all team members. Or you can create them at the user level, making them available across all your projects.Setting Up Reusable Prompt Files for WorkspacesEnable Prompt Files:  Open your .vscode/settings.json  Set the chat.promptFiles setting to true to enable reusable prompt files.{   \"chat.promptFiles\": true,   \"chat.promptFilesLocations\": {      \".github/prompts\": true,      \".vscode/prompts\": true   }}Now you can create a folder named .github/prompts or .vscode/prompts in your workspace. Inside this folder, you can create prompt files with the .prompt.md extension.Later, you can attach these prompt files as context to your prompt.ExampleHere is an example of my pros-and-cons.prompt.md file:---description: Pros and Cons Analysis---# Definition:Analyze the proposed solution, focusing on its strengths and weaknesses. Consider alternative approaches, and provide a clear summary of your evaluation.## Constraints- Ask follow-up questions if needed to clarify the solution. - If any questions arise, wait for the user to respond before proceeding with the analysis.- Provide Pros and Cons in a bulleted list format.- Highlight best practices and common pitfalls, where relevant.- Suggest alternative solutions or improvements, if applicable.- Provide pros and cons for alternative solutions if applicable. Provide at least 3 pros and cons for each alternative solution.- Use provided emojis below to enhance readability:- Use ‚úÖ to highlight pros in the list.- Use ‚ùå to indicate cons in the list.- Use ‚ú® to highlight best practice items in the list.- Use ‚òùÔ∏è to indicate common pitfalls.- Use üîí to highlight security (InfoSec)-related topics.- End with a single-paragraph summary of your overall assessment.## Structure- **Proposed Solution**: &lt;NAME&gt;- **Description**: &lt;DESCRIPTION&gt;- **Pros**:- **Cons**:- **Best Practices**:- **Common Pitfalls**:- **Alternative Solution**: &lt;NAME&gt;- **Description**: &lt;DESCRIPTION&gt;- **Pros**:- **Cons**:&lt;!-- ... --&gt;üöÄ DemoLet‚Äôs say we want to implement Basic Authentication in some application. We can use the pros-and-cons.prompt.md file to analyze the proposed solution.    üí° The great thing about this is that you can tailor the prompt to your specific needs.Other Use Cases1Ô∏è‚É£ Generated Knowledge Prompting: Rather than including every detail in your initial prompt, you can instruct Copilot to ask clarifying questions. Your responses to these questions are then incorporated as context, enabling Copilot to generate more accurate and relevant code.For example, here is my qa.prompt.md file:---description: Q&amp;A Session---# DefinitionAsk me a series of yes/no questions to better understand my needs and give a more accurate recommendation.## Constraints- Follow best practices, suggest best practices, and avoid common pitfalls.- The questions should be relevant to the topic at hand.- The questions should be clear and concise.- Ask questions in batches of 5.- Do not ask more than 5 questions.- Ask all questions in one go.- Do not proceed without my answer.2Ô∏è‚É£ Generate Repeatable Code Constructs. Assume you have a specific approach to adding CRUD operations to your application. You can create a reusable prompt file that describes the process and use it in Agent mode to generate the whole feature.3Ô∏è‚É£ Prompts as Artifacts. Instead of writing in the chat window directly, you can carefully craft the prompt file with necessary requirements and use it to generate the new feature. As a result, not only do you get things done, but you also have a nice artifact for future reference.Conclusion  üôå I hope you found it helpful. If you have any questions, please feel free to reach out. If you‚Äôd like to support my work, a star on GitHub would be greatly appreciated! üôèReferences  Prompt engineering for Copilot Chat  GitHub Copilot Customization  Prompt Engineering Guide"
    },
  
    {
      "id": "19",
      "title": "Composable AI with .NET Aspire: Extending DIAL Assistants with Add-Ons",
      "url": "/dotnet/ai/2025/04/18/introduction-to-dial-addons.html",
      "date": "April 18, 2025",
      "categories": ["dotnet","ai"],
      "tags": ["ai","aspire","dial","llm-orchestration"],
      "shortinfo": "Learn how to build composable AI applications with .NET Aspire and AI DIAL Addons.",
      "content": "TL;DR  In this post we will extend AI DIAL with Addons using .NET Aspire. We will build a simple TODO Assistant that allows us to manage our TODO list using natural language.Source code: https://github.com/NikiforovAll/ai-dial-dotnetTable of Contents:  TL;DR  Introduction  Build the Assistant from Scratch          Add DIAL Core      Add Assistant with Addon      Deploy OpenAI Model - GPT-4o      Run Everything Together        Conclusion  ReferencesIntroductionIn my previous blog post, I‚Äôve introduced you to DIAL, an open-source AI orchestration platform.AI DIAL promises to be an extensible platform for building and deploying AI applications. In this post, we will learn how to add Addons to AI DIAL. Addons are a powerful way to extend the functionality of your AI applications by integrating with external services or APIs.The great thing about DIAL Addons is that they are based on OpenAI GPT Actions (aka Plugins). At their core, GPT Actions leverage Function Calling to execute API calls.In this blog, we will build a todo-assistant based on the official TODO Assistant from OpenAI. This Addon will allow us to manage our TODO list using natural language commands.In AI DIAL, conversational agents enable direct interactions between end-users and applications or language models via the AI DIAL Chat interface or AI DIAL API.Build the Assistant from ScratchLet‚Äôs create empty Aspire AppHost project:dotnet new aspire-apphost -n AppHost -o AppHost The default Program.cs looks like this:var builder = DistributedApplication.CreateBuilder(args);builder.Build().Run();Add DIAL Coreüì¶ Let‚Äôs install Aspire Host integration - Nall.EPAM.Dial.Aspire.Hosting.dotnet add package Nall.EPAM.Dial.Aspire.HostingNow we can add the DIAL hosting to our application. The Program.cs file should look like this:var builder = DistributedApplication.CreateBuilder(args);var dial = builder.AddDial(\"dial\", port: 8080).WithChatUI(port: 3000); // &lt;--Adds DIAL and Chat UIbuilder.Build().Run();üöÄ Now, we can use Aspire cli to run our application:aspire run --project AppHost/AppHost.csproj  Here is the the Aspire Dashboard:  Add Assistant with AddonOnce we deployed the DIAL Core, we can add the Addon. The Addon is a simple HTTP API that will be called by the DIAL Core when it needs to execute an action.üêã Here is an example of Dockerfile of  ‚ÄúTODO List‚Äù Addon:FROM python:3.11-alpine AS builderRUN apk update &amp;&amp; apk add gitRUN git clone https://github.com/openai/plugins-quickstart.gitFROM python:3.11-alpineCOPY --from=builder /plugins-quickstart /plugins-quickstartWORKDIR /plugins-quickstartRUN pip install -r requirements.txtEXPOSE 5003CMD [\"python\", \"main.py\"]We will run this addon as container using Aspire:var builder = DistributedApplication.CreateBuilder(args);var dial = builder.AddDial(\"dial\", port: 8080).WithChatUI(port: 3000);var todoAddonContainer = builder    .AddDockerfile(\"todo-addon-container\", contextPath: \"addons\", dockerfilePath: \"TodoDockerfile\")    .WithHttpEndpoint(port: null, targetPort: 5003, \"http\");builder.Build().Run();Next, we wil register the Addon with the DIAL Core:var builder = DistributedApplication.CreateBuilder(args);var dial = builder.AddDial(\"dial\", port: 8080).WithChatUI(port: 3000);var todoAddonContainer = builder    .AddDockerfile(\"todo-addon-container\", contextPath: \"addons\", dockerfilePath: \"TodoDockerfile\")    .WithHttpEndpoint(port: null, targetPort: 5003, \"http\");var todoAddon = dial.AddAddon(\"todo-addon\")    .WithUpstream(todoAddonContainer)    .WithDisplayName(\"TODO List\")    .WithDescription(\"Addon that allows to manage user's TODO list.\");dial.AddAssistantsBuilder()    .AddAssistant(\"todo-assistant\")    .WithPrompt(        \"You are assistant that helps to manage TODO list for the user. You can add, remove and view your TODOs.\"    )    .WithDescription(        \"The assistant that manages your TODO list. It can add, remove and view your TODOs.\"    )    .WithDisplayName(\"TODO Assistant\")    .WithAddon(todoAddon);builder.Build().Run();üöÄ Let‚Äôs run the AppHost and see the result:aspire run --project AppHost/AppHost.csprojHere is the ouput of the Aspire CLI:  And here is the Aspire Dashboard:  If you open the Chat UI, you will see the TODO Assistant:  ‚òùÔ∏è But before we can use it, we need to deploy OpenAI model that supports Function Calling.When you open Chat UI, you will see the following message - ‚ÄúNot available agent selected. Please, change the agent to proceed‚Äú.Let‚Äôs fix this one.Deploy OpenAI Model - GPT-4oüì¶ We will add Azure OpenAI hosting integration to the AppHost:dotnet add package Aspire.Hosting.Azure.CognitiveServicesLet‚Äôs add model deployment to the AppHost:var builder = DistributedApplication.CreateBuilder(args);var openAIApiKey = builder.AddParameter(\"azure-openai-api-key\", secret: true);var openai = builder    .AddAzureOpenAI(\"openai\")    .ConfigureInfrastructure(infra =&gt;    {        var resources = infra.GetProvisionableResources();        var account = resources.OfType&lt;CognitiveServicesAccount&gt;().Single();        account.Properties.DisableLocalAuth = false; // so we can use api key    });var gpt4o = openai.AddDeployment(\"gpt-4o\", \"gpt-4o\", \"2024-08-06\");Now we can register this model with the DIAL Core:var dial = builder.AddDial(\"dial\", port: 8080)    .WithChatUI(port: 3000)    .WaitFor(gpt4o); // &lt;-- Wait for the model to be deployedvar dialGpt4o = dial.AddOpenAIModelAdapter(\"dial-gpt-4o\", deploymentName: \"gpt-4o\")    .WithUpstream(gpt4o, openAIApiKey)    .WithDisplayName(\"gpt-4o\")    .WithDescription(        \"gpt-4o is engineered for speed and efficiency. Its advanced ability to handle complex queries with minimal resources can translate into cost savings and performance.\"    )    .WithWellKnownIcon(WellKnownIcon.GPT4);Run Everything TogetherAnd that is it! We have added the OpenAI model to our DIAL Core and we are ready to see how everything works together.The final Program.cs file should look like this:var builder = DistributedApplication.CreateBuilder(args);var openAIApiKey = builder.AddParameter(\"azure-openai-api-key\", secret: true);var openai = builder    .AddAzureOpenAI(\"openai\")    .ConfigureInfrastructure(infra =&gt;    {        var resources = infra.GetProvisionableResources();        var account = resources.OfType&lt;CognitiveServicesAccount&gt;().Single();        account.Properties.DisableLocalAuth = false; // so we can use api key    });var gpt4o = openai.AddDeployment(\"gpt-4o\", \"gpt-4o\", \"2024-08-06\");var dial = builder.AddDial(\"dial\", port: 8080).WithChatUI(port: 3000);var dialGpt4o = dial.AddOpenAIModelAdapter(\"dial-gpt-4o\", deploymentName: \"gpt-4o\")    .WithUpstream(gpt4o, openAIApiKey)    .WithDisplayName(\"gpt-4o\")    .WithDescription(        \"gpt-4o is engineered for speed and efficiency. Its advanced ability to handle complex queries with minimal resources can translate into cost savings and performance.\"    )    .WithWellKnownIcon(WellKnownIcon.GPT4);var todoAddonContainer = builder    .AddDockerfile(\"todo-addon-container\", contextPath: \"addons\", dockerfilePath: \"TodoDockerfile\")    .WithHttpEndpoint(port: null, targetPort: 5003, \"http\");var todoAddon = dial.AddAddon(\"todo-addon\")    .WithUpstream(todoAddonContainer)    .WithDisplayName(\"TODO List\")    .WithDescription(\"Addon that allows to manage user's TODO list.\");dial.AddAssistantsBuilder()    .AddAssistant(\"todo-assistant\")    .WithPrompt(        \"You are assistant that helps to manage TODO list for the user. You can add, remove and view your TODOs.\"    )    .WithDescription(        \"The assistant that manages your TODO list. It can add, remove and view your TODOs.\"    )    .WithDisplayName(\"TODO Assistant\")    .WithAddon(todoAddon);builder.Build().Run();üöÄ Let‚Äôs run the AppHost again:aspire run --project AppHost/AppHost.csprojHere is the output of the Aspire CLI:  And here is the the Aspire Dashboard:  ‚ú® Even more interesting is Aspire Graph Dashboard:  Now, if you open the Chat UI, you will see the TODO Assistant. Let‚Äôs try to add some TODOs:  ConclusionIn this blog post, we have learned how to extend AI DIAL with Addons using .NET Aspire. We have built a simple TODO Assistant that allows us to manage our TODO list using natural language commands.We have also learned how to deploy OpenAI model that supports Function Calling and how to integrate it with the DIAL Core.This is just the beginning of what you can do with AI DIAL and Addons. You can create more complex Addons that integrate with other services or APIs, or you can create your own custom Addons that provide unique functionality.  üôå I hope you found it helpful. If you have any questions, please feel free to reach out. If you‚Äôd like to support my work, a star on GitHub would be greatly appreciated! üôèReferences  https://github.com/NikiforovAll/ai-dial-dotnet  https://epam-rail.com  https://docs.epam-rail.com  https://github.com/epam/ai-dial  https://platform.openai.com/docs/actions/introduction  https://docs.epam-rail.com/tutorials/quick-start-with-addon"
    },
  
    {
      "id": "20",
      "title": "Learn how to use Model Context Protocol (MCP) Server Template in Hybrid Mode",
      "url": "/dotnet/2025/04/08/hybrid-mcp-template.html",
      "date": "April 08, 2025",
      "categories": ["dotnet"],
      "tags": ["dotnet","ai","aspire","mcp","mcp-server"],
      "shortinfo": "Use the 'mcp-server-hybrid' template to be able to easily switch between stdio and sse transports.",
      "content": "  TL;DR  TL;DR  Create from Template          SSE vs Stdio      Review the Code        Aspire Integration  Conclusion  ReferencesUse the mcp-server-hybrid template to be able to easily switch between stdio and sse transports.Source code: https://github.com/NikiforovAll/mcp-template-dotnet            Package      Version                  Nall.ModelContextProtocol.Template                    Nall.ModelContextProtocol.Inspector.Aspire.Hosting            Create from TemplatePreviously, I‚Äôve shared with you the blog post - Simplifying Model Context Protocol (MCP) Server Development with Aspire. In this post we explored two ways to run the MCP server using Aspire.üéØ In reality, depending on the context, you may want to run the MCP server in different ways. For example, you may want to run the MCP server in sse mode for debugging/development purposes, but in stdio mode for production.In this post, I will show you how to use a simple template to create an MCP server that can be run in both modes.üì¶ As in my previous post, let‚Äôs install Nall.ModelContextProtocol.Aspire.Template package:dotnet new install Nall.ModelContextProtocol.Template# These templates matched your input: 'mcp'# Template Name      Short Name         Language  Tags# -----------------  -----------------  --------  -------------# MCP Server         mcp-server         [C#]      dotnet/ai/mcp# MCP Server SSE     mcp-server-sse     [C#]      dotnet/ai/mcp# MCP Server Hybrid  mcp-server-hybrid  [C#]      dotnet/ai/mcp‚ûïCreate an mcp-server-hybrid project: dotnet new mcp-server-hybrid -o MyAwesomeMCPServer -n MyAwesomeMCPServerNow we can run it in two different modes:In SSE mode:dotnet run# info: Microsoft.Hosting.Lifetime[14]#       Now listening on: http://localhost:3001# info: Microsoft.Hosting.Lifetime[0]#       Application started. Press Ctrl+C to shut down.# info: Microsoft.Hosting.Lifetime[0]#       Hosting environment: Development# info: Microsoft.Hosting.Lifetime[0]#       Content root path: ${HOME}/MyAwesomeMCPServer# info: Microsoft.Hosting.Lifetime[0]Start the  MCP Inpsector and configure it to listen on the default address: ‚Äúhttp://localhost:3001/sse‚Äù.npx @modelcontextprotocol/inspectorIn Stdio mode:npx @modelcontextprotocol/inspector dotnet run -v q -- --stdio# ‚öôÔ∏è Proxy server listening on port 6277# üîç MCP Inspector is up and running at http://127.0.0.1:6274 üöÄ# New SSE connection# Query parameters: {#   transportType: 'stdio',#   command: 'dotnet',#   args: 'run -v q --stdio'# }# Stdio transport: command=C:\\Program Files\\dotnet\\dotnet.exe, args=run,-v,q,--stdioSSE vs StdioThe benefit of the SSE mode is that you can run the MCP server with a debugger attached and/or see the logs directly. The Stdio mode is slightly more complex, as it relies on the MCP Client (e.g., MCP Inspector) to start the server, and it disables logging on the MCP server to maintain compatibility with the MCP Client.On the other hand, the Stdio Server‚Äôs lifetime is managed by the MCP Client. This makes it much easier to consume MCP servers in this mode because you typically don‚Äôt have to worry about the server‚Äôs lifetime. It is started by the MCP Client and stopped when the MCP Client is stopped.Review the CodeBefore you start developing your own MCPs using this template, let‚Äôs take a look at the code generated by the template. Here is a content of Program.cs:var builder = WebApplication.CreateBuilder(args);builder.WithMcpServer(args).WithToolsFromAssembly();var app = builder.Build();app.MapMcpServer(args);app.Run();[McpServerToolType]public static class EchoTool{    [McpServerTool, Description(\"Echoes the message back to the client.\")]    public static string Echo(string message) =&gt; $\"hello {message}\";}üí° All ‚Äúmagic‚Äù happens in the McpServerExtensions.cs class. In the code below, we check if the --stdio argument is present. If it is, we configure the server to use the Stdio transport. Otherwise, we use the SSE transport. You don‚Äôt need to worry about how to switch between the two modes. The template does it for you.public static IMcpServerBuilder WithMcpServer(this WebApplicationBuilder builder, string[] args){    var isStdio = args.Contains(\"--stdio\");    if (isStdio)    {        builder.WebHost.UseUrls(\"http://*:0\"); // random port        // logs from stderr are shown in the inspector        builder.Services.AddLogging(builder =&gt;            builder                .AddConsole(consoleBuilder =&gt;                {                    consoleBuilder.LogToStandardErrorThreshold = LogLevel.Trace;                    consoleBuilder.FormatterName = \"json\";                })                .AddFilter(null, LogLevel.Warning)        );    }    var mcpBuilder = isStdio        ? builder.Services.AddMcpServer().WithStdioServerTransport()        : builder.Services.AddMcpServer();    return mcpBuilder;}public static WebApplication MapMcpServer(this WebApplication app, string[] args){    var isSse = !args.Contains(\"--stdio\");    if (isSse)    {        app.MapMcp();    }    return app;}Aspire IntegrationDown below I demonstrate how to run the MCP server using the Aspire hosting integration in two different modes simultaneously.‚ûï Create AppHost:dotnet new aspire-apphost -n AppHost -o AppHostüì¶ Install Nall.ModelContextProtocol.Inspector.Aspire.Hosting package:dotnet add ./Apphost package Nall.ModelContextProtocol.Inspector.Aspire.Hostingüîó Add project reference to AppHost:dotnet add ./AppHost/AppHost.csproj reference ./MyAwesomeMCPServer/MyAwesomeMCPServer.csprojAdd the following code to Program.cs of the AppHost:var builder = DistributedApplication.CreateBuilder(args);var sse = builder.AddProject&lt;Projects.MyAwesomeMCPServer&gt;(\"server\");builder.AddMCPInspector(\"mcp-sse\", serverPort: 9000, clientPort: 8080).WithSSE(sse);builder    .AddMCPInspector(\"mcp-stdio\")    .WithStdio&lt;Projects.MyAwesomeMCPServer&gt;();builder.Build().Run();Here is how the Aspire Dashboard looks like:  And it works like a charm!Conclusion  üôå I hope you found it helpful. If you have any questions, please feel free to reach out. If you‚Äôd like to support my work, a star on GitHub would be greatly appreciated! üôèReferences  https://github.com/NikiforovAll/mcp-template-dotnet  https://modelcontextprotocol.io/docs/concepts/transports"
    },
  
    {
      "id": "21",
      "title": "Simplifying Model Context Protocol (MCP) Server Development with Aspire",
      "url": "/dotnet/2025/04/04/mcp-template-and-aspire.html",
      "date": "April 04, 2025",
      "categories": ["dotnet"],
      "tags": ["dotnet","ai","aspire","mcp","mcp-server","developer-tools"],
      "shortinfo": "Learn how to develop MCP servers with Aspire.",
      "content": "  TL;DR  TL;DR  Introduction  ‚ÄòStdio‚Äô Mode  ‚ÄòSSE‚Äô Mode  Stdio vs SSE  Conclusion  ReferencesYou can use Nall.ModelContextProtocol.Inspector.Aspire.Hosting hosting integration to run MCP Inspector and integrate it with your MCP Servers using Aspire.Source code: https://github.com/NikiforovAll/mcp-template-dotnetIntroductionThis blog will be in a form of tutorial, we will build a simple echo MCP server by using Nall.ModelContextProtocol.Template template that I‚Äôve shared with you in the previous post.üöÄ Let‚Äôs get started. I want to demonstrate two ways (aka Transports) to run the MCP server: stdio and sse.But first, let‚Äôs create Aspire project.‚ûï Create AppHost:dotnet new aspire-apphost -n AppHost -o AppHostüì¶ Install Nall.ModelContextProtocol.Inspector.Aspire.Hosting package:dotnet add ./Apphost package Nall.ModelContextProtocol.Inspector.Aspire.Hostingüì¶ As in my previous post, let‚Äôs install Nall.ModelContextProtocol.Template package:dotnet new install Nall.ModelContextProtocol.Template# These templates matched your input: 'mcp'# Template Name   Short Name      Language  Tags# --------------  --------------  --------  -------------# MCP Server      mcp-server      [C#]      dotnet/ai/mcp# MCP Server SSE  mcp-server-sse  [C#]      dotnet/ai/mcp‚ÄòStdio‚Äô Mode‚ûïCreate an MCP server: dotnet new mcp-server -o MyAwesomeMCPServer -n MyAwesomeMCPServerüîó Add project reference to AppHost:dotnet add ./AppHost/AppHost.csproj reference ./MyAwesomeMCPServer/MyAwesomeMCPServer.csprojIn Program.cs of the AppHost, add the following code:var builder = DistributedApplication.CreateBuilder(args);builder.AddMCPInspector().WithStdio&lt;Projects.MyAwesomeMCPServer&gt;();builder.Build().Run();It runs @modelcontextprotocol/inspector under the hood. It is an MCP proxy that allows you to test and debug MCP servers.üí° Note: The Inspector is responsible for starting the .NET project. So, no corresponding Aspire Resource will be available on the dashboard for the ‚ÄòStdio‚Äô mode.  Open http://127.0.0.1:6274 in your browser and click ‚ÄúConnect.‚Äù. Now, you can test the server using the Inspector tool.  ‚ÄòSSE‚Äô ModeLet‚Äôs generate a new MCP server using the sse transport. You can learn more about MCP transports here - https://modelcontextprotocol.io/docs/concepts/transports‚ûï Create a new MCP server:dotnet new mcp-server-sse -o MyAwesomeMCPServerSSE -n MyAwesomeMCPServerSSEüîó Add project reference to AppHost:dotnet add ./AppHost/AppHost.csproj reference ./MyAwesomeMCPServerSSE/MyAwesomeMCPServerSSE.csprojIn Program.cs of the AppHost, add the following code:var builder = DistributedApplication.CreateBuilder(args);var mcp = builder.AddProject&lt;Projects.MyAwesomeMCPServerSSE&gt;(\"server\"); // NOTE, for SSE mode it's a separate projectbuilder.AddMCPInspector().WithSSE(mcp);builder.Build().Run();The Aspire Dashboard looks slightly different for the sse mode. Because, this approach creates a separate Aspire.Hosting.ApplicationModel.ProjectResource with an exposed /sse endpoint. This endpoint allows you to seamlessly connect to the MCP server using the Inspector tool.  Open http://127.0.0.1:6274 in your browser and click ‚ÄúConnect.‚Äù. Now, you can test the server using the Inspector tool.  ‚ö†Ô∏è As for now, it is not possible to specify sse endpoint in the Inspector command line to automatically configure MCP Inspector. I plan to extend the Aspire integration when the feature will be implemented - modelcontextprotocol/inspector/issues/239. For now, you have to configure the Inspector manually if you divert from standard configuration.Stdio vs SSEI demonstrated two approaches to running MCP servers. Personally, I favor the sse mode due to its transparency and ease of troubleshooting. For instance, you can launch an MCP server with a debugger attached or view the output logs directly. In contrast, the stdio mode relies on the Inspector to start the server, which disables logging on the MCP server to maintain compatibility with the Inspector.ConclusionI hope this blog post has provided you with a clear way to start building your own MCP servers using the Aspire hosting integration.  üôå I hope you found it helpful. If you have any questions, please feel free to reach out. If you‚Äôd like to support my work, a star on GitHub would be greatly appreciated! üôèReferences  https://github.com/NikiforovAll/mcp-template-dotnet  https://nikiforovall.github.io/dotnet/2025/04/02/mcp-template-getting-started.html  https://modelcontextprotocol.io/docs/concepts/transports"
    },
  
    {
      "id": "22",
      "title": "Simplifying Model Context Protocol (MCP) Server Distribution with .NET Global Tools",
      "url": "/dotnet/2025/04/02/mcp-template-getting-started.html",
      "date": "April 02, 2025",
      "categories": ["dotnet"],
      "tags": ["dotnet","ai","mcp","mcp-server"],
      "shortinfo": "In this post, I will show you how to use a simple template to create an MCP server that can be distributed as a global tool.",
      "content": "  TL;DRUse the Nall.ModelContextProtocol.Template template to create a Model Context Protocol (MCP) server that can be distributed as a global tool:Install:dotnet new install Nall.ModelContextProtocol.TemplateSource code: https://github.com/NikiforovAll/mcp-template-dotnetIntroductionIn this post, I would like to show you how you can use .NET Global Tool to distribute your Model Context Protocol (MCP) server. I‚Äôve created dotnet new template to support this approach and I will show you how to use it.Getting StartedInstalldotnet new install Nall.ModelContextProtocol.TemplateVerify installation:dotnet new list mcp# These templates matched your input: 'mcp'# Template Name  Short Name  Language  Tags# -------------  ----------  --------  -------------# MCP Server     mcp-server  [C#]      dotnet/ai/mcpVerify output:dotnet new mcp-server -o MyAwesomeMCPServer -n MyAwesomeMCPServer --dry-run# File actions would have been taken:#   Create: MyAwesomeMCPServer\\.vscode\\launch.json#   Create: MyAwesomeMCPServer\\MyAwesomeMCPServer.csproj#   Create: MyAwesomeMCPServer\\Program.cs#   Create: MyAwesomeMCPServer\\Properties\\launchSettings.json#   Create: MyAwesomeMCPServer\\README.md#   Create: MyAwesomeMCPServer\\appsettings.Development.json#   Create: MyAwesomeMCPServer\\appsettings.jsonCreate from template:dotnet new mcp-server -o MyAwesomeMCPServer -n MyAwesomeMCPServerHere is a content of Program.cs:using Microsoft.Extensions.Hosting;var builder = Host.CreateApplicationBuilder(args);builder.Services    .AddMcpServer()    .WithStdioServerTransport()    .WithToolsFromAssembly();await builder.Build().RunAsync();[McpServerToolType]public static class EchoTool{    [McpServerTool, Description(\"Echoes the message back to the client.\")]    public static string Echo(string message) =&gt; $\"hello {message}\";}It is a simple echo server that listens for incoming messages and responds with a greeting. You can add more tools by creating additional methods with the [McpServerTool] attribute. The WithToolsFromAssembly() method automatically registers all tools in the assembly.Run Locally‚öôÔ∏è Build from the project directory:dotnet build -o Artefacts -c ReleaseüöÄ Run the inspector:npx @modelcontextprotocol/inspector -e DOTNET_ENVIRONMENT=Production dotnet \"$(PWD)/Artefacts/MyAwesomeMCPServer.dll\"Open inspector in your browser and test the server:  Distribute as .NET ToolThe basic idea behind this approach is to create a .NET tool that can be installed globally on the user‚Äôs machine. This allows users to run the MCP server from anywhere without needing to specify the full path to the executable.  üí° Another benefit of this approach is that you can package your MCP server as a NuGet package, making it easy to distribute, version, and share with others.üì¶ Pack from the project directory:dotnet pack -o Artefacts -c ReleaseInstall the tool globally:dotnet tool install --global --add-source ./Artefacts MyAwesomeMCPServer# You can invoke the tool using the following command: MyAwesomeMCPServer# Tool 'myawesomemcpserver' (version '1.0.0') was successfully installed.Now, after you installed this tool globally, you can run it from anywhere on your system. The tool will be available as MyAwesomeMCPServer (or myawesomemcpserver) in your terminal.  üí° You can also create local tool manifest and install specific MCP versions per manifest.üöÄ Run the inspector:npx @modelcontextprotocol/inspector -e DOTNET_ENVIRONMENT=Production myawesomemcpserverConclusionI think MCP opens up a lot of fun possibilities for building AI applications. In post, I showed you how to you mcp-server template to make your life a little easier.  üôå I hope you found it helpful. If you have any questions, please feel free to reach out. If you‚Äôd like to support my work, a star on GitHub would be greatly appreciated! üôèReferences  https://github.com/NikiforovAll/mcp-template-dotnet  https://www.nuget.org/packages/Nall.ModelContextProtocol.Template  https://github.com/modelcontextprotocol/csharp-sdk  https://learn.microsoft.com/en-us/dotnet/core/tools/global-tools  https://learn.microsoft.com/en-us/dotnet/core/tools/custom-templates"
    },
  
    {
      "id": "23",
      "title": "Introducing AI DIAL: The Open-Source AI Orchestration Platform",
      "url": "/dotnet/ai/2025/03/30/introduction-to-dial.html",
      "date": "March 30, 2025",
      "categories": ["dotnet","ai"],
      "tags": ["ai","aspire","dial","llm-orchestration"],
      "shortinfo": "Use AI DIAL to streamline and orchestrate Software Development Lifecycle (SDLC) for enterprise Gen AI applications. In this post, I will introduce you to the platform and its capabilities and...",
      "content": "TL;DRUse AI DIAL to streamline and orchestrate Software Development Lifecycle (SDLC) for enterprise Gen AI applications. In this post, I will introduce you to the platform and its capabilities and show you how to get started with it using .NET Aspire.Source code: https://github.com/NikiforovAll/ai-dial-dotnetTable of Contents:  TL;DR  Introduction  Key Features  Getting Started via .NET Aspire          Hosting Integration      Client Integration      Demo        Conclusion  ReferencesIntroductionAI DIAL stands for Deterministic Integrator of Applications and Language Models. It is an enterprise-grade, open-source AI orchestration platform that simplifies the development, deployment, and management of AI-driven applications. AI DIAL acts as both a development studio and an application server, enabling seamless integration between various AI models, data pipelines, and business applications.See: DIAL 2.0: The Open Source AI Orchestration Platform Overview - YouTube  Key Features  üîó Models Connectivity - provides out-of-the-box adapters for all major LLM providers, including all models hosted in Amazon Bedrock, Google‚Äôs Vertex AI, and Azure OpenAI Service. Additionally, you can use language models from the open-source community, alternative vendors, and fine-tuned micro models, as well as self-hosted or models listed on HuggingFace or DeepSeek. As of now, there are over 75 ready-to-use adapters, with more being added regularly. If necessary, DIAL SDK can be used to develop adapters for additional models and vendors. [Learn More]  üíª Application Server &amp; Unified API - provides a single Unified API, based on OpenAI API, for accessing all language models, embedding models and applications. The key design principle is to create a unification layer that allows all models and applications to be interchangeable, delivering a cohesive conversational experience and future-proof development of custom GenAI applications.[Learn More]  üõ†Ô∏è Extensibility - AI DIAL can be extended beyond its standard capabilities to meet specific business requirements. You can leverage the SDK to create custom model adapters and GenAI applications, and even include new application types to build fully custom implementations. AI DIAL Chat also enables the creation of custom chat UI components. [Learn More]  üí¨ Customizable Chat &amp; Overlay - Powerful and highly customizable chat application for end-users, with enterprise-grade access control, extendable functionality and ability to add custom GenAI applications. Overlay enables a seamless embedding of chat into any existing web application. [Learn More]  ‚ûï Marketplace - Marketplace gives access to all conversational agents available within the organization. Additionally, the marketplace offers collaboration tools for users and supports Role-Based Access Control (RBAC) to streamline teamwork and ensure secure access to resources. [Learn More]Getting Started via .NET AspireNow, you know what DIAL is. Let‚Äôs see how to get started with it using .NET Aspire.üéØ Goal. Let‚Äôs say we want to:  Create DIAL installation with two models: deepseek-r1 and phi3  Use DIAL Chat UI to interact with the models  Use them in our application programmatically through DIAL Unified APIHosting IntegrationFirst, we want to add DIAL hosting integration to AppHost project by adding the EPAM.Dial.Aspire.Hosting NuGet package:dotnet add package Nall.EPAM.Dial.Aspire.HostingThen, we need to add the DIAL hosting integration to the AppHost/Program.cs:var builder = DistributedApplication.CreateBuilder(args);var ollama = builder    .AddOllama(\"ollama\")    .WithOpenWebUI()    .WithDataVolume()    .WithLifetime(ContainerLifetime.Persistent);ollama.AddModel(\"ollama-deepseek-r1\", \"deepseek-r1:1.5b\");ollama.AddModel(\"ollama-phi3\", \"phi3.5\");var dial = builder.AddDial(\"dial\", port: 8080).WaitFor(ollama).WithChatUI(port: 3000);var deepseek = dial.AddModel(\"deepseek\", deploymentName: \"deepseek-r1:1.5b\")    .WithEndpoint(ollama.Resource.PrimaryEndpoint)    .WithDisplayName(\"DeepSeek-R1\");var phi3 = dial.AddModel(\"phi3\", deploymentName: \"phi3.5\")    .WithEndpoint(ollama.Resource.PrimaryEndpoint)    .WithDisplayName(\"Phi-3.5\"); builder.AddProject&lt;Projects.Api&gt;(\"api\").WithReference(deepseek).WithReference(phi3).WaitFor(dial);builder.Build().Run();In the example above, we are adding the DIAL hosting integration to the AppHost project. We are also adding two models: ollama-deepseek-r1 and ollama-phi3, which are hosted on the Ollama server. We are also adding a DIAL Chat UI that will be available on port 3000.üí° You are not limited to only self-hosted models. AI DIAL allows you to access models from all major LLM providers, language models from the open-source community, etc. See Supported Models for more information.Now, we can open Aspire Dashboard and explore the component graph.  Let‚Äôs open the DIAL Chat UI and navigate to the marketplace to see the available models.  Select the deepseek model and click on the ‚ÄúUse Model‚Äù button. You will be redirected to the chat UI, where you can interact with the model.  The Chat UI is pretty powerful. For example, you can compare the output of two models by using Compare Mode. Here you can see the output of deepseek and phi3 models side by side.  Client IntegrationNow, let‚Äôs see how to use the DIAL Unified API in our application. We will use the EPAM.Dial.Aspire NuGet package to access the IChatClient implementation to trigger chat completions.dotnet add package EPAM.Dial.AspireAll we need is to add the DialClient to the Api/Program.cs:var builder = WebApplication.CreateBuilder(args);builder.AddServiceDefaults();builder.AddDialOpenAIClient`(\"deepseek\").AddChatClient(); // &lt;--- client integrationvar app = builder.Build();app.MapGet(    \"/chat\",    async ([FromQuery] string query, [FromServices] IChatClient client) =&gt;    {        var prompt = $\"You are helpful assistant. Answer the following question: '{query}'\";        var response = await client.GetResponseAsync(prompt);        return Results.Ok(response);    });app.MapDefaultEndpoints();app.Run();In the example above, we are adding the DIAL client implementation of the IChatClient from Microsoft.Extensions.AI which is a standard way of integrating with LLMs in .NET.üí° Note, it is called AddDialOpenAIClient because essentially, it adds OpenAI under the hood. This is possible because completion part of the Unified API is compatible with the OpenAI API.DemoüöÄNow, let‚Äôs this in action:curl --url 'http://localhost:5181/chat?query=Test'Output:{  \"messages\": [    {      \"authorName\": null,      \"role\": \"assistant\",      \"contents\": [        {          \"$type\": \"text\",          \"text\": \"&lt;think&gt;\\n\\n&lt;/think&gt;\\n\\nIt seems like you might be referring to a \\\"test\\\" or something related to testing, but I don‚Äôt have specific information about what you‚Äôre asking. Could you clarify your needs? Are you asking about how to perform a test, taking a test as part of an educational purpose, or something else? Let me know so I can assist you better!\",          \"additionalProperties\": null        }      ],      \"messageId\": \"chatcmpl-161\",      \"additionalProperties\": null    }  ],  \"responseId\": \"chatcmpl-161\",  \"chatThreadId\": null,  \"modelId\": \"deepseek-r1:1.5b\",  \"createdAt\": \"2025-03-30T13:05:43+00:00\",  \"finishReason\": \"stop\",  \"usage\": {    \"inputTokenCount\": 16,    \"outputTokenCount\": 76,    \"totalTokenCount\": 92,    \"additionalCounts\": {}  },  \"additionalProperties\": {    \"SystemFingerprint\": \"fp_ollama\"  }}Conclusionüôå In this post, I have introduced you to AI DIAL, an open-source AI orchestration platform that simplifies the development, deployment, and management of AI-driven applications. I have also shown you how to get started with it using .NET Aspire.This is just the a scratch of the surface of what you can do with DIAL. I encourage you to explore the platform and its capabilities further. Let me know if you are interested in more posts about DIAL and its features.References  https://epam-rail.com  https://docs.epam-rail.com  https://github.com/epam/ai-dial  https://learn.microsoft.com/en-us/dotnet/aspire/get-started/aspire-overview"
    },
  
    {
      "id": "24",
      "title": "Enforcing .NET Coding Guidelines with GitHub Copilot Custom Instructions",
      "url": "/productivity/2025/03/08/github-copilot-instructions-for-dotnet.html",
      "date": "March 08, 2025",
      "categories": ["productivity"],
      "tags": ["productivity","ai","copilot","dotnet"],
      "shortinfo": "Learn how to setup GitHub Copilot to follow your coding guidelines (aka custom instructions). I've created a repository with a set of rules for .NET development that you can use...",
      "content": "TL;DRUsing GitHub Copilot with custom instructions can improve the quality of generated code by enforcing coding guidelines. I will show you how to use GitHub Copilot custom instructions in the context of .NET development. You can use my repository with a set of rules as a starting point.Source code: https://github.com/NikiforovAll/dotnet-copilot-rules  IntroductionüéØ If you want to build a successful project, ensuring your code is clean, maintainable, and idiomatic is crucial. This is where coding guidelines come into play.Coding guidelines promote consistency, readability, and maintainability within a project. Documenting these guidelines helps developers adhere to best practices, streamline collaboration, and minimize technical debt.ü§ñ As AI coding tools and agents become more pervasive, clear coding guidelines are more important than ever. These tools assist in generating, refactoring, and reviewing code. Documenting coding guidelines provides essential context, ensuring AI-generated code maintains consistency, readability, and best practices. Without structured rules, AI contributions may introduce inconsistencies, increasing technical debt and maintenance overhead.ConfigureTo configure GitHub Copilot to follow your coding guidelines, you can use the custom instructions feature. This allows you to define specific rules and preferences for code generation.Here is an example of my custom instructions configuration:{    \"github.copilot.chat.codeGeneration.instructions\": [        {            \"file\": \".vscode/rules/csharp/coding-guidelines.md\"        },        {            \"file\": \".vscode/rules/csharp/coding-style.md\"        }    ],    \"github.copilot.chat.reviewSelection.enabled\": true,    \"github.copilot.chat.reviewSelection.instructions\": [        {            \"file\": \".vscode/rules/csharp/coding-guidelines.md\"        },        {            \"file\": \".vscode/rules/csharp/coding-style.md\"        }    ],    \"github.copilot.chat.testGeneration.instructions\": [        {            \"file\": \".vscode/rules/csharp/testing-xunit.md\"        }    ],    \"github.copilot.chat.commitMessageGeneration.instructions\": [        {            \"file\": \".vscode/rules/git-message.md\"        }    ],    \"chat.promptFiles\": true,    \"chat.promptFilesLocations\": {        \".github/prompts\": true,        \".vscode/prompts\": true    },}Now, we can add as many files as we want to the .vscode/rules folder. The file could be in any format, but I recommend using Markdown for better readability. The files should contain the rules you want Copilot to follow.For example, here is .vscode/rules/coding-guidelines.md file with coding guidelines:---description: This file provides guidelines for writing clean, maintainable, and idiomatic C# code with a focus on functional patterns and proper abstraction.---# Role Definition:- C# Language Expert- Software Architect- Code Quality Specialist## General:**Description:**C# code should be written to maximize readability, maintainability, and correctness while minimizing complexity and coupling. Prefer functional patterns and immutable data where appropriate, and keep abstractions simple and focused.**Requirements:**- Write clear, self-documenting code- Keep abstractions simple and focused- Minimize dependencies and coupling- Use modern C# features appropriately## Code Organization:- Use meaningful names:    ```csharp    // Good: Clear intent    public async Task&lt;Result&lt;Order&gt;&gt; ProcessOrderAsync(OrderRequest request, CancellationToken cancellationToken)        // Avoid: Unclear abbreviations    public async Task&lt;Result&lt;T&gt;&gt; ProcAsync&lt;T&gt;(ReqDto r, CancellationToken ct)    ```&lt;!-- And so on... --&gt;Practiceü§ñ Let‚Äôs see how it works in practice. Here is a result of simple prompt: ‚ÄúGenerate FizzBuzz‚Äù:  As you can see, two additional files were used to generate the code:  .vscode/rules/csharp/coding-guidelines.md - contains coding guidelines  .vscode/rules/csharp/coding-style.md - contains coding style rulesThe generated code is something that I would expect from a developer who follows the rules. ‚òùÔ∏èLimitationsüß† Keep in mind that Copilot is not perfect and may generate code that does not follow the rules. It is important to review the generated code and provide feedback to Copilot. .Here are my findings after using this setup for a little while:  In practice, if you have too many rules it may degrade the overall quality of generated code or make copilot hallucinate  It is definitely easier to tell Copilot what to do rather than what not to do (LLMs are not good at negations)  Copilot is not perfect, but it is getting better. I would say that ~ 70% of the time it generates code that follows the rules.  We want to have rules understandable by both humans and Copilot, sometimes it is hard to strike a balanceConclusionUsing GitHub Copilot with custom instructions can significantly improve the quality of generated code by enforcing coding guidelines. This approach has a lot of potential and is worth investing time in. With the advancement of LLMs and GitHub Copilot, I expect even better results in the future. üöÄReferences  https://github.com/NikiforovAll/dotnet-copilot-rules  https://github.com/Aaronontheweb/dotnet-cursor-rules  https://code.visualstudio.com/docs/copilot/copilot-customization"
    },
  
    {
      "id": "25",
      "title": "Deploy Your Bot to Azure in 5 Minutes with Azure Developer CLI (azd)",
      "url": "/dotnet/2024/12/22/azd-bot-service.html",
      "date": "December 22, 2024",
      "categories": ["dotnet"],
      "tags": ["dotnet","azure","bot"],
      "shortinfo": "Learn how to use a starter template to deploy a bot to Azure with Azure Bot Service and Azure Developer CLI (azd)",
      "content": "TL;DRSource code: https://github.com/NikiforovAll/azd-bot-service-starterIntroductionBots are everywhere, as developers, we should embrace this trend and learn how to prototype and deploy bots quickly.Using Azure Developer CLI (azd), we can quickly deploy and manage our solutions. This tutorial shows you how to deploy a bot to Azure in just five minutes using a starter template.üöÄ Let‚Äôs get started and deploy your bot to Azure!Install From TemplateSince I already prepared a starter template, you can deploy a bot to Azure in just a few minutes.Here‚Äôs how:azd init --template https://github.com/NikiforovAll/azd-bot-service-starterHere is the sneak peek of the bot that we are going to deploy:// Program.csvar builder = WebApplication.CreateBuilder(args);builder.Services.AddSingleton&lt;IBotFrameworkHttpAdapter, AdapterWithErrorHandler&gt;();builder.Services.AddTransient&lt;IBot, EchoBotHandler&gt;();var app = builder.Build();app    .UseDefaultFiles()    .UseStaticFiles()    .UseWebSockets()    .UseRouting()    .UseAuthorization();app.MapPost(    \"/api/messages\",    async (IBotFrameworkHttpAdapter adapter, IBot bot, HttpRequest req, HttpResponse res) =&gt;    {        await adapter.ProcessAsync(req, res, bot);    });app.Run();It‚Äôs a simple echo bot that repeats the user‚Äôs message back to them. Nothing special, but this is a good starting point for your bot.public class EchoBotHandler : ActivityHandler{    protected override async Task OnMessageActivityAsync(        ITurnContext&lt;IMessageActivity&gt; turnContext,        CancellationToken cancellationToken    )    {        var replyText = $\"Echo: {turnContext.Activity.Text}\";        await turnContext.SendActivityAsync(            MessageFactory.Text(replyText, replyText),            cancellationToken        );    }}Run and DeployNow, we can create a new azd environment:azd env new azd-bot-service-devAnd deploy the bot to Azure.azd upHere is the output of the deployment (it took me less than 5 minutes to deploy the bot ü§Ø):  The azd CLI outputs the URL of the deployed bot. You can test it by sending a message to the bot.  The benefit of using azd template is that it automatically creates a new resource group, app service plan, application insights, vault, and app service for you. You don‚Äôt need to worry about the infrastructure, just focus on your code.For example, here is an application insights dashboard for the bot:  TeardownWhen you are done with the bot, you can delete the resources using the following command:azd downThis command will delete all the resources created by the azd up command.  Conclusion  üôå I hope you found it helpful. If you have any questions, please feel free to reach out. If you‚Äôd like to support my work, a star on GitHub would be greatly appreciated! üôèReferences  awesome-azd  Azure Developer CLI - Getting Started"
    },
  
    {
      "id": "26",
      "title": "Hybrid Search with Elasticsearch in .NET",
      "url": "/dotnet/2024/11/02/elastic-hybrid-search.html",
      "date": "November 02, 2024",
      "categories": ["dotnet"],
      "tags": ["dotnet","elasticsearch","search","ai"],
      "shortinfo": "Use the reciprocal rank fusion algorithm to combine the results of BM25 and kNN semantic search.",
      "content": "TL;DRUse the reciprocal rank fusion algorithm to combine the results of BM25 and kNN semantic search.Source code: https://github.com/NikiforovAll/elasticsearch-dotnet-playground/blob/main/src/elasticsearch-getting-started/02-hybrid-search.ipynbIntroductionI‚Äôve prepared a Jupyter notebook that demonstrates how to use the reciprocal rank fusion algorithm using Elastic.Clients.Elasticsearch. You can find the source code here.    Hybrid SearchIn my previous blog posts, you have seen two different approaches to search a collection of documents (Semantic Search with Elasticsearch in .NET and Querying and Filtering via Elastic.Clients.Elasticsearch in .NET), each with its own particular benefits. If one of these methods matches your needs then you don‚Äôt need anything else, but in many cases each method of searching returns valuable results that the other method would miss, so the best option is to offer a combined result set.For these cases, Elasticsearch offers Reciprocal Rank Fusion, an algorithm that combines results from two or more lists into a single list.How RRF Works in Elasticsearch‚òùÔ∏è RRF is based on the concept of reciprocal rank, which is the inverse of the rank of the first relevant document in a list of search results. The goal of the technique is to take into account the position of the items in the original rankings, and give higher importance to items that are ranked higher in multiple lists. This can help improve the overall quality and reliability of the final ranking, making it more useful for the task of fusing multiple ordered search results.Elasticsearch integrates the RRF algorithm into the search query. Consider the following example, which has query and knn sections to request full-text and vector searches respectively, and a rrf section that combines them into a single result list.{    \"query\":{        // full-text search query here    },    \"knn\":{        // vector search query here    },    \"rank\":{        \"rrf\": {}    }}While RRF works fairly well for short lists of results without any configuration, there are some parameters that can be tuned to provide the best results. Consult the documentation to learn about these in detail.Demo‚ö†Ô∏è I assume you already have the ‚Äúbook_index‚Äù dataset from my previous post Semantic Search with Elasticsearch in .NET. If you don‚Äôt, please follow the instructions in that post to set up the dataset.1Ô∏è‚É£ First, let‚Äôs define a method to convert a search query to an embedding vector. I will use Microsoft.Extensions.AI. It requires Azure Open AI model to generate embeddings. I will go with (text-embedding-3-small) because it allows you to specify embedding size. This is important because the size of the embedding vector should match the size of the vector field in the Elasticsearch index and Elasticsearch has a limit of 512 dimensions for vector fields.üí° Larger embeddings can capture more nuances and subtle relationships in the data, potentially leading to better model accuracy. However, very large embeddings can also lead to overfitting, where the model performs well on training data but poorly on unseen data. This is because the model might learn to memorize the training data rather than generalize from it.Here is the code to generate an embedding vector for a given text:using Azure.AI.OpenAI;using Microsoft.Extensions.AI;using System.ClientModel;AzureOpenAIClient aiClient = new AzureOpenAIClient(    new Uri(envs[\"AZURE_OPENAI_ENDPOINT\"]),    new ApiKeyCredential(envs[\"AZURE_OPENAI_APIKEY\"]));var generator = aiClient.AsEmbeddingGenerator(modelId: \"text-embedding-3-small\");async Task&lt;float[]&gt; ToEmbedding(string text){    var textEmbeddingDimension = 384;    var embeddings = await generator.GenerateAsync([text], new EmbeddingGenerationOptions {        Dimensions = textEmbeddingDimension    });    return embeddings.First().Vector.ToArray();}2Ô∏è‚É£ Now we can use this method to generate an embedding vector for a search query and use it in both full-text and vector search queries.var searchQuery = \"python programming\";var queryEmbedding = await ToEmbedding(searchQuery);var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Index(\"book_index\")    .Query(d =&gt; d.Match(m =&gt; m.Field(f =&gt; f.Summary).Query(searchQuery)))    .Knn(d =&gt; d        .Field(f =&gt; f.TitleVector)        .QueryVector(queryEmbedding)        .k(5)        .NumCandidates(10))    .Rank(r =&gt; r.Rrf(rrf =&gt; {})));PrettyPrint(searchResponse);‚öôÔ∏èOutput: Hybrid Search  Conclusion  üôå I hope you found it helpful. If you have any questions, please feel free to reach out. If you‚Äôd like to support my work, a star on GitHub would be greatly appreciated! üôèReferences  https://github.com/NikiforovAll/elasticsearch-dotnet-playground/tree/main/src/elasticsearch-getting-started  https://github.com/NikiforovAll/elasticsearch-dotnet-playground/blob/main/src/elasticsearch-getting-started/02-hybrid-search.ipynb  https://www.elastic.co/search-labs/tutorials/search-tutorial/welcome  https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking"
    },
  
    {
      "id": "27",
      "title": "Querying and Filtering via Elastic.Clients.Elasticsearch in .NET",
      "url": "/dotnet/2024/10/20/querying-and-filtering-elastic-dotnet.html",
      "date": "October 20, 2024",
      "categories": ["dotnet"],
      "tags": ["dotnet","elasticsearch","search"],
      "shortinfo": "Learn how to query and filter documents in Elasticsearch using the Elastic.Clients.Elasticsearch NuGet package.",
      "content": "TL;DRSource code: https://github.com/NikiforovAll/elasticsearch-dotnet-playground/blob/main/src/elasticsearch-getting-started/01-keyword-querying-filtering.ipynb  TL;DR  Introduction  Hands-on  Querying          Full text queries                  Match query          Multi-match query                      Term-level Queries          Term search                  Range search          Prefix search          Fuzzy search                    Combining Query Conditions                  bool.must (AND)          bool.should (OR)                      Filtering          bool.filter      bool.must_not      Using Filters with Queries        Conclusion  ReferencesIntroductionQuerying and filtering are two fundamental operations when working with Elasticsearch. Querying involves searching for documents that match certain criteria, often using full-text search capabilities. This is useful when you need to find documents based on relevance or specific content. Filtering, on the other hand, is used to narrow down the search results by applying specific conditions, such as range filters or term filters. Filters are generally faster and more efficient because they do not score documents like queries do. Understanding the distinction between querying and filtering can help you optimize your search operations and improve the performance of your Elasticsearch queries.Hands-onI‚Äôve prepared a Jupyter notebook that demonstrates how to query and filter Elasticsearch using Elastic.Clients.Elasticsearch. You can find the source code here.      ‚ö†Ô∏è Note: to run the queries that follow you need the ‚Äúbook_index‚Äù dataset from my previous post Semantic Search with Elasticsearch in .NET.QueryingIn the query context, a query clause answers the question ‚ÄúHow well does this document match this query clause?‚Äù. In addition to deciding whether or not the document matches, the query clause also calculates a relevance score in the _score metadata field.Full text queriesFull text queries enable you to search analyzed text fields such as the body of an email. The query string is processed using the same analyzer that was applied to the field during indexing.  match. The standard query for performing full text queries, including fuzzy matching and phrase or proximity queries.  multi-match. The multi-field version of the match query.Match queryReturns documents that match a provided text, number, date or boolean value. The provided text is analyzed before matching.The match query is the standard query for performing a full-text search, including options for fuzzy matching.üí° Docs: Read morevar searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Query(q =&gt; q        .Match(m =&gt; m            .Field(f =&gt; f.Summary)            .Query(\"guide\"))    )    .Size(5));DumpRequest(searchResponse);PrettyPrint(searchResponse);‚öôÔ∏èOutput: Match query  Multi-match queryThe multi_match query builds on the match query to allow multi-field queries.üí° Docs: Read more.var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Query(q =&gt; q .MultiMatch(m =&gt; m        .Fields(Fields.FromStrings([\"summary\", \"title\"]))        .Query(\"javascript\"))    ));DumpRequest(searchResponse);PrettyPrint(searchResponse);‚öôÔ∏èOutput: Multi-match query  ‚ûï Individual fields can be boosted with the caret (^) notation. Note in the following query how the score of the results that have ‚ÄúJavaScript‚Äù in their title is multiplied.var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Query(q =&gt; q .MultiMatch(m =&gt; m        .Fields(Fields.FromStrings([\"summary\", \"title^3\"]))        .Query(\"javascript\"))    ));DumpRequest(searchResponse);PrettyPrint(searchResponse);‚öôÔ∏èOutput: Multi-match query with boost  Term-level QueriesYou can use term-level queries to find documents based on precise values in structured data. Examples of structured data include date ranges, IP addresses, prices, or product IDs.Term searchReturns document that contain exactly the search term.var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Query(q =&gt; q.Term(t =&gt; t        .Field(f =&gt; f.Publisher)        .Value(\"addison-wesley\"))    ));DumpRequest(searchResponse);PrettyPrint(searchResponse);‚öôÔ∏èOutput: Term search  Range searchReturns documents that contain terms within a provided range.The following example returns books that have at least 45 reviews.var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Query(q =&gt; q.Range(r =&gt; r        .NumberRange(nr =&gt; nr.Field(f =&gt; f.num_reviews).Gte(45)))    ));DumpRequest(searchResponse);PrettyPrint(searchResponse);‚öôÔ∏èOutput: Range search  Prefix searchReturns documents that contain a specific prefix in a provided field.üí° Docs: Read more.var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Query(q =&gt; q.Prefix(p =&gt; p        .Field(f =&gt; f.Title)        .Value(\"java\"))    ));DumpRequest(searchResponse);PrettyPrint(searchResponse);‚öôÔ∏èOutput: Prefix search  Fuzzy searchReturns documents that contain terms similar to the search term, as measured by a Levenshtein edit distance.An edit distance is the number of one-character changes needed to turn one term into another. These changes can include:  Changing a character (box ‚Üí fox)  Removing a character (black ‚Üí lack)  Inserting a character (sic ‚Üí sick)  Transposing two adjacent characters (act ‚Üí cat)üí° Docs: Read more.var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Query(q =&gt; q        .Fuzzy(f =&gt; f            .Field(ff =&gt; ff.Title)            .Value(\"pyvascript\")        )    ));DumpRequest(searchResponse);PrettyPrint(searchResponse);‚öôÔ∏èOutput: Fuzzy search  Combining Query ConditionsCompound queries wrap other compound or leaf queries, either to combine their results and scores, or to change their behaviour. They also allow you to switch from query to filter context, but that will be covered later in the Filtering section.bool.must (AND)The clauses must appear in matching documents and will contribute to the score. This effectively performs an ‚ÄúAND‚Äù logical operation on the given sub-queries.var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Query(q =&gt; q.Bool(b =&gt; b        .Must(m =&gt; m            .Term(t =&gt; t                .Field(f =&gt; f.Publisher)                .Value(\"addison-wesley\")            ),            m =&gt; m            .Term(t =&gt; t                .Field(f =&gt; f.Authors)                .Value(\"richard helm\")            )        ))    ));DumpRequest(searchResponse);PrettyPrint(searchResponse);‚öôÔ∏èOutput: bool.must (AND)  bool.should (OR)The clause should appear in the matching document. This performs an ‚ÄúOR‚Äù logical operation on the given sub-queries.var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Query(q =&gt; q.Bool(b =&gt; b        .Should(m =&gt; m            .Term(t =&gt; t                .Field(f =&gt; f.Publisher)                .Value(\"addison-wesley\")            ),            m =&gt; m            .Term(t =&gt; t                .Field(f =&gt; f.Authors)                .Value(\"richard helm\")            )        ))    ));DumpRequest(searchResponse);PrettyPrint(searchResponse);‚öôÔ∏èOutput: bool.should (OR)  FilteringIn a filter context, a query clause answers the question ‚ÄúDoes this document match this query clause?‚Äù The answer is a simple Yes or No‚Äâ‚Äî‚Äâno scores are calculated. Filter context is mostly used for filtering structured data, for example:  Does this timestamp fall into the range 2015 to 2016?  Is the status field set to ‚Äúpublished‚Äù?Filter context is in effect whenever a query clause is passed to a filter parameter, such as the filter or must_not parameters in the bool query.üí° Docs: Read more.bool.filterThe clause (query) must appear for the document to be included in the results. Unlike query context searches such as term, bool.must or bool.should, a matching score isn‚Äôt calculated because filter clauses are executed in filter context.var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Query(q =&gt; q.Bool(b =&gt; b        .Filter(m =&gt; m            .Term(t =&gt; t                .Field(f =&gt; f.Publisher)                .Value(\"prentice hall\")            )        ))    ));DumpRequest(searchResponse);PrettyPrint(searchResponse);‚öôÔ∏èOutput: bool.filter  bool.must_notThe clause (query) must not appear in the matching documents. Because this query also runs in filter context, no scores are calculated; the filter just determines if a document is included in the results or not.var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Query(q =&gt; q.Bool(b =&gt; b        .MustNot(m =&gt; m            .Range(r =&gt; r.NumberRange(nr =&gt; nr.Field(f =&gt; f.num_reviews).Lte(45)))        ))    ));DumpRequest(searchResponse);PrettyPrint(searchResponse);‚öôÔ∏èOutput: bool.must_not  Using Filters with QueriesFilters are often added to search queries with the intention of limiting the search to a subset of the documents. A filter can cleanly eliminate documents from a search, without altering the relevance scores of the results.The next example returns books that have the word ‚Äújavascript‚Äù in their title, only among the books that have more than 45 reviews.var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Query(q =&gt; q.Bool(b =&gt; b        .Must(m =&gt; m            .Match(t =&gt; t                .Field(f =&gt; f.Title).Query(\"javascript\")            )        )        .MustNot(m =&gt; m            .Range(r =&gt; r.NumberRange(nr =&gt; nr.Field(f =&gt; f.num_reviews).Lte(45)))        ))    ));DumpRequest(searchResponse);PrettyPrint(searchResponse);‚öôÔ∏èOutput: Filters and Queries  ConclusionThat is all for now! üéâ We‚Äôve covered the basics of querying and filtering in Elasticsearch using the Elastic.Clients.Elasticsearch NuGet package. Understanding how to construct queries and filters can help you build powerful search capabilities in your applications.  üôå I hope you found it helpful. If you have any questions, please feel free to reach out. If you‚Äôd like to support my work, a star on GitHub would be greatly appreciated! üôèReferences  https://github.com/NikiforovAll/elasticsearch-dotnet-playground/tree/main/src/elasticsearch-getting-started  https://github.com/NikiforovAll/elasticsearch-dotnet-playground/blob/main/src/elasticsearch-getting-started/01-keyword-querying-filtering.ipynb"
    },
  
    {
      "id": "28",
      "title": "Semantic Search with Elasticsearch in .NET",
      "url": "/dotnet/2024/10/19/semantic-search-via-elastic-dotnet.html",
      "date": "October 19, 2024",
      "categories": ["dotnet"],
      "tags": ["dotnet","elasticsearch","ai","rag","search"],
      "shortinfo": "This post demonstrates how to perform Semantic Search using Elasticsearch and Polyglot Notebooks",
      "content": "TL;DRIn this post, we will explore how to perform Semantic Search in .NET.Source code: https://github.com/NikiforovAll/elasticsearch-dotnet-playground/blob/main/src/elasticsearch-getting-started/00-quick-start.ipynb  TL;DR  Introduction  Getting Started  Initialize the Elasticsearch Client  Generate Embeddings  Index Data  Making queries          Semantic Search      Semantic Search and Filtering        Conclusion  ReferencesIntroductionSemantic search is a technique used to improve search accuracy by understanding the contextual meaning of terms within a search query. Unlike traditional keyword-based search, which matches exact words, semantic search aims to understand the intent and contextual meaning behind the words. This approach improves search results and provides more relevant information to the user.Getting StartedI‚Äôve prepared a Jupyter notebook that demonstrates how to perform a semantic search using the Elastic.Clients.Elasticsearch. You can find the source code here.    üìù Down below, I will guide you through the main steps of the notebook:  Initialize the Elasticsearch Client  Generate Embeddings  Index Data  Making queriesInitialize the Elasticsearch ClientWe can use Testcontainers to run Elasticsearch from the notebook. Here is how you can do it:var elasticsearchContainer =  new ElasticsearchBuilder()    .WithPortBinding(9200, 9200)    .WithPortBinding(9300, 9300)    .WithReuse(true)    .Build();await elasticsearchContainer.StartAsync();var connectionString = elasticsearchContainer.GetConnectionString(); // https://elastic:elastic@127.0.0.1:9200/Now, we can initialize the Elasticsearch client:var elasticSettings = new ElasticsearchClientSettings(connectionString)    .DisableDirectStreaming()    .ServerCertificateValidationCallback(CertificateValidations.AllowAll);var client = new ElasticsearchClient(elasticSettings);Let‚Äôs see if it works:var info = await client.InfoAsync();DumpResponse(info);And here is the output:{  \"name\": \"35937efa7867\",  \"cluster_name\": \"docker-cluster\",  \"cluster_uuid\": \"IZOZjoDyRpKHFN1sNGjs1g\",  \"version\": {    \"number\": \"8.6.1\",    \"build_flavor\": \"default\",    \"build_type\": \"docker\",    \"build_hash\": \"180c9830da956993e59e2cd70eb32b5e383ea42c\",    \"build_date\": \"2023-01-24T21:35:11.506992272Z\",    \"build_snapshot\": false,    \"lucene_version\": \"9.4.2\",    \"minimum_wire_compatibility_version\": \"7.17.0\",    \"minimum_index_compatibility_version\": \"7.0.0\"  },  \"tagline\": \"You Know, for Search\"}üôå Everything looks good so far, let‚Äôs continue and see how to generate and embeddings.Generate EmbeddingsEmbeddings are a type of representation for text where words, phrases, or even entire documents are mapped to vectors of real numbers. These vectors capture the semantic meaning of the text, allowing for more nuanced and context-aware comparisons between different pieces of text.Traditional keyword-based search might not recognize ‚Äúcar‚Äù and ‚Äúautomobile‚Äù as related, but embeddings will map these words to similar vectors, understanding that they are synonyms and thus improving search relevance.We can use Microsoft.Extensions.AI.OpenAI and Azure.AI.OpenAI NuGet packages to create an instance of IEmbeddingGenerator:var client = new AzureOpenAIClient(new Uri(envs[\"AZURE_OPENAI_ENDPOINT\"]), new ApiKeyCredential(envs[\"AZURE_OPENAI_APIKEY\"]));IEmbeddingGenerator&lt;string,Embedding&lt;float&gt;&gt; generator = client.AsEmbeddingGenerator(modelId: \"text-embedding-3-small\");We can implement ToEmbedding method to convert a string to an embedding:async Task&lt;float[]&gt; ToEmbedding(string text) {    var dimension = 384;    GeneratedEmbeddings&lt;Embedding&lt;float&gt;&gt; embeddings = await generator        .GenerateAsync(text, new EmbeddingGenerationOptions{            AdditionalProperties = new AdditionalPropertiesDictionary{                {\"dimensions\", dimension}            }        });    return embeddings.First().Vector.ToArray();}float[] embedding = await ToEmbedding(\"The quick brown fox jumps over the lazy dog\");display($\"Dimensions length = {embedding.Length}\");Index DataAssume we have a dataset with information about popular programming books. The data model can be defined as following:public class Book{    [JsonPropertyName(\"title\")]    public string Title { get; set; }    [JsonPropertyName(\"summary\")]    public string Summary { get; set; }    [JsonPropertyName(\"authors\")]    public List&lt;string&gt; Authors { get; set; }    [JsonPropertyName(\"publish_date\")]    public DateTime publish_date { get; set; }    [JsonPropertyName(\"num_reviews\")]    public int num_reviews { get; set; }    [JsonPropertyName(\"publisher\")]    public string Publisher { get; set; }    public float[] TitleVector { get; set; }}Now, we can create an index with the following mapping:var indexDescriptor = new CreateIndexRequestDescriptor&lt;Book&gt;(\"book_index\")    .Mappings(m =&gt; m        .Properties(pp =&gt; pp            .Text(p =&gt; p.Title)            .DenseVector(                Infer.Property&lt;Book&gt;(p =&gt; p.TitleVector),                d =&gt; d.Dims(dimension).Index(true).Similarity(DenseVectorSimilarity.Cosine))            .Text(p =&gt; p.Summary)            .Date(p =&gt; p.publish_date)            .IntegerNumber(p =&gt; p.num_reviews)            .Keyword(p =&gt; p.Publisher)        )    );await client.Indices.CreateAsync&lt;Book&gt;(indexDescriptor);Note that we are using the DenseVector type to store the embeddings. We also specify the Cosine similarity function to compare the vectors.Let‚Äôs download the test data and calculate ‚ÄúTitle‚Äù field embeddings:var http = new HttpClient();var url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/notebooks/search/data.json\";var books =  await http.GetFromJsonAsync&lt;Book[]&gt;(url);foreach (var book in books){    book.TitleVector = await ToEmbedding(book.Title);}Now we can use Bulk API to upload data to Elasticsearch.await client.BulkAsync(\"book_index\", d =&gt; d.IndexMany&lt;Book&gt;(books, (bd, b) =&gt; bd.Index(\"book_index\")));Making queriesLet‚Äôs use the keyword search to see if we have relevant data indexed. For example, we can search for books that contain ‚ÄúJavaScript‚Äù in the title:var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Index(\"book_index\")    .Query(q =&gt; q.Match(m =&gt; m.Field(f =&gt; f.Title).Query(\"JavaScript\"))));DumpRequest(searchResponse);searchResponse.Documents.Select(x =&gt; x.Title).DisplayTable();‚öôÔ∏èOutput:  Semantic SearchüéØ We want to perform a semantic search for books that are similar to a given query. We embed the query and perform a search.Let‚Äôs say we want to find ‚Äújavascript books‚Äù. We can use the KNN search to find the top 5 books that are similar to the searchQuery.var searchQuery = \"javascript books\";var queryEmbedding = await ToEmbedding(searchQuery);var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Index(\"book_index\")    .Knn(d =&gt; d        .Field(f =&gt; f.TitleVector)        .QueryVector(queryEmbedding)        .k(5)        .NumCandidates(100)));var threshold = 0.7;searchResponse.Hits    .Where(x =&gt; x.Score &gt; threshold)    .Select(x =&gt; new { x.Source.Title, x.Score })    .DisplayTable();‚öôÔ∏èOutput:  Semantic Search and FilteringFilter context is mostly used for filtering structured data. For example, use filter context to answer questions like:  Does this timestamp fall into the range 2015 to 2016?  Is the status field set to ‚Äúpublished‚Äù?Filter context is in effect whenever a query clause is passed to a filter parameter, such as the filter or must_not parameters in a bool query.Learn more about filter context in the Elasticsearch docs.The example below retrieves the top books that are similar to ‚Äújavascript books‚Äù based on their title vectors, and also Addison-Wesley as publisher.var searchQuery = \"javascript books\";var queryEmbedding = await ToEmbedding(searchQuery);var searchResponse = await client.SearchAsync&lt;Book&gt;(s =&gt; s    .Index(\"book_index\")    .Knn(d =&gt; d        .Field(f =&gt; f.TitleVector)        .QueryVector(queryEmbedding)        .k(5)        .NumCandidates(100)        .Filter(f =&gt; f.Term(t =&gt; t.Field(p =&gt; p.Publisher).Value(\"addison-wesley\")))     ));searchResponse.Hits    .Select(x =&gt; new { x.Source.Title, x.Score })    .DisplayTable(); ‚öôÔ∏èOutput:  Conclusion  üôå I hope you found it helpful. If you have any questions, please feel free to reach out. If you‚Äôd like to support my work, a star on GitHub would be greatly appreciated! üôèReferences  https://github.com/elastic/elasticsearch-labs  https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html  https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource  https://www.galileo.ai/blog/mastering-rag-how-to-select-an-embedding-model"
    },
  
    {
      "id": "29",
      "title": "10 Lessons I Learned from Using Aspire in Production",
      "url": "/dotnet/aspire/2024/09/30/aspire-lessons-learned.html",
      "date": "September 30, 2024",
      "categories": ["dotnet","aspire"],
      "tags": ["dotnet","aspire"],
      "shortinfo": "A collection of lessons learned from using .NET Aspire in production.",
      "content": "TL;DRI have been using .NET Aspire for a while now and have learned a lot along the way. In this blog post, I will share some of the lessons I have learned from using .NET Aspire in production.  .NET Aspire version: 8.2.1    TL;DR  Case Study  Lesson 1. Adding Aspire is quite straightforward and incremental  Lesson 2. Be ready to write custom integrations  Lesson 3. Aspire workload should be installed every time you run the CI/CD pipeline  Lesson 4. Managing Startup Dependencies is a must have  Lesson 5. Aspire project can take some time to boot up  Lesson 6. It can be challenging to run things without Aspire  Lesson 7. There is no easy way to run a partial setup  Lesson 8. Using Podman instead of Docker can be troublesome  Lesson 9. Aspire client integrations require IHostApplicationBuilder  Lesson 10. Learn by example  ConclusionCase StudyAssume we have a project, it is a web application with advanced enterprise search capabilities, utilizing Elasticsearch for data querying. It features a background jobs for scraping and processing data from various sources, storing structured data in PostgreSQL and files in Azure Blob Storage. Additionally, we have a CLI tool for essential administrative tasks such as reindexing and database migrations, ensuring system maintenance and automation.graph LR;    subgraph BS [Backing Services]        B[Elasticsearch]        C[PostgreSQL]        D[Azure Blob Storage]    end    A[Web App] --&gt; B[Elasticsearch]    E[Jobs] --&gt; B    E --&gt; C[PostgreSQL]    E --&gt; D[Blob Storage]    F[CLI] --&gt; B    F --&gt; C    F --&gt; DüéØ Our objective was to improve the developer experience by integrating .NET Aspire. We aimed to create a seamless so called F5 setup for every team member.üí° An ‚ÄúF5 setup‚Äù refers to a development environment configuration where a developer can simply press the F5 key (commonly used to start debugging in many IDEs, including Visual Studio) to run the application. This setup aims to streamline the development process by ensuring that all necessary services, dependencies, and configurations are automatically handled, allowing developers to focus on coding without worrying about manual setup or configuration.  For existing developers, this means more efficient development and debugging workflows.  For new developers, this means they can start working on the project without the hassle of manually configuring and setting up the underlying infrastructure. This approach ensures a smoother onboarding process and a more efficient development workflow.Furthermore, AppHost serves as a composition root of the solution. If done correctly, you can reason what this solution is about just by looking at it.üöÄ Down below, I will share some of the lessons learned from this experience. Let‚Äôs dive in!Lesson 1. Adding Aspire is quite straightforward and incrementalIt turns out that transitioning your entire project to Aspire doesn‚Äôt have to be an all-or-nothing endeavor. You can begin by adding the AppHost and progressively migrate your backing services at your own pace.For example, if you already have Elasticsearch configured in a non-Aspire way, you don‚Äôt have to use the official Aspire Elasticsearch client integration. You can just use the hosting integration and write adapter code to bridge the gap between Aspire and your existing code. My suggestion is to make these changes to AppHost to limit the amount of changes and avoid altering existing working code.üí° Additionally, we didn‚Äôt really want to change the way we configure and manage cross-cutting concerns, so we abandoned the idea of using ServiceDefaults altogether.We simply added an OpenTelemetry integration to enable logs, traces, and metrics in the Aspire Dashboard.üóíÔ∏è References:  Tutorial: Add .NET Aspire to an existing .NET appLesson 2. Be ready to write custom integrationsWe adopted Aspire at an early stage, and during our migration to Aspire, there was no official integration for Elasticsearch. Consequently, we developed a custom integration to configure Elasticsearch with Aspire by creating a custom ElasticsearchResource.Custom hosting integrations often depend on an existing Dockerfile or Docker image. Therefore, creating one involves using IDistributedApplicationBuilder to define the Docker equivalents.public static IResourceBuilder&lt;ElasticsearchResource&gt; AddElasticsearch(    this IDistributedApplicationBuilder builder,    string name,    IResourceBuilder&lt;ParameterResource&gt;? password = null,    int? port = null,    int? internalPort = null){    ArgumentNullException.ThrowIfNull(builder);    var passwordParameter =        password?.Resource        ?? ParameterResourceBuilderExtensions.CreateDefaultPasswordParameter(            builder,            $\"{name}-password\"        );    var elasticsearch = new ElasticsearchResource(name, passwordParameter);    return builder        .AddResource(elasticsearch)        .WithImage(ElasticsearchContainerImageTags.Image, ElasticsearchContainerImageTags.Tag)        .WithImageRegistry(ElasticsearchContainerImageTags.Registry)        .WithHttpEndpoint(            targetPort: ElasticsearchPort,            port: port,            name: ElasticsearchResource.PrimaryEndpointName        )        .WithEndpoint(            targetPort: ElasticsearchInternalPort,            port: internalPort,            name: ElasticsearchResource.InternalEndpointName        )        .WithEnvironment(\"discovery.type\", \"single-node\")        .WithEnvironment(\"xpack.security.http.ssl.enabled\", \"false\")        .WithEnvironment(\"xpack.security.enabled\", \"true\")        .WithEnvironment(\"ELASTIC_PASSWORD\", elasticsearch.PasswordParameter);}As you can see above, we created a custom AddElasticsearch extension method to configure Elasticsearch with Aspire. This method adds an ElasticsearchResource to the IDistributedApplicationBuilder and configures the necessary settings for the Elasticsearch Docker container.It is a good idea to put separate custom integrations in a separate project to keep the codebase clean and organized. This approach also makes it easier to manage and maintain custom integrations as your project grows.üí°To gracefully migrate to Aspire and limit the amount of changes, we employed a previously mentioned technique (Lesson 1). I call this approach hosting integration adapter method.You can provide an overload of WithReference to configure a custom integration in an opinionated way.For example, here is how to provide a connection string and environment variables simultaneously:public static IResourceBuilder&lt;TDestination&gt; WithReference&lt;TDestination&gt;(    this IResourceBuilder&lt;TDestination&gt; builder,    IResourceBuilder&lt;ElasticsearchResource&gt; source)    where TDestination : IResourceWithEnvironment{    ArgumentNullException.ThrowIfNull(source);        var resource = source.Resource;    const string Prefix = \"Elastic\";    // Add connection string as is (Aspire-way)    IResourceBuilder&lt;IResourceWithConnectionString&gt; connectionSTringResource = source;    builder.WithReference(connectionSTringResource);    // Adapter code (Custom-way)    builder.WithEnvironment($\"{Prefix}__Url\", resource.PrimaryEndpoint);    builder.WithEnvironment($\"{Prefix}__Login\", ElasticsearchResource.UserName);    builder.WithEnvironment($\"{Prefix}__Password\", resource.PasswordParameter.Value);    return builder;}The benefit of this approach is that you can just conform to an existing way you configure your services and minimize the changes.üí° Often, managing backing services requires additional tools such as pgAdmin for PostgreSQL, Kibana for Elasticsearch, or Azure Storage Explorer for Azure Blob Storage. A growing pattern is to add tooling support for hosting integrations using With{ToolName}. Here is an example:public static IResourceBuilder&lt;T&gt; WithKibana&lt;T&gt;(    this IResourceBuilder&lt;T&gt; builder,    Action&lt;IResourceBuilder&lt;KibanaContainerResource&gt;&gt;? configureContainer = null,    string? containerName = null)    where T : ElasticsearchResource{  // implementation goes here}This approach works well because it is easier to encapsulate two Aspire resources and use them together since their lifecycles are the same. I call this approach hosting integration tool.References:  .NET Aspire integrations overviewLesson 3. Aspire workload should be installed every time you run the CI/CD pipelineInstalling the Aspire workload is necessary for it to function, but this can be cumbersome, especially in a CI/CD pipeline. The workload must be installed every time the pipeline runs, which can be time-consuming and tedious.To address this, we created a separate solution for Aspire AppHost and its dependencies, excluding it from the main solution. This effectively removes the Aspire component from the CI/CD pipeline.üôåüÜï Luckily, you don‚Äôt have to do it in Aspire 9.0. A standalone MSBuild SDK - Aspire.AppHost.Sdk should be referenced from the AppHost project.See the related GitHub issue: Remove Aspire.Hosting.SDK from the Workload #5444Lesson 4. Managing Startup Dependencies is a must haveIn real-world applications, managing startup dependencies is crucial. For instance, you may need to ensure that a database is up and running before starting a migration CLI tool. Once the migration is complete, you might need to start the web application.Although Aspire does not support this out of the box, you can achieve it using Aspire‚Äôs extension points.For a detailed guide, refer to my blog post Managing Startup Dependencies in .NET Aspire. Basically, I have created a NuGet package Nall.Aspire.Hosting.DependsOn to address this issue.Here is an example of how you can use it:var builder = DistributedApplication.CreateBuilder(args);var dbServer = builder    .AddPostgres(\"db-server\")    .WithHealthCheck();dbServer.WithPgAdmin(c =&gt; c.WithHostPort(5050).WaitFor(dbServer));var db = dbServer.AddDatabase(\"db\");var migrator = builder.AddProject&lt;Projects.MigrationService&gt;(\"migrator\")    .WithReference(db)    .WaitFor(db);var api = builder.AddProject&lt;Projects.Api&gt;(\"api\")    .WithReference(db)    .WaitForCompletion(migrator);builder.Build().Run();In the code above, we use WithHealthCheck, WaitFor, WaitForCompletion to define dependency graph. This ensures that the database is up and running before starting the migration service, and the migration service is completed before starting the API.üôåüÜï I have another good news for you! In Aspire 9.0, this is now a built-in feature.See the related GitHub issue: WaitFor/WaitForCompletion implementation. #5394Lesson 5. Aspire project can take some time to boot upSince there is no hot-reload support for AppHost, you have to rebuild and restart the project every time you make a change. This can be time-consuming, especially if your project is large and has many dependencies.In our case, the Elasticsearch container took some time to start up, which slowed down the development process. It can take 30-60 seconds to start up, which at first glance seems like not a lot, but trust me, things add up.To overcome this, we‚Äôve implemented a mechanism to run only infrastructure services when needed. It is like a docker-compose.infrastructure.yml file, but for Aspire.üôåüÜï There is a solution to this problem! One of my favorite improvements in Aspire 9.0 is the - option to set up and keep resources (containers) after AppHost shutdown #923. It saves us from the headache of restarting Elasticsearch every time we make a change.Lesson 6. It can be challenging to run things without AspireSometimes you need to run things without Aspire, but it is not easy because everything is tightly integrated with Aspire, and a lot of things are abstracted away.For example, assume I want to run a Blazor application in hot-reload mode without starting the Aspire AppHost but still utilize the infrastructure provisioned and managed by Aspire.To overcome this, we created a specific project called EnvDumper that dumps the environment variables and connection strings to a file called appsettings.Development.Aspire.json. The idea is that you can run the application without Aspire by using this file.So the scenario is like this:  You exclude a project from the Aspire AppHost by using configuration.  When you do this, the excluded projects are replaced by the EnvDumper project.  The EnvDumper project dumps the environment variables and connection strings to a file.  You add the corresponding file to an excluded project and run it without Aspire in hot-reload mode.From AppHost perspective it looks like this:var builder = DistributedApplication.CreateBuilder(args);builder.RegisterProject&lt;Projects.WebApp&gt;(\"web-app\")    .WithReference(db)    .WithReference(blobStorage)    .WithReference(elastic);Here is RegisterProject extension method:public static IResourceBuilder&lt;IResourceWithEnvironment&gt; RegisterProject&lt;TProject&gt;(    this IDistributedApplicationBuilder builder,    string name)    where TProject : IProjectMetadata, new(){    IResourceBuilder&lt;IResourceWithEnvironment&gt;? project =         IsExcludedProject(builder, name)        ? builder.AddProject&lt;Projects.EnvDumper&gt;(componentName)        : builder.AddProject&lt;TProject&gt;(name);    return project;}üôåüÜï Alternatively, to speed up the development process, consider exploring the new feature in Aspire 9.0 - Add support for restarting services from the dashboard #295. Although I haven‚Äôt tried it yet, I believe this feature will address many of my requirements.Lesson 7. There is no easy way to run a partial setupSometimes you need to run a partial setup, for example, to test a specific feature or component without starting the entire application. However, Aspire does not provide an easy way to do this out of the box. As you already know, we added the feature to exclude projects from the Aspire AppHost by using configuration. (See Lesson 5)Lesson 8. Using Podman instead of Docker can be troublesomeüí• Networking issues can arise when using Podman instead of Docker from Windows. For example, we encountered problems with the Kibana container not being able to connect to the Elasticsearch container.The problem exists to this day, and there are no workarounds available.See the related GitHub issue: Podman-hosted containers may not be able to reach Aspire services #4136Lesson 9. Aspire client integrations require IHostApplicationBuilderAspire client integrations require IHostApplicationBuilder, which might not always be available. For instance, if you want to use client integrations and add dependencies to IServiceCollection in a console application, or if you have existing code that uses IServiceCollection and ConfigurationManager separately, you may find out your self in a tricky situation. Now, you have to refactor entire codebase just to use a client integration.This is what happened to us, take a look at:public static void AddNpgsqlDataSource(    this IHostApplicationBuilder builder,    string connectionName,    Action&lt;NpgsqlSettings&gt;? configureSettings = null,    Action&lt;NpgsqlDataSourceBuilder&gt;? configureDataSourceBuilder = null);As you can see, we should provide an instance of IHostApplicationBuilder to use this extension method. This was a problem in our case because we had a lot of existing code that uses IServiceCollection, and since our solution consists of a hierarchy of projects with different configurations, it takes some time to refactor everything.ü§î I think this design choice breaks the Postel‚Äôs Law principle, which states that you should be liberal in what you accept and conservative in what you send. In other words, it should be easy to use and hard to misuse. I would imagine an overload that does less but requires IServiceCollection instead. It would make client integrations more composable.Lesson 10. Learn by exampleThe best way to learn how to use Aspire is by example. I recommend checking out the official Aspire samples and the Aspire GitHub repository.‚≠ê Here are some of my favorite reference applications:  eShopSupport - A reference .NET application using AI for a customer support ticketing system  eShop - A reference .NET application implementing an eCommerce site  eShopOnAzure - A variant of eShop that uses Azure services  cecilphillip/shadowShop - A modified version of the Aspire Shop sample application that adds integration with Stripe for payment processing, temporal for durable workflows, other customer Aspire Integrations  thangchung/practical-dotnet-aspire - The practical .NET Aspire builds on the coffeeshop app business domain  Aspirant - Aspirant is a set of extensions and experiments for .NET Aspire App Host projectsConclusionAspire is more than just an alternative to docker-compose; it is a comprehensive ecosystem designed to simplify the building, running, and management of your applications. The key feature of Aspire is its programmability, which enables you to extend and customize the platform to meet your specific requirements.  üôå I hope you found it helpful. If you have any questions, please feel free to reach out."
    },
  
    {
      "id": "30",
      "title": "A Tyrant Guide to Code Quality Gates featuring CSharpier, Husky.NET, and SonarCloud",
      "url": "/dotnet/2024/09/14/quality-gates-dotnet.html",
      "date": "September 14, 2024",
      "categories": ["dotnet"],
      "tags": ["dotnet","cicd"],
      "shortinfo": "Learn how to enforce code quality gates in your .NET projects.",
      "content": "TL;DRLearn how to enforce code quality gates in your .NET projects.  TL;DR  Motivation          Developer Feedback Loop        Code Quality Aspects          Code Formatting      Spelling      Coding Style and Code Analysis      Code Analysis Tools        Demo  Conclusion  ReferencesSource code: https://github.com/NikiforovAll/quality-gateways-demo-dotnet  MotivationIn today‚Äôs fast-paced development world, maintaining high code quality is important, especially when working in a team environment. One effective way to ensure this is by enforcing code quality gates.ü´∏Code quality gates are automated checks that ensure code meets predefined quality standards before it is merged into the main codebase. They help catch issues early in the development process, reducing the likelihood of bugs and technical debt.üí° From my experience, maintaining consistent code base is crucial for the long-term success of a project. It helps developers understand the codebase better, reduces the time spent on code reviews, and makes it easier to onboard new team members.Small issues and inconsistencies are left unaddressed in the codebase can create a perception of low quality and encourage the introduction of more issues over time. This can result in a deteriorating codebase and increased technical debt.It is so called ‚Äúbroken windows theory ü™ü‚Äù in the context of software development.üí° The quality of code base reflects the team‚Äôs professionalism and commitment to delivering high-quality software.On another hand, too strict code quality gates can slow down the development process and frustrate developers. It is important to strike a balance between enforcing quality standards and maintaining developer productivity.For example, it should be possible to bypass code quality checks in exceptional cases. However, ‚Äútechnical debt‚Äù management practices should be in place to ensure that these exceptions eventually get addressed.There are two main aspects of enforcing quality gates:  Make sure that code quality is enforced, so it is not possible to merge code that does not meet the quality standards.  Keep the feedback loop as short as possible, so developers can fix the issues as soon as possible. ‚ùó Beware, developers will rebel against quality gates if it impacts their productivity and developer experience (DevEX).Developer Feedback LoopQuality gates should provide feedback to developers as soon as possible. But it is important to strike a balance between providing feedback and not overwhelming developers with too many notifications and increased compilation times.We can implement quality gates at various stages of the inner development loop:  üìùIDE/Linter  ü™ùPre-commit hooks  ‚öôÔ∏èCI/CD          Build      Automated tests (unit, integration, end-to-end, performance, etc.)      Code quality checks      Security checks        ü§ºCode ReviewThe more important the check, the earlier it should be in the development loop. For example, it is better to catch syntax errors and formatting issues in the IDE or pre-commit hooks rather than in the CI/CD pipeline.Code Quality AspectsThere are several aspects of code quality that can be enforced by quality gates:  Consistent formatting  Consistent code style  High code quality  No code smells  No spelling mistakes  No security vulnerabilities  No performance issues  No bugsCode FormattingI prefer csharpier, because it is fast and has a good default configuration. It is an opinionated code formatter for C#. It is similar to Prettier, and the philosophy is to have a single way to format the code.Personally, I don‚Äôt want to spend time discussing code formatting in code reviews. I want to have a single way to format the code, and I want to have it done automatically.The only option you can argue about is the line length. The default line length is 120 characters, but you can change it to 80 characters if you prefer.SpellingSpelling mistakes can be detected by the cspell linter. It is a fast and configurable spell checker that runs in the terminal.Coding Style and Code AnalysisCoding style can be enforced by standard (Roslyn) analyzers. Code analysis rules have various configuration options. Some of these options are specified as key-value pairs in an analyzer configuration file (.editorconfig).You can configure the severity level for any rule, including code quality rules and code style rules. For example, to enable a rule as a warning, add the following key-value pair to an analyzer configuration file:[*.{cs,vb}]# IDE0040: Accessibility modifiers requireddotnet_diagnostic.IDE0040.severity = warning# or# IDE0040: Accessibility modifiers requireddotnet_style_require_accessibility_modifiers = always:warning.NET compiler platform  analyzers inspect your code for code quality and style issues. Starting in .NET 5, these analyzers are included with the .NET SDK and you don‚Äôt need to install them separately.While the .NET SDK includes all code analysis rules, only some of them are enabled by default. The analysis mode determines which, if any, set of rules to enable. You can choose a more aggressive analysis mode where most or all rules are enabled - &lt;AnalysisMode&gt;All&lt;/AnalysisMode&gt;.There are two other options to enforce coding style I would like to mention:  TreatWarningsAsErrors - All warning messages are instead reported as errors. The build process halts (no output files are built).  EnforceCodeStyleInBuild - .NET code style analysis is disabled, by default, on build for all .NET projects.From my experience, it can negatively impact the local build time, so I prefer to run the checks in the CI/CD pipeline.üí° The only exception to this is to use &lt;WarningsAsErrors&gt;Nullable&lt;/WarningsAsErrors&gt; to treat nullable warnings as errors.There is a way to write your own custom analyzers. You can use the Roslyn SDK to create custom rules and code fixes. It is a great way to enforce custom coding standards and best practices used in your organization.‚≠ê Additionally, you may consider the following analyzers created by the community:  https://github.com/SonarSource/sonar-dotnet  https://github.com/dotnet/roslynator  https://github.com/bkoelman/CSharpGuidelinesAnalyzer  https://github.com/meziantou/Meziantou.Analyzer  https://github.com/SergeyTeplyakov/ErrorProne.NET‚òùÔ∏è Note, you don‚Äôt have to adopt analyzers entirely. You can use only the rules that make sense for your project.üß† My suggestion is to work on the code quality and code style rules together with the team during the code review process. The rules should be discussed and agreed upon by the team.Code Analysis ToolsIf you want to go further, you can use code analysis tools. The difference between code analysis tools and code analyzers is that code analysis tools are external tools that analyze your codebase and provide insights into the quality of your code.Some of the popular tools are:  SonarCloud - SonarCloud is a cloud-based code analysis service that automatically detects bugs, vulnerabilities, and code smells in your code. It integrates with GitHub, Azure DevOps, and Bitbucket.  CodeQL - CodeQL is a semantic code analysis engine that allows you to write queries to find vulnerabilities in your code.  NDepend - NDepend is a static analysis tool that helps you manage complex .NET codebases and achieve high code quality.Here is an example of a SonarCloud report:  üí° You can include SonarCloud check as part of a CI/CD pipeline. Use /d:sonar.qualitygate.wait=true option. Otherwise, the CI/CD pipeline will not wait for the SonarCloud analysis to finish.By default, Sonar configures pretty strict rules called ‚ÄúSonar Way‚Äù:  üëé The downside of using different code analysis tools is that you have to configure them separately. You have to configure the rules, the severity levels, and the exclusions. Ideally, I want to have a single configuration file (aka source of truth) that configures all the code quality checks.DemoCheck out the example I‚Äôve prepared - https://github.com/NikiforovAll/quality-gateways-demo-dotnetüéØ This project, demonstrates how to enforce quality gates in a .NET project using the following tools:  Code Analysis - .NET compiler platform (Roslyn) analyzers (shipped as part of the .NET SDK).  SonarAnalyzer.CSharp - Custom analyzers for C# that are part of SonarCloud.  CSharpier - CSharpier is an opinionated code formatter for C#.  cspell - cspell is a command line tool that checks the spelling of your code.  Husky.Net - Husky.Net is a .NET tool that allows you to run tasks before committing the code.  GitHub Actions - Automate, customize, and execute your software development workflows.  SonarCloud - SonarCloud is a cloud-based code analysis service that automatically detects bugs, vulnerabilities, and code smells in your code.Assume we have initial code that does not meet the quality standards.    First of all, the developer will receive feedback from the IDE and linter.  Before committing the code, Husky.NET will run pre-commit hooks. And if any of the checks fail, the commit will be rejected.  The CI/CD pipeline will run the same checks (and maybe other checks such as automated tests). And if any of the checks fail, the pipeline will fail.Prerequisites:  dotnet tool restore - installs husky, csharpier  npm install -g cspellThe benefit of using Husky.NET is that it allows you to run pre-commit hooks in a cross-platform way. It is a .NET port of the popular husky tool for Node.js.Let‚Äôs run it locally to see how it works:dotnet run huskyThis command runs the following checks sequentially:{    \"$schema\": \"https://alirezanet.github.io/Husky.Net/schema.json\",    \"tasks\": [        {            \"name\": \"format\",            \"group\": \"pre-commit\",            \"command\": \"dotnet\",            \"args\": [\"csharpier\", \".\", \"--check\"]        },        {            \"name\": \"style\",            \"group\": \"pre-commit\",            \"command\": \"dotnet\",            \"args\": [\"format\", \"style\", \".\", \"--verify-no-changes\"]        },        {            \"name\": \"analyzers\",            \"group\": \"pre-commit\",            \"command\": \"dotnet\",            \"args\": [\"format\", \"analyzers\", \".\", \"--verify-no-changes\"]        },        {            \"name\": \"spelling\",            \"group\": \"pre-commit\",            \"command\": \"cspell\",            \"args\": [\"lint\", \"**.cs\", \"--no-progress\", \"--no-summary\"]        }    ]}We can run the checks individually:dotnet husky run --name formatdotnet husky run --name styledotnet husky run --name analyzersdotnet husky run --name spellingNow, assume the developer ignores the warning and somehow commits the code and creates a pull request. The CI/CD pipeline will run the same checks, and if any of the checks fail, the pipeline will fail.üí° I configured parallel execution for code quality gates, which helps to receive feedback faster.  Now let‚Äôs fix the issues, by running ‚Äúquick code fixes‚Äù (ctrl + .) in the IDE.string filePath = \"path/to/your/file.txt\";using var fileStream = new FileStream(filePath, FileMode.Open, FileAccess.Read);using var reader = new StreamReader(fileStream);var lines = new List&lt;string&gt;();while (!reader.EndOfStream){    var line = await reader.ReadLineAsync();    if (string.IsNullOrWhiteSpace(line))    {        continue;    }    lines.Add($\"{Guid.NewGuid()} - {line}[{CountCapitalLetters(line)}]\");}foreach (string line in lines){    Console.WriteLine(line);}// Counts the number of capital letters in a stringstatic int CountCapitalLetters(string input){    return input.Count(char.IsUpper);}üîÅ And run the checks again.‚ùØ dotnet husky run[Husky] üöÄ Loading tasks ...--------------------------------------------------[Husky] ‚ö° Preparing task 'format'[Husky] ‚åõ Executing task 'format' ...Formatted 1 files in 952ms.[Husky]  ‚úî Successfully executed in 2,307ms--------------------------------------------------[Husky] ‚ö° Preparing task 'style'[Husky] ‚åõ Executing task 'style' ...[Husky]  ‚úî Successfully executed in 15,299ms--------------------------------------------------[Husky] ‚ö° Preparing task 'analyzers'[Husky] ‚åõ Executing task 'analyzers' ...[Husky]  ‚úî Successfully executed in 12,293ms--------------------------------------------------[Husky] ‚ö° Preparing task 'spelling'[Husky] ‚åõ Executing task 'spelling' ...[Husky]  ‚úî Successfully executed in 2,802ms--------------------------------------------------üí° Note: The pre-commit hook takes about 30 seconds. This is already too long, so we might consider removing something from the git hook and relying only on the CI/CD.Now, we commit the changes and push them to the repository. The CI/CD pipeline will run the checks again, and if all checks pass, the pipeline will succeed.  ConclusionEnforcing code quality gates is an important part of maintaining a high-quality codebase. It helps catch issues early in the development process, reducing the likelihood of bugs and technical debt.  üôå I hope you found it helpful. If you have any questions, please feel free to reach out. If you‚Äôd like to support my work, a star on GitHub would be greatly appreciated! üôèReferences  https://alirezanet.github.io/Husky.Net/  https://csharpier.com/  https://docs.sonarsource.com/sonarcloud/  https://rules.sonarsource.com/csharp/  https://github.com/bkoelman/CSharpGuidelinesAnalyzer  https://github.com/streetsidesoftware/cspell  https://learn.microsoft.com/en-us/dotnet/fundamentals/code-analysis/configuration-options  https://microsoft.github.io/code-with-engineering-playbook/developer-experience/"
    },
  
    {
      "id": "31",
      "title": "Using Polyglot Notebooks and Kernel Memory in Your Day-to-Day Tasks",
      "url": "/dotnet/ai/2024/09/11/notebook-agent.html",
      "date": "September 11, 2024",
      "categories": ["dotnet","ai"],
      "tags": ["dotnet","semantic-kernel","kernel-memory","ai","aspire","developer-tools"],
      "shortinfo": "Learn how to use Polyglot Notebooks and Kernel Memory together to enhance your day-to-day tasks with the power of AI.",
      "content": "TL;DRLearn how to use Polyglot Notebooks and Kernel Memory together to enhance your day-to-day tasks with the power of AI.  TL;DR  Motivation  Example - ‚ÄòGenerate a summary of a blog‚Äô          Demo        Anatomy of the Setup (aka notebook-agent)          Another Example - ‚ÄòAsk .NET 9 Release Notes‚Äô        Conclusion  ReferencesSource code: https://github.com/NikiforovAll/notebook-agentMotivationPolyglot Notebooks is a powerful tool for developers. They allow you to combine code, text, and visualizations in a single document. This makes it easy to explore data, experiment with different tasks, and share your results with others.üí° Notebooks are a great way to bring programmability to your work, allowing you to use code to automate repetitive tasks, analyze data, and generate reports. AI capabilities are a great addition to that.üí° The more you invest in notebooks, the easier it becomes for you to be productive. Your experiments are stored over time, constituting a knowledge base that you can refer to at any moment.üéØ For this blog post, my goal is to show you how to work with LLMs in a notebook environment. It would be great to be able to use external sources as a context for my prompts to support my day-to-day activities.Example - ‚ÄòGenerate a summary of a blog‚ÄôLet‚Äôs say you have a blog post that you want to summarize. You can use the LLM to generate a summary of the blog post. Here‚Äôs how you can do it:  Crawl the blog post and extract the text from the HTML content.  Use the LLM to generate a summary of the blog post.It turns out that the Kernel Memory takes care of it for you. All the heavy lifting is done by the Kernel Memory. You just need to provide the context and the prompt.Here is the code that shows you how to do it:var docId = await memory.ImportWebPageAsync(\"https://nikiforovall.github.io/tags.html\");var answer = await memory.AskAsync(\"What the nikiforovall blog is about?\", filter: MemoryFilters.ByDocument(docId));DemoHere is the demo:  Output:The nikiforovall blog, titled \"N+1 Blog,\" focuses on topics related to programming and IT. It features a variety of posts that cover different aspects of software development, particularly within the .NET ecosystem. The blog includes discussions on modern API development, asynchronous programming, microservices, cloud computing, and tools like Keycloak for identity management. Additionally, it explores design patterns, coding stories, and practical coding techniques, making it a resource for developers looking to enhance their skills and knowledge in these areas.ü§î But, how do I get an instance of memory you may ask? Let‚Äôs see how to do it using TestcontainersAnatomy of the Setup (aka notebook-agent)I have created a project called notebook-agent. Basically, it is a starting point that has all the necessary bits to get you started.See: https://github.com/NikiforovAll/notebook-agentHere is the structure of the project:.‚îú‚îÄ‚îÄ .devcontainer‚îÇ  ‚îî‚îÄ‚îÄ devcontainer.json‚îú‚îÄ‚îÄ .gitignore‚îú‚îÄ‚îÄ .vscode‚îÇ  ‚îî‚îÄ‚îÄ extensions.json‚îú‚îÄ‚îÄ README.md‚îî‚îÄ‚îÄ src  ‚îú‚îÄ‚îÄ OpenAiOptions.cs      - Options for OpenAI  ‚îú‚îÄ‚îÄ ServiceDefaults.cs  ‚îú‚îÄ‚îÄ appsettings.json      - Configuration file  ‚îú‚îÄ‚îÄ playground.ipynb      - entry point (example)  ‚îú‚îÄ‚îÄ ask-release-notes.ipynb   - (example)  ‚îú‚îÄ‚îÄ setup-infrastructure.ipynb - PostgreSQL, pgvector  ‚îî‚îÄ‚îÄ setup-kernel.ipynb     - Compose Kernel MemoryWe have a few notebooks that will help you get started. Let‚Äôs start from the infrastructure bit: setup-infrastructure.ipynb// setup-infrastructure.ipynbusing Testcontainers.PostgreSql;using Npgsql;var db = new PostgreSqlBuilder()  .WithImage(\"pgvector/pgvector:pg16\")  .WithPortBinding(5432, 5432)  .WithDatabase(\"memory-db\")  .WithUsername(\"postgres\")  .WithPassword(\"postgres\")  .WithReuse(true)  .WithVolumeMount(\"memory-db-volume\", \"/var/lib/postgresql/data\")  .Build();await db.StartAsync();var connectionString = db.GetConnectionString();In the code above, we are using Testcontainers to start a PostgreSQL instance with the pgvector extension. The pgvector extension is used to store the embeddings of the documents. The embeddings are used to search for the most relevant documents based on the context.Once the infrastructure is ready, let‚Äôs see how to configure the Kernel Memory: setup-kernel.ipynb// setup-kernel.ipynbusing Microsoft.Extensions.Hosting;using Microsoft.Extensions.DependencyInjection;using Microsoft.Extensions.Configuration;using Microsoft.KernelMemory;using Microsoft.KernelMemory.SemanticKernel;using Microsoft.SemanticKernel.ChatCompletion;using Microsoft.SemanticKernel.Connectors.OpenAI;HostApplicationBuilder builder = Host.CreateApplicationBuilder();var textGenerationOpetions = builder.Configuration // configured in appsettings.json  .GetSection(\"TextGeneration\")  .Get&lt;OpenAiOptions&gt;();var textEmbeddingOptions = builder.Configuration // configured in appsettings.json  .GetSection(\"TextEmbeddingGeneration\")  .Get&lt;OpenAiOptions&gt;();builder.Services.AddKernelMemory&lt;MemoryServerless&gt;(memoryBuilder =&gt;{  memoryBuilder    .WithPostgresMemoryDb(new()       {        ConnectionString = builder.Configuration.GetConnectionString(\"memory-db\")! // connection string to the PostgreSQL      }    )    .WithSemanticKernelTextGenerationService( // text completion      new AzureOpenAIChatCompletionService(        deploymentName: textGenerationOpetions.Deployment,        endpoint: textGenerationOpetions.Endpoint,        apiKey: textGenerationOpetions.ApiKey      ),      new SemanticKernelConfig()    )    .WithSemanticKernelTextEmbeddingGenerationService( // text embedding      new AzureOpenAITextEmbeddingGenerationService(        deploymentName: textEmbeddingOptions.Deployment,        endpoint: textEmbeddingOptions.Endpoint,        apiKey: textEmbeddingOptions.ApiKey      ),      new SemanticKernelConfig()    );});IHost host = builder.Build();IKernelMemory memory = host.Services.GetRequiredService&lt;IKernelMemory&gt;();IServiceProvider services = host.Services;üöÄ Now, we have an the instance Kernel Memory named - memory. Just run the import command from a notebook: #!import ./setup-kernel.ipynbAnother Example - ‚ÄòAsk .NET 9 Release Notes‚ÄôFor example, assume we want to be able to chat with LLM about the latest release of .NET 9. Here is how to do it:  Create a new notebook: ask-release-notes.ipynb  Import the setup-kernel.ipynb notebook  Index of the release notes  Ask the question1Ô∏è‚É£ Index release notes:var tags = new TagCollection() { [\"release\"] = [\".NET 9\",\"RC1\"] }; // tagging provides additional information, improves search resultsvar librariesReleaseNotes = \"https://raw.githubusercontent.com/dotnet/core/main/release-notes/9.0/preview/rc1/libraries.md\";var aspnetCoreReleaseNotes = \"https://raw.githubusercontent.com/dotnet/core/main/release-notes/9.0/preview/rc1/libraries.md\";Task[] tasks = [  memory.ImportWebPageAsync(librariesReleaseNotes, tags: tags),  memory.ImportWebPageAsync(aspnetCoreReleaseNotes, tags: tags),];await Task.WhenAll(tasks);2Ô∏è‚É£ Ask the question:var answer = await memory.AskAsync(\"What are the latest additions to .NET 9 release?\", minRelevance: 0.80);answer.Result.DisplayAs(\"text/markdown\");Here is the output:  ConclusionIn this post, we have seen how to use Polyglot Notebooks and Kernel Memory together to empower your day-to-day tasks with the power of AI. We have seen how to set up the infrastructure using Testcontainers and how to configure the Kernel Memory to use the LLM.  üôå I hope you found it helpful. If you have any questions, please feel free to reach out. If you‚Äôd like to support my work, a star on GitHub would be greatly appreciated! üôèReferences  Source Code: notebook-agent  Semantic Kernel  Kernel Memory  .NET Interactive  Testcontainers"
    },
  
    {
      "id": "32",
      "title": "Typical RAG Implementation Using Semantic Kernel, Kernel Memory, and Aspire in .NET",
      "url": "/dotnet/ai/2024/09/04/typical-rag-dotnet.html",
      "date": "September 04, 2024",
      "categories": ["dotnet","ai"],
      "tags": ["dotnet","semantic-kernel","kernel-memory","opentelemetry","ai","aspire","rag"],
      "shortinfo": "Learn how to use the out-of-the-box solution provided by Kernel Memory to build a typical RAG in .NET.",
      "content": "TL;DR  TL;DR  Introduction  Code          Set up Aspire      Set up Kernel Memory        Demo          Upload a Document      Ask a Question      Observability        Conclusion  References    Source code: https://github.com/NikiforovAll/typical-rag-dotnetIntroductionKernel Memory (KM) is a multi-modal AI Service specialized in the efficient indexing of datasets through custom continuous data hybrid pipelines, with support for Retrieval Augmented Generation (RAG), synthetic memory, prompt engineering, and custom semantic memory processing.Kernel Memory works and scales best when running as a service (separate process), allowing the ingestion of thousands of documents and information without blocking your app However, you can also use the MemoryServerless class in your app, using KernelMemoryBuilder.var memory = new KernelMemoryBuilder()    .WithOpenAIDefaults(Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\"))    .Build&lt;MemoryServerless&gt;();// Import a fileawait memory.ImportDocumentAsync(\"meeting-transcript.docx\", tags: new() { { \"user\", \"Blake\" } });// Ask questionsvar answer1 = await memory.AskAsync(\"How many people attended the meeting?\");The MemoryServerless class is a lightweight in-process version of the Kernel Memory service.The Kernel Memory architecture can be divided in two main areas: Ingestion and Retrieval.When a client sends a file, the service first saves data in the Content Storage, such as Azure Blobs or Local Disk. During this phase, a client stays connected, sending data and waiting for the data to be stored. Only when this operation is complete, Kernel Memory releases the client request and starts an asynchronous pipeline, to complete the ingestion without blocking the client.Kernel Memory ingestion offers a default pipeline, composed by these sequential steps. Each step depends on the previous to complete successfully before starting. These steps are implemented by Handlers, shipped with the Core library. The list of steps and the logic of handlers can be customized. The default pipeline looks as follows:  Text Extraction (e.g.: from PDF, DOCX,  web page, etc.) (name: extract)  Splitting text into partitions (name: partition)  Generating embedding vectors for each partition (name: gen_embeddings)  Store embedding vectors and metadata in a memory DB. (name: save_records)KM uses two types of storage:  Content Storage: stores raw data uploaded by clients, pipeline status, unique IDs assigned to documents.  Memory Storage: databases with search capability, where KM stores partitions and metadata (aka ‚Äúmemories‚Äù)Kernel Memory is composable in two ways:  Customizable components: you can choose a Vector Database, Content Storage, Embedding Generator, and Summarizer (LLM).  Extensible pipeline: you can add custom handlers to the pipeline, or replace the default ones.Now, let‚Äôs see how to build a typical RAG solution using Semantic Kernel, Kernel Memory, and Aspire in .NET.CodeThe awesome thing about Kernel Memory is that it provides an out-of-the-box solution. Microsoft.KernelMemory.Service.AspNetCore package provides a set of endpoints to interact with the service. You can use these endpoints to upload documents, ask questions, and get answers.Basically, it boils down to the following:var builder = WebApplication.CreateBuilder(args);builder.Services.AddKernelMemory&lt;MemoryServerless&gt;();var app = builder.Build();app.AddKernelMemoryEndpoints(apiPrefix: \"/rag\");app.Run();As a result, you will get the following endpoints:    Let‚Äôs see how to actually make it work by adding essential components, such as LLM, Vector Database, and Content Storage.Before we see how to configure Kernel Memory, let‚Äôs see how to configure Aspire.Set up AspireWe will use Postgres as a Vector and Content Database and we will use Azure Open AI to generate embeddings and summaries.  üí° Aspire comes with OpenTelemetry support. I strongly suggest adding observability to your RAG solutions from the beginning because you really want to know how everything works together and how long it takes to serve a response.  üí° Consider latency as part of your UI/UX since the retrieval and call to the LLM can significantly increase the response time.// AppHost/Program.csvar builder = DistributedApplication.CreateBuilder(args);var db = builder    .AddPostgres(\"postgres\", port: 5432)    .WithImage(\"pgvector/pgvector\")    .WithImageTag(\"pg16\")    .WithInitBindMount(\"resources/init-db\")    .WithDataVolume()    .WithPgAdmin()    .AddDatabase(\"rag-db\")    .WithHealthCheck();var openai = builder.ExecutionContext.IsPublishMode    ? builder        .AddAzureOpenAI(\"openai\")        .AddDeployment(            new AzureOpenAIDeployment(                name: \"gpt-4\",                modelName: \"gpt-4\",                modelVersion: \"2024-05-13\",                skuName: \"Standard\",                skuCapacity: 1000            )        )    : builder.AddConnectionString(\"openai\");builder    .AddProject&lt;Projects.WebRAG&gt;(\"rag-web\")    .WithReference(db)    .WithReference(openai)    .WaitFor(db);builder.Build().Run();Set up Kernel MemoryNow, when we have Aspire set up, let‚Äôs see how to configure Kernel Memory.var builder = WebApplication.CreateBuilder(args);builder.AddServiceDefaults();builder.AddNpgsqlDataSource(\"rag-db\"); // Adds OpenTelemetry and health checks for Postgresbuilder.ConfigureAiOptions();var aiOptions = builder.GetAiOptions();builder.Services.AddKernelMemory&lt;MemoryServerless&gt;(memoryBuilder =&gt;{    memoryBuilder        .WithPostgresMemoryDb(            new PostgresConfig()            {                ConnectionString = builder.Configuration.GetConnectionString(\"rag-db\")!            }        )        .WithSemanticKernelTextGenerationService(            new AzureOpenAIChatCompletionService(                deploymentName: aiOptions.Deployment,                endpoint: aiOptions.Endpoint,                apiKey: aiOptions.ApiKey            ),            new SemanticKernelConfig()        )        .WithSemanticKernelTextEmbeddingGenerationService(            new AzureOpenAITextEmbeddingGenerationService(                deploymentName: \"text-embedding-ada-002\",                endpoint: aiOptions.Endpoint,                apiKey: aiOptions.ApiKey            ),            new SemanticKernelConfig()        );});var app = builder.Build();app.MapDefaultEndpoints();app.AddKernelMemoryEndpoints(apiPrefix: \"/rag\");app.Run();IKernelMemoryBuilder provides a fluent API to configure Kernel Memory.üí° We are using Azure OpenAI services through Semantic Kernel because it provides additional OpenTelemetry instrumentation. Later, we will see how it works in practice.DemoLet‚Äôs run the application and see how it works.But before that, we need to configure the OpenAI connection string in AppHost/appsettings.json:{  \"ConnectionStrings\": {    \"OpenAI\": \"Endpoint=https://{account_name}.openai.azure.com/;Key={account_key};Deployment=gpt-4\"  }}And run:dotnet run --project src/AppHost/Upload a DocumentNow, let‚Äôs upload a document by calling the POST /rag/upload endpoint.POST https://localhost:7016/rag/upload HTTP/1.1accept: application/jsonContent-Type: multipart/form-data; boundary=boundary--boundaryContent-Disposition: form-data; name=\"file\"; filename=\"AzureFundamentals.pdf\"Content-Type: pdf&lt; ./AzureFundamentals.pdf--boundary--Response:{  \"index\": \"\",  \"documentId\": \"20240903.124353.69a8ef269a0d43989c53719128054436\",  \"message\": \"Document upload completed, ingestion pipeline started\"}It took about 1 minute to process the document. As you can see below, multiple calls were made to the OpenAI service to generate an embedding for each partition.    And about 133 partitions/embeddings were created for a single document (8 MB/500 pages).      üí°Note that the way you chunk your data is important and can impact the way RAG works. For example, you may want to make your chunks bigger and apply summarization to get more abstract answers.Ask a Questioncurl -X 'POST' \\    'https://localhost:7016/rag/ask' \\    -H 'accept: application/json' \\    -H 'content-type: application/json' \\    -d '{ \"question\": \"What kind of Azure Database services can I use?\" }'Response:{  \"question\": \"What kind of Azure Database services can I use?\",  \"noResult\": false,  \"text\": \"Azure offers a variety of database services to cater to different needs, including both SQL and NoSQL options:\\n\\n1. **Azure SQL Database**: This is a fully managed relational database with built-in intelligence that supports self-driving features such as performance tuning and threat alerts. Azure SQL Database is highly scalable and compatible with the SQL Server programming model.\\n\\n2. **SQL Server on Azure Virtual Machines**: This service allows you to run SQL Server inside a fully managed virtual machine in the cloud. It is suitable for applications that require a high level of control over the database server and compatibility with SQL Server data management and business intelligence capabilities.\\n\\n3. **Azure Cosmos DB**: Formerly known as DocumentDB, Azure Cosmos DB is a globally distributed, multi-model database service. It is designed to provide low-latency, scalable, and highly available access to your data, suitable for any scale of business application.\\n\\n4. **Azure Database for MySQL**: This is a managed service that enables you to run, manage, and scale highly available MySQL databases in the cloud. Using Azure Database for MySQL provides capabilities such as high availability, security, and recovery built into the service.\\n\\n5. **Azure Database for PostgreSQL**: Similar to Azure Database for MySQL, this service provides a fully managed, scalable PostgreSQL database service with high availability and security features.\\n\\n6. **Azure Table Storage**: A service that stores large amounts of structured NoSQL data in the cloud, providing a key/attribute store with a schema-less design.\",  \"relevantSources\": [    {      \"link\": \"default/20240903.124353.69a8ef269a0d43989c53719128054436/9b7accc78c164db7a2a630ca57e38d8f\",      \"index\": \"default\",      \"documentId\": \"20240903.124353.69a8ef269a0d43989c53719128054436\",      \"fileId\": \"9b7accc78c164db7a2a630ca57e38d8f\",      \"sourceContentType\": \"application/pdf\",      \"sourceName\": \"ExampleTestDocument.pdf\",      \"sourceUrl\": \"/download?index=default&amp;documentId=20240903.124353.69a8ef269a0d43989c53719128054436&amp;filename=ExampleTestDocument.pdf\",      \"partitions\": [        {          \"text\": \"\",          \"relevance\": 0.8672107,          \"partitionNumber\": 99,          \"sectionNumber\": 0,          \"lastUpdate\": \"2024-09-03T09:45:34+03:00\",          \"tags\": {            \"__document_id\": [              \"20240903.124353.69a8ef269a0d43989c53719128054436\"            ],            \"__file_type\": [\"application/pdf\"],            \"__file_id\": [\"9b7accc78c164db7a2a630ca57e38d8f\"],            \"__file_part\": [\"1999e1ab04a24174bf6d2c79284b04b5\"],            \"__part_n\": [\"99\"],            \"__sect_n\": [\"0\"]          }        }      ]    }  ]}As you can see, the response contains the answer to the question and the relevant sources. The answer is generated by the OpenAI service, and the relevant sources are the partitions that contain the answer.ObservabilityThe response time is quite high. It took about 14 seconds to get the answer. This is because the OpenAI service is called to generate the answer, and it takes time.We can use OpenTelemetry to trace the request and see how much time it takes to get the answer.public static IHostApplicationBuilder ConfigureOpenTelemetry(    this IHostApplicationBuilder builder){    builder.Logging.AddOpenTelemetry(logging =&gt;    {        logging.IncludeFormattedMessage = true;        logging.IncludeScopes = true;    });    AppContext.SetSwitch(\"Microsoft.SemanticKernel.Experimental.GenAI.EnableOTelDiagnostics\", true);    AppContext.SetSwitch(\"Microsoft.SemanticKernel.Experimental.GenAI.EnableOTelDiagnosticsSensitive\", true);    builder        .Services.AddOpenTelemetry()        .WithMetrics(metrics =&gt;        {            metrics                .AddAspNetCoreInstrumentation()                .AddMeter(\"Microsoft.SemanticKernel*\")                .AddHttpClientInstrumentation()                .AddRuntimeInstrumentation();        })        .WithTracing(tracing =&gt;        {            tracing                .AddAspNetCoreInstrumentation()                .AddSource(\"Microsoft.SemanticKernel*\") // add this to trace Semantic Kernel                .AddHttpClientInstrumentation();        });    builder.AddOpenTelemetryExporters();    return builder;}    Semantic Kernel instrumentation adds a lot of interesting information that can help you to understand how the interaction with LLM works:            Key      Value                  gen_ai.operation.name      chat.completions              gen_ai.request.max_tokens      300              gen_ai.request.model      gpt-4              gen_ai.request.temperature      0              gen_ai.request.top_p      0              gen_ai.prompt      ...              gen_ai.completion      ...      Here is an example of gen_ai.prompt and gen_ai.prompt completion.Prompt:[    {        \"role\": \"system\",        \"content\": \"Facts:\\n==== [File:ExampleTestDocument.pdf;Relevance:86.7%](CONTENT_ABBREVAITED)\\nQuestion: What kind of Azure Database services can I use?\\nAnswer: \"    }]Completion:[    {        \"role\": \"assistant\",        \"content\": \"Azure offers a variety of database services catering to different needs, including both SQL and NoSQL options:\\n\\n1. **Azure SQL Database**: This is a fully managed relational database with built-in intelligence that supports self-driving features such as performance tuning and threat alerts. Azure SQL Database is highly scalable and compatible with the SQL Server programming model.\\n\\n2. **SQL Server on Azure Virtual Machines**: This service allows you to run SQL Server inside a fully managed virtual machine in the cloud. It is suitable for applications that require complete control over the database server and is ideal for migrating existing SQL Server workloads to the cloud.\\n\\n3. **MySQL Database**: Azure facilitates MySQL as a managed service, which is easy to set up, manage, and scale. This service is provided through a partnership with SuccessBricks\\u2019 ClearDb, offering a fully managed MySQL database.\\n\\n4. **Azure Database for PostgreSQL**: This is a managed service that makes it easier to set up, manage, and scale PostgreSQL databases in the cloud.\\n\\n5. **Azure Cosmos DB**: Formerly known as DocumentDB, Cosmos DB is a globally distributed, multi-model database service that supports schema-less data, which makes it a suitable option for web, mobile, gaming, and IoT applications.\\n\\n6. **Azure Table Storage**: This service offers highly available, massively scalable storage, which is ideal for applications requiring a flexible NoSQL key-value store.\\n\\n7. **Azure MongoDB**: Through the Azure Marketplace, MongoDB can be hosted on Azure Virtual Machines, providing\"    }]ConclusionIn this article, we have seen how to build a typical RAG solution using Semantic Kernel, Kernel Memory, and Aspire in .NET. We have also seen how to add OpenTelemetry instrumentation to trace the request and see how long it takes to get the answer. I hope you find this article helpful. üôåReferences  https://learn.microsoft.com/en-us/semantic-kernel/get-started/quick-start-guide?pivots=programming-language-csharp - Semantic Kernel Quick Start Guide  https://microsoft.github.io/kernel-memory - Kernel Memory Docs  https://github.com/microsoft/semantic-kernel/blob/main/dotnet/docs/TELEMETRY.md - OpenTelemetry + Semantic Kernel"
    },
  
    {
      "id": "33",
      "title": "Building pipelines with IAsyncEnumerable in .NET",
      "url": "/dotnet/2024/08/22/async-enumerable-pipelines.html",
      "date": "August 22, 2024",
      "categories": ["dotnet"],
      "tags": ["dotnet","async","pipelines"],
      "shortinfo": "This article demonstrates how to use IAsyncEnumerable and System.Linq.Async to build pipelines in C#.",
      "content": "TL;DR  TL;DR  Introduction  Examples          Using System.Linq.Async operators to build a pipeline      Combining IAsyncEnumerable with IObservable      Implementing reusable operators - Batch      Implementing reusable domain-specific operators - TextSummarization with Semantic Kernel        Conclusion  ReferencesThis article demonstrates how to use IAsyncEnumerable and System.Linq.Async to build pipelines in C#.Source code: https://github.com/NikiforovAll/async-enumerable-pipelinesYou can see all demos by running:dotnet example --list‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ‚îÇ Example                                 ‚îÇ Description                                                                                ‚îÇ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§‚îÇ CalculateWordCountPipeline              ‚îÇ Demonstrates how to build async-enumerable pipelines based on standard LINQ operators      ‚îÇ‚îÇ CalculateWordCountFileWatcherPipeline   ‚îÇ Demonstrates how to combine async-enumerable pipelines with IObservable. E.g: file watcher ‚îÇ‚îÇ CalculateWordCountBatchPipeline         ‚îÇ Demonstrates how to use batching in async-enumerable pipelines                             ‚îÇ‚îÇ TextSummarizationAndAggregationPipeline ‚îÇ Demonstrates how to build custom async-enumerable operators                                ‚îÇ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØHere‚Äôs a sneak peek üëÄ:// TextSummarizationAndAggregationPipelinevar pipeline = Directory    .EnumerateFiles(path)    .ToAsyncEnumerable()    .ReportProgress()    .SelectAwait(ReadFile)    .Where(IsValidFileForProcessing)    .SelectAwait(Summarize)    .WriteResultToFile(path: Path.Combine(Path.GetTempPath(), \"summaries.txt\"))    .ForEachAsync(x =&gt; AnsiConsole.MarkupLine($\"Processed [green]{x.Name}[/]\"));IntroductionPipelines are a powerful way to process data in a streaming fashion. They are a series of stages that transform data from one form to another. In this article, we will explore how to build pipelines using IAsyncEnumerable and System.Linq.Async.Pipelines are a common pattern in modern software development. They are used to process data in a streaming fashion, which can be more efficient than processing it all at once. Pipelines are also composable, meaning that you can combine multiple stages together to create more complex processing logic.üí° I already describe an approach to building pipelines in my previous blog post, you might want to take a look at Building pipelines with System.Threading.Channels. Both System.Threading.Channels and IAsyncEnumerable provide powerful tools for managing asynchronous data streams in .NET. However, while System.Threading.Channels offers a more explicit approach to handling producer-consumer scenarios, IAsyncEnumerable brings a more integrated and LINQ-friendly way to work with asynchronous sequences. Understanding the strengths and nuances of each can help you choose the right tool for your specific use case.ExamplesThere are many interesting concepts that I‚Äôm going to cover in this article. Let‚Äôs start with the basics.Using System.Linq.Async operators to build a pipelineLet‚Äôs say we want to build a pipeline that reads files from a directory, parses them, and counts the number of words in each file.This can be illustrated as follows:        graph LR;        A[Read files] --&gt; B[Parse files];        B --&gt; C[Count words];        C --&gt; D[Output results];    üéØ Our goal is to represent each stage of the pipeline in the code using IAsyncEnumerable and System.Linq.Async.System.Linq.Async is a library that provides asynchronous versions of LINQ operators. It allows you to work with IAsyncEnumerable in a similar way to how you would work with IEnumerable. It makes it easy to build pipelines.Basically, you have control-flow described as chain of methods calls, and you can implement each stage of the pipeline as a separate method. In my opinion, it makes the code more readable and maintainable. The benefit of this approach is once you determine the stages of the pipeline, you can implement them independently and focus on the logic of each stage.The process of parsing files can be implemented as follows:var path = Path.Combine(Directory.GetCurrentDirectory(), \"..\", \"..\", \"Data\");var pipeline = Directory    .EnumerateFiles(path)    .ToAsyncEnumerable()    .SelectAwait(ReadFile)    .Where(IsValidFileForProcessing)    .Select(CalculateWordCount)    .OrderByDescending(x =&gt; x.WordCount)    .ForEachAsync(Console.WriteLine);await pipeline;Everything starts with the conversion of IEnumerable&lt;string&gt; file paths to IAsyncEnumerable&lt;string&gt;.üí° Alternatively, we could write our own method that returns IAsyncEnumerable, for example, we could easily swap the local file system with Azure Blob Storage. It means we can reuse the same pipeline with different data sources.üí° Later, we will see that not only IEnumerable can be converted to IAsyncEnumerable, but also IObservable.As you can see System.Linq.Async provides a set of extension methods that allow you to work with IAsyncEnumerable in a similar way to how you would work with IEnumerable. The SelectAwait method is used to asynchronously project each element of the sequence. The Where method is used to filter elements based on a predicate. The OrderByDescending method is used to sort the elements of the sequence in descending order. The ForEachAsync method is used to asynchronously iterate over the sequence.It worth to point out that ForEachAsync is a terminal operation that triggers the execution of the pipeline. It is important to remember that IAsyncEnumerable is a cold sequence, meaning that it does not start processing until you start iterating over it.Here are the building blocks of the pipeline:public static class Steps{    public static async ValueTask&lt;FilePayload&gt; ReadFile(string file)    {        var content = await File.ReadAllTextAsync(file);        var name = Path.GetFileName(file);        return new FilePayload(name, content);    }    public static bool IsValidFileForProcessing(FilePayload file) =&gt;        file is { Content.Length: &gt; 0, Name: [.., 't', 'x', 't'] };    public static WordCountPayload CalculateWordCount(FilePayload payload)    {        var words = payload.Content.Split(' ');        return new(payload.Name, words.Length);    }}public record FilePayload(string Name, string Content);public record WordCountPayload(string Name, int WordCount);Let‚Äôs see the pipeline in action:    Combining IAsyncEnumerable with IObservableLet‚Äôs say we want to use simple file watcher to monitor changes in the directory and trigger the pipeline when a new file is created or an existing file is modified.var fileWatcher = CreateFileObservable(path);var pipeline = fileWatcher    .TakeUntil(DateTimeOffset.Now.AddSeconds(15))    .ToAsyncEnumerable()    .SelectAwait(ReadFile)    .Where(IsValidFileForProcessing)    .Select(CalculateWordCount)    .ForEachAsync(Console.WriteLine);In this example, we use IObservable to monitor changes in the directory. We create an observable sequence of file paths using the CreateFileObservable method. We then use the TakeUntil operator to limit the duration of the sequence to 15 seconds. We convert the observable sequence to an asynchronous enumerable sequence using the ToAsyncEnumerable method. We then apply the same pipeline as before to process the files.The CreateFileObservable method is implemented as follows:static IObservable&lt;string&gt; CreateFileObservable(string path) =&gt;    Observable.Create&lt;string&gt;(observer =&gt;    {        var watcher = new FileSystemWatcher(path)        {            NotifyFilter = NotifyFilters.FileName | NotifyFilters.LastWrite,            Filter = \"*.*\",            EnableRaisingEvents = true        };        void onChanged(object sender, FileSystemEventArgs e)        {            try            {                observer.OnNext(e.FullPath);            }            catch (Exception ex)            {                observer.OnError(ex);            }        }        watcher.Created += onChanged;        watcher.Changed += onChanged;        return () =&gt;        {            watcher.Created -= onChanged;            watcher.Changed -= onChanged;            watcher.Dispose();        };    });Let‚Äôs see the pipeline in action:In the demo below, I‚Äôm appending ‚Äú word‚Äù to the end of the file content to trigger the pipeline.    Implementing reusable operators - BatchLet‚Äôs say we want to batch the processing of files to improve performance. We can implement a custom operator called Batch that groups elements of the sequence into batches of a specified size.In the example above, we are reading files in batches in parallel. We are using the Batch operator to group files into batches of size 2. We then process each batch in parallel using the ProcessEachAsync method.const int batchSize = 2;var pipeline = Directory    .EnumerateFiles(path)    .ToAsyncEnumerable()    .Batch&lt;string, FilePayload&gt;(batchSize)    .ProcessEachAsync(ReadFile)    .Where(IsValidFileForProcessing)    .Select(CalculateWordCount)    .OrderByDescending(x =&gt; x.WordCount)    .ForEachAsync(Console.WriteLine);await pipeline;In the example above, we are reading files in batches in parallel. We are using the Batch operator to group files into batches of size 2. We then process each batch in parallel using the ProcessEachAsync method.üí° I will leave the implementation of the Batch operator as an exercise for the reader. Please check source code for the full implementation. https://github.com/NikiforovAll/async-enumerable-pipelines/blob/main/Pipelines.Core/PipelineBuilderExtensions.csLet‚Äôs see the pipeline in action:    Implementing reusable domain-specific operators - TextSummarization with Semantic KernelTo demonstrate something more complex, let‚Äôs say we want to summarize the content of the files using the Semantic Kernel library. Summarization is a common task in natural language processing (NLP) that involves generating a concise representation of a text document.var pipeline = Directory    .EnumerateFiles(path)    .ToAsyncEnumerable()    .ReportProgress()    .SelectAwait(ReadFile)    .Where(IsValidFileForProcessing)    .SelectAwait(Summarize)    .WriteResultToFile(path: Path.Combine(Path.GetTempPath(), \"summaries.txt\"))    .ForEachAsync(x =&gt; AnsiConsole.MarkupLine($\"Processed [green]{x.Name}[/]\"));In the example above, we are reading files, summarizing their content, and writing the results to a file. We are using the ReportProgress operator to report progress as each file is processed. We are using the Summarize operator to summarize the content of each file. We are using the WriteResultToFile operator to write the results to a file.Before we move forward, let‚Äôs see how the pipeline works in the demo below:    Now, we are ready to move forward and see the details of the implementation.The Summarize method is implemented as follows:async ValueTask&lt;SummarizationPayload&gt; Summarize(FilePayload file){    var prompt = \"\"\"                Please summarize the content above in 20 words or less:        The output format should be: [title]: [summary]        \"\"\";    var result = await kernel.InvokePromptAsync(prompt, new KernelArguments() { [\"input\"] = file.Content });    return new(file.Name, result.ToString());}Than we want to write the results to a file:public static async IAsyncEnumerable&lt;SummarizationPayload&gt; WriteResultToFile(    this IAsyncEnumerable&lt;SummarizationPayload&gt; values,    string path){    const int batchSize = 10;    using var streamWriter = new StreamWriter(path, append: true);    await foreach (var batch in values.Buffer(batchSize))    {        foreach (var value in batch)        {            await streamWriter.WriteLineAsync(value.Summary);            yield return value;        }        await streamWriter.FlushAsync();    }    AnsiConsole.MarkupLine($\"Results written to [green]{path}[/]\");}üí° Note, IAsyncEnumerable is pull-based model. With this approach, each summary is read individually and appended to the end of the file. This means that results are continuously saved as each batch is processed by calling the FlushAsync method.The ReportProgress method is quite interesting because it eagerly reads all elements of the sequence to determine the total count. It then reports progress as each element is processed.public static async IAsyncEnumerable&lt;string&gt; ReportProgress(this IAsyncEnumerable&lt;string&gt; values){    var totalCount = await values.CountAsync();    await foreach (var (value, index) in values.Select((value, index) =&gt; (value, index)))    {        yield return value;        AnsiConsole            .Progress()            .Start(ctx =&gt;            {                var task = ctx.AddTask($\"Processing - {Path.GetFileName(value)}\", true, totalCount);                task.Increment(index + 1);                task.StopTask();            });    }}üí° This is a good demonstration of leaky abstractions. Not all data sources can provide the full sequence immediately, so we need to be careful.ConclusionThat is it! üôå We have seen how to build pipelines using IAsyncEnumerable and System.Linq.Async. I hope you found this article helpful. If you have any questions or comments, please feel free to leave them below.References  https://learn.microsoft.com/en-us/archive/msdn-magazine/2019/november/csharp-iterating-with-async-enumerables-in-csharp-8  https://github.com/dotnet/reactive  https://github.com/microsoft/semantic-kernel/tree/main/dotnet/notebooks"
    },
  
    {
      "id": "34",
      "title": "Unlocking the Power of TypedResults in Endpoints: A Consistent Approach to Strongly Typed APIs in .NET",
      "url": "/dotnet/2024/08/17/result-endpoints.html",
      "date": "August 17, 2024",
      "categories": ["dotnet"],
      "tags": ["dotnet","aspnetcore"],
      "shortinfo": "Explore the use of TypedResults in .NET endpoints to achieve a consistent and strongly typed APIs",
      "content": "TL;DR  TL;DR  Introduction          Motivation        Solution - use TypedResults          Example                  Adding OpenAPI metadata          Automatic model validation          Exception handling                    Control the serialization via JsonSerializerOptions        Bonus: Nall.ResultEndpoints.Template template  Conclusion  ReferencesThis blog post explores the use of TypedResults in .NET endpoints to achieve a consistent and strongly typed API development approach. We discuss the benefits of TypedResults over traditional ActionResult, emphasizing how they enhance type safety and API consistency.Source code: https://github.com/NikiforovAll/result-endpoints-templateYou can use dotnet new template to get started with TypedResults in your .NET projects:dotnet new install Nall.ResultEndpoints.Templatedotnet new result-endpoints-apiHere is a simple example of how you can use TypedResults in your .NET endpoints:public class HelloWorld : EndpointBaseSync.WithRequest&lt;string&gt;.WithResult&lt;Ok&lt;string&gt;&gt;{    [EndpointSummary(\"Says hello\")]    [EndpointName(nameof(HelloWorld))]    [HttpGet(\"/hello-world\")]    public override Results&lt;Ok&lt;string&gt;&gt; Handle([FromQuery(Name = \"q\")] string request)    {        return TypedResults.Ok($\"Hello, {request}\");    }}IntroductionIn this blog post I want to show you the opinionated approach to building APIs in .NET using TypedResults in MVC based Web API implementations (aka Controllers).üí° Thought this article, I will use an awesome library called Ardalis.ApiEndpoints it‚Äôs basically an alternative to the traditional Controller approach in ASP.NET Core built on top of MVC. It provides a more structured and consistent way to build APIs using Endpoints instead of Controllers. The approach resembles the minimal API approach but differs in many ways.From the Ardalis.ApiEndpoints documentation:  MVC Controllers are essentially an antipattern. They‚Äôre dinosaurs. They are collections of methods that never call one another and rarely operate on the same state. They‚Äôre not cohesive. They tend to become bloated and to grow out of control‚Ä¶My goal is to show you how to enhance the Ardalis.ApiEndpoints library with TypedResults to achieve a more consistent and strongly typed API development approach.MotivationBut, first, let‚Äôs talk about the problem we are trying to solve. When you are building an API, you need to return a response to the client. In .NET, you can use ActionResult to return a response from your endpoint. However, ActionResult is an umbrella type that can represent any kind of response.But, these details are important since they are part of the public contract of your API, we should manage them explicitly and carefully.‚òùÔ∏èFurthermore, since the OpenAPI specification is becoming more and more popular, it is important to have a consistent response structure across your API. Nowadays, it is common to generate OpenAPI documentation and client SDKs based on your API code. If you use ActionResult, you will have to manually document the response structure in your OpenAPI definitions, which can lead to inconsistencies between the actual response and OpenAPI definitions.Take a look at example below:public class HelloWorldActionResult : EndpointBaseSync.WithRequest&lt;string&gt;.WithActionResult&lt;string&gt;{    [HttpGet(\"/hello-world\")]    [ProducesResponseType(typeof(string), StatusCodes.Status200OK)]    [ProducesResponseType(StatusCodes.Status400BadRequest)]    public override ActionResult&lt;string&gt; Handle(string request)    {        if (request == \"badrequest\")        {            BadRequest(new { message = \"Something bad happened\" });        }        return Ok($\"Hello, {request}\");    }}What is wrong with this code? The problem is that the response structure is not explicitly defined in the code. The response structure is defined in the ProducesResponseType attribute, which is not enforced by the compiler. This can lead to inconsistencies between the actual response and OpenAPI definitions.Solution - use TypedResultsTurns out you can use TypedResults to solve this problem. ü§îThe IResult interface defines a contract that represents the result of an HTTP endpoint. The static Results class and the static TypedResults are used to create various IResult objects that represent different types of responses. It is usually used in in minimal APIs scenarios.Returning TypedResults rather than Results has the following advantages:  TypedResults helpers return strongly typed objects, which can improve code readability, unit testing, and reduce the chance of runtime errors.  The implementation type automatically provides the response type metadata for OpenAPI to describe the endpoint. ü§©Although, TypedResults class is usually used in minimal APIs scenarios,it can be used in MVC based Web API implementations as well.ExampleDown below, we define an endpoints that returns Results&lt;Ok&lt;string&gt;, BadRequest&lt;ProblemDetails&gt;&gt; which is like a discriminated union of Ok&lt;string&gt; and BadRequest&lt;ProblemDetails&gt;. It means that the endpoint can return either Ok&lt;string&gt; or BadRequest&lt;ProblemDetails&gt;. Trying to return any other type will result in a compile-time error. This is exactly what we want to achieve!public class HelloWorld : EndpointBaseSync.WithRequest&lt;string&gt;.WithResult&lt;Results&lt;Ok&lt;string&gt;, BadRequest&lt;ProblemDetails&gt;&gt;    &gt;{    [HttpGet(\"/hello-world\")]    public override Results&lt;Ok&lt;string&gt;, BadRequest&lt;ProblemDetails&gt;&gt; Handle(string request)    {        if (request == \"badrequest\")        {            return TypedResults.BadRequest(                TypedResults.Problem(\"Something bad happened\").ProblemDetails            );        }        return TypedResults.Ok($\"Hello, {request}\");    }}My goal is to give you a comprehensive guide to this approach. Let‚Äôs take a look at some other scenarios and gotchas you might encounter when using TypedResults and Ardalis.ApiEndpoints in your .NET projects.Adding OpenAPI metadataTo add OpenAPI metadata to your endpoint, you can use the EndpointSummary and EndpointName attributes. They are defined in Microsoft.AspNetCore.Http namespace and are used by Swashbuckle.AspNetCore to generate OpenAPI documentation. The great thing about these attributes is that they are OpenAPI-generation library agnostic which gives you the flexibility to switch between different libraries.public class HelloWorld : EndpointBaseSync.WithRequest&lt;string&gt;.WithResult&lt;Ok&lt;string&gt;&gt;{    [EndpointSummary(\"Says hello\")]    [EndpointDescription(\"Says hello based on the request\")]    [Tags(\"Hello\")]    [EndpointName(nameof(HelloWorld))]    [HttpGet(\"/hello-world\")]    public override Ok&lt;string&gt; Handle([FromQuery(Name = \"q\")] [Length(3, 100)] string request)    {        return TypedResults.Ok($\"Hello, {request}\");    }}Automatic model validationBy default, EndpointBaseSync and EndpointBaseAsync inherits from ControllerBase which means that the model validation is automatically enabled. This means that if you have a model with data annotations, the model will be automatically validated before the endpoint is executed. If the model is invalid, the endpoint will return BadRequest&lt;ProblemDetails&gt; with the validation errors. You can control this behavior by setting the SuppressModelStateInvalidFilter property in the ApiBehaviorOptions option.But because of this automatic validation, a problem arises. The BadRequest&lt;ProblemDetails&gt; response produced via model validation is not a regular response type because it is not returned by the endpoint itself; instead, it is returned by the framework when the model is invalid.To fix this, we can assume that all endpoints can potentially return BadRequest&lt;ProblemDetails&gt; and add it to the list of possible responses globally. This way, we can ensure that the response structure is consistent across all endpoints.var builder = WebApplication.CreateBuilder(args);builder.Services.AddProblemDetails();builder.Services.AddControllers(options =&gt;{    options.Filters.Add(new ProducesResponseTypeAttribute(typeof(ProblemDetails), 400)); // add this to solve the problem});builder.Services.Configure&lt;ApiBehaviorOptions&gt;(options =&gt;{    options.SuppressModelStateInvalidFilter = false; // just an example, validation is enabled by default});var app = builder.Build();app.MapControllers();app.Run();Let‚Äôs apply some request validation to our endpoint:public class HelloWorld : EndpointBaseSync.WithRequest&lt;string&gt;.WithResult&lt;Ok&lt;string&gt;&gt;{    [EndpointSummary(\"Says hello\")]    [EndpointDescription(\"Says hello based on the request\")]    [Tags(\"Hello\")]    [EndpointName(nameof(HelloWorld))]    [HttpGet(\"/hello-world\")]    public override Ok&lt;string&gt; Handle([FromQuery(Name = \"q\")] [Length(3, 100)] string request)    {        return TypedResults.Ok($\"Hello, {request}\");    }}    And here is the response:‚ùØ curl -X 'GET'   'http://localhost:5164/hello-world?q=1'   -H 'accept: application/json' -s | jq{  \"type\": \"https://tools.ietf.org/html/rfc9110#section-15.5.1\",  \"title\": \"One or more validation errors occurred.\",  \"status\": 400,  \"errors\": {    \"q\": [      \"The field request must be a string or collection type with a minimum length of '3' and maximum length of '100'.\"    ]  },  \"traceId\": \"00-6f6f77eb35c176f153d8d9b5378b69d9-9fa65ede2b6d7ba0-00\"}Exception handlingI don‚Äôt recommend to use exceptions for the control flow in your application. However, they are inevitable. It means this is another case when we want to extend common response types to make our API consistent. The approach is similar to the one we used for model validation.It is a global exception handler added via UseExceptionHandler that returns InternalServerError&lt;ProblemDetails&gt;.See the official docs for more details: Handle errors in ASP.NET Core.var builder = WebApplication.CreateBuilder(args);builder.Services.AddProblemDetails();builder.Services.AddControllers(options =&gt;{    options.Filters.Add(new ProducesResponseTypeAttribute(typeof(ProblemDetails), 500)); // add this to solve the problem    options.Filters.Add(new ProducesResponseTypeAttribute(typeof(ProblemDetails), 400));})var app = builder.Build();app.MapControllers();app.UseExceptionHandler();app.UseStatusCodePages();app.Run();Here is an example of response:‚ùØ curl -X 'GET'   'http://localhost:5164/hello-world?q=error'   -H 'accept: application/json' -s | jq{  \"type\": \"https://tools.ietf.org/html/rfc9110#section-15.6.1\",  \"title\": \"System.Exception\",  \"status\": 500,  \"detail\": \"Something went wrong...\",  \"exception\": {    \"details\": \"System.Exception: Something went wrong...\\r\\n   at ResultEndpoints.Endpoints.HelloWorld.Handle(String request) ...\",    \"path\": \"/hello-world\",    \"endpoint\": \"ResultEndpoints.Endpoints.HelloWorld.Handle (ResultEndpoints)\",    \"routeValues\": {      \"action\": \"Handle\",      \"controller\": \"HelloWorld\"    }  },  \"traceId\": \"00-1b84386819492efe10684550ca737f1e-d06bfba710a5655c-00\"}Control the serialization via JsonSerializerOptionsThere is a gotacha when using TypedResults - when returning a TypedResult from a controller or minimal API endpoint the configured JsonOptions are not used.Take a look at https://github.com/dotnet/aspnetcore/issues/45872 for more details.For example, if you want to specify JsonStringEnumConverter to control the serialization of enums, you need to add it to multiple places to make it work correctly.var builder = WebApplication.CreateBuilder(args);var enumConverter = new JsonStringEnumConverter(JsonNamingPolicy.SnakeCaseLower);builder.Services.AddControllers().AddJsonOptions(options =&gt;{    options.JsonSerializerOptions.Converters.Add(enumConverter);});builder.Services.ConfigureHttpJsonOptions(opt =&gt;{    // this one is used during the serialization of the response TypedResults, ref: https://github.com/dotnet/aspnetcore/issues/45872    opt.SerializerOptions.Converters.Add(enumConverter);});var app = builder.Build();app.MapControllers();app.Run();Bonus: Nall.ResultEndpoints.Template templateI‚Äôve prepare  a dotnet new template to get started with TypedResults in your .NET projects. You can install it via the following command:dotnet new install Nall.ResultEndpoints.It has everything you need to get started with this approach. You can create a new project using the following command:dotnet new result-endpoints-api    Here are some of the endpoints that you can find in the template:List all todos:public record ListTodosRequest{    [FromQuery(Name = \"completed\")]    public bool? Completed { get; set; }}public class ListTodos(TodoContext context)    : EndpointBaseSync.WithRequest&lt;ListTodosRequest&gt;.WithResult&lt;Ok&lt;IEnumerable&lt;TodoViewModel&gt;&gt;&gt;{    [EndpointSummary(\"List todos\")]    [EndpointDescription(\"List todos based on the request\")]    [Tags(\"Todo\")]    [EndpointName(nameof(ListTodos))]    [HttpGet(\"/todos\")]    public override Ok&lt;IEnumerable&lt;TodoViewModel&gt;&gt; Handle(ListTodosRequest request)    {        var query = context.Todos.AsQueryable();        if (request.Completed.HasValue)        {            query = query.Where(t =&gt; t.Completed == request.Completed.Value);        }        var items = query.OrderBy(t =&gt; t.Order).ToList();        return TypedResults.Ok(items.Select(TodoViewModel.From));    }}üí° Note, we can use a ‚Äúrequest object‚Äù (ListTodosRequest in this case) to structure the input and use model binding attributes to control the process.Create a new todo:public record CreateTodoRequest{    [Required]    [Length(1, 100)]    public required string Title { get; set; }    [Range(1, int.MaxValue)]    public int Order { get; set; }    public static TodoItem From(CreateTodoRequest todo) =&gt;        new() { Title = todo.Title, Order = todo.Order };}public class CreateTodo(TodoContext context)    : EndpointBaseAsync.WithRequest&lt;CreateTodoRequest&gt;.WithResult&lt;CreatedAtRoute&lt;TodoViewModel&gt;&gt;{    [EndpointSummary(\"Create todo\")]    [EndpointDescription(\"Create todo based on the request\")]    [Tags(\"Todo\")]    [EndpointName(nameof(CreateTodo))]    [HttpPost(\"/todos\")]    public override async Task&lt;CreatedAtRoute&lt;TodoViewModel&gt;&gt; HandleAsync(        [FromBody] CreateTodoRequest request,        CancellationToken cancellationToken = default    )    {        var item = CreateTodoRequest.From(request);        context.Todos.Add(item);        await context.SaveChangesAsync(cancellationToken);        return TypedResults.CreatedAtRoute(            TodoViewModel.From(item),            nameof(GetTodo),            new RouteValueDictionary(new { id = item.Id })        );    }}üí°Note, we can use EndpointBaseAsync to describe async endpoints.ConclusionIn this blog post, I‚Äôve shown you how to use TypedResults in .NET endpoints to achieve a consistent and strongly typed API development approach. I‚Äôve discussed the benefits of TypedResults over traditional ActionResult. Hope this article gives you enough information to get started with this approach in your .NET projects üöÄ. Let me know what you think!References  https://github.com/NikiforovAll/result-endpoints-template  https://github.com/ardalis/ApiEndpoints  https://learn.microsoft.com/en-us/aspnet/core/fundamentals/minimal-apis/responses?view=aspnetcore-8.0#iresult-return-values  https://learn.microsoft.com/en-us/aspnet/core/fundamentals/error-handling?view=aspnetcore-8.0"
    },
  
    {
      "id": "35",
      "title": "Supercharge your Dependify tool with AI chat assistant",
      "url": "/dotnet/2024/08/09/dependify-ai.html",
      "date": "August 09, 2024",
      "categories": ["dotnet"],
      "tags": ["dotnet","aspire","dependify"],
      "shortinfo": "Discover how to leverage the new chat assistant in Dependify 1.5.1",
      "content": "TL;DRDiscover how to leverage the new chat assistant in Dependify 1.5.1dotnet tool install -g Dependify.CliSource code: https://github.com/NikiforovAll/dependify            Package      Version      Description                  Dependify.Cli            Use Dependify directly from the CLI (supports plain, mermaidjs, and JSON formats) or from the browser.              Dependify.Aspire.Hosting            Aspire support              Dependify.Aspire.Hosting.Ollama            Ollama Aspire Component enables local chat completions, e.g., you can run the phi3:mini model and use it within Dependify.      IntroductionThe AI chat feature enables you to inquire about dependencies. You can ask questions regarding the relationships between modules, identify potential issues, and gain a deeper understanding of your project structure.üí°Find my more about other features from my previous blog post: Explore .NET application dependencies by using Dependify tool    RunAll you need to do it to provide the API key and the endpoint to the chat assistant. You can do it via the command line:dependify serve $dev/cap-aspire/ \\    --endpoint https://api.openai.azure.com/ \\    --deployment-name gpt-4o-mini \\    --api-key &lt;api-key&gt;    ConfigureYou can configure the chat assistant in two ways:  Environment variables (recommended way)  Command-line argumentsE.g:export DEPENDIFY__AI__ENDPOINT=\"https://api.openai.azure.com\"export DEPENDIFY__AI__DEPLOYMENT_NAME=\"gpt-4o-mini\"export DEPENDIFY__AI__API_KEY=\"\"Aspire supportYou can use Dependify as a tool/extensions for Aspire.Add the package to AppHost:dotnet add package Dependify.Aspire.HostingIn the code below, I‚Äôve added the Dependify to the Aspire starter project. (dotnet new aspire-starter)Register via IDistributedApplicationBuilder. Add the following code to your Program.cs:var builder = DistributedApplication.CreateBuilder(args);var endpointParam = builder.AddParameter(\"endpoint\");var deploymentNameParam = builder.AddParameter(\"deployment-name\");var apiKeyParam = builder.AddParameter(\"api-key\", secret: true);var apiService = builder.AddProject&lt;Projects.aspire_project_ApiService&gt;(\"apiservice\");builder.AddProject&lt;Projects.aspire_project_Web&gt;(\"webfrontend\")    .WithExternalHttpEndpoints()    .WithReference(apiService);builder.AddDependify().ServeFrom(\"../../aspire-project/\")    .WithAzureOpenAI(        endpointParam.Resource.Value,        deploymentNameParam.Resource.Value,        apiKeyParam.Resource.Value    );builder.Build().Run();Configure:dotnet user-secrets set \"Parameters:endpoint\" \"https://api.openai.azure.com\"dotnet user-secrets set \"Parameters:deployment-name\" \"gpt-4o-mini\"dotnet user-secrets set \"Parameters:api-key\" \"\"Run:dotnet run --project ./AppHostüí° It might take some time to download Dependify from GitHub Container Registry, but eventually, you will be able to run Dependify as Aspire Component.ConclusionIn this post, we explored the Dependify tool, which can help you navigate the dependency graph of your .NET application. By using this tool, you can visualize the dependencies between modules, identify potential issues, and gain a better understanding of your project structure.References  https://github.com/NikiforovAll/dependify"
    },
  
    {
      "id": "36",
      "title": "Explore .NET application dependencies by using Dependify tool",
      "url": "/dotnet/2024/08/03/dependify.html",
      "date": "August 03, 2024",
      "categories": ["dotnet"],
      "tags": ["dotnet","aspire","dependify"],
      "shortinfo": "Use Dependify tool to navigate the dependency graph of your .NET application. You can use UI, CLI or API to explore the dependencies.",
      "content": "TL;DRThis post demonstrates how to install and use the Dependify tool. It can be used to explore application dependencies between modules.Source code: https://github.com/NikiforovAll/dependifyIntroductionIn large projects, managing dependencies can become challenging due to the complexity and interconnectedness of various components. It can be difficult to navigate through the project and make assumptions about the dependencies without proper tooling or documentation.Here are a few reasons why it can be hard to navigate project dependencies in large projects:Complexity: Large projects often consist of numerous modules. Understanding how these dependencies interact with each other can be overwhelming, especially when there are multiple layers of dependencies.Dependency Chains: Dependencies can form long chains, where one module depends on another, which in turn depends on another, and so on. Tracking these chains and understanding the impact of changes can be challenging, as a modification in one module may have cascading effects on other modules.Lack of Documentation: In some cases, projects may lack comprehensive documentation that clearly outlines the dependencies and their relationships. Without proper documentation, developers may need to spend extra time investigating and reverse-engineering the project structure to understand the dependencies.To address these challenges, you can use the Dependify tool, which provides a visual representation of the dependencies in your .NET application. This tool allows you to explore the dependency graph, view the relationships between components, and identify potential issues or bottlenecks in your project.The tool can be used in different ways, such as through a graphical user interface (UI), a command-line interface (CLI), or an application programming interface (API). This flexibility allows you to choose the most suitable method for your workflow and preferences.            Package      Version      Description                  Dependify.Cli            CLI              Dependify.Core            Core library (API)              Dependify.Aspire.Hosting            Aspire support      UsageHere is how to install it:dotnet tool install -g Dependify.Cli‚ùØ dependify -hUSAGE:    Dependify.Cli.dll [OPTIONS] &lt;COMMAND&gt;EXAMPLES:    Dependify.Cli.dll graph scan ./path/to/folder --framework net8    Dependify.Cli.dll graph show ./path/to/project --framework net8OPTIONS:    -h, --help    Prints help informationCOMMANDS:    graph    serve &lt;path&gt;As you can see, there is a serve command. It starts a web server that allows you to explore the dependencies in your project using a web browser.dependify serve $dev/path-to-folder/You will see something like the following output in the terminal.    Features  Workbench ‚öôÔ∏è gives you high level overview of the dependencies in the solution and allows you to show dependencies between component in a form of a mermaid diagram. It can be a graph diagram or simple C4 component diagram.      Dependency Explorer üîé: This feature offers a more interactive user interface. It allows you to select dependencies and their descendants on-demand.    My suggestion is to install the tool and try it out on your project. It can be a great help in understanding the dependencies and relationships between components. üöÄAspire supportYou can add Dependify.Web as resource to your Aspire AppHost project.Add the package to AppHost:dotnet add package Dependify.Aspire.HostingIn the code below, I‚Äôve added the Dependify to the Aspire starter project. (dotnet new aspire-starter)Register via IDistributedApplicationBuilder. Add the following code to your Program.cs:var builder = DistributedApplication.CreateBuilder(args);var apiService = builder.AddProject&lt;Projects.aspire_project_ApiService&gt;(\"apiservice\");builder.AddProject&lt;Projects.aspire_project_Web&gt;(\"webfrontend\")    .WithExternalHttpEndpoints()    .WithReference(apiService);builder.AddDependify().ServeFrom(\"../../aspire-project/\"); // &lt;-- location of .sln filebuilder.Build().Run();All you need to do is to add the builder.AddDependify().ServeFrom() to the AppHost and run it:    ConclusionIn this post, we explored the Dependify tool, which can help you navigate the dependency graph of your .NET application. By using this tool, you can visualize the dependencies between modules, identify potential issues, and gain a better understanding of your project structure.References  https://github.com/NikiforovAll/dependify"
    },
  
    {
      "id": "37",
      "title": "Managing Startup Dependencies in .NET Aspire",
      "url": "/dotnet/aspire/2024/06/28/startup-dependencies-aspire.html",
      "date": "June 28, 2024",
      "categories": ["dotnet","aspire"],
      "tags": ["dotnet","aspnetcore","aspire"],
      "shortinfo": "This article demonstrates how to utilize .NET Aspire as an orchestrator. You will discover how effortless it is to define a dependency graph between components during startup.",
      "content": "TL;DRThis article demonstrates how to utilize .NET Aspire as an orchestrator. You will discover how effortless it is to define a dependency graph between components during startup.Source code: https://github.com/NikiforovAll/aspire-depends-onExample: https://github.com/NikiforovAll/aspire-depends-on/tree/main/samples/TodoTable of Contents:  TL;DR  Introduction  Using docker-compose üê≥  Why .NET Aspire? üöÄ  Using Aspire          Writing Custom Health Checks for Aspire        Conclusion  References    IntroductionManaging startup dependencies is a common task for any kind of a project in modern days, especially in microservices solutions.‚òùÔ∏è Here are a few reasons why it is important:      Order of Initialization: Multiple services may need to be initialized and started in a specific order. For example, if Service A depends on Service B, Service B needs to be up and running before Service A can start. By managing startup dependencies, you ensure that services are initialized in the correct order, preventing potential issues and ensuring smooth operation and startup.        Graceful Startup and Shutdown: Managing startup dependencies allows you to implement graceful startup and shutdown procedures. During startup, you can perform necessary initialization tasks, such as establishing database connections or loading configuration settings, before accepting incoming requests. Similarly, during shutdown, you can gracefully stop services, release resources, and perform any necessary cleanup operations.        Fault Tolerance and Resilience: In a distributed environment, failures are inevitable. By managing startup dependencies, you can implement fault tolerance and resilience mechanisms. For example, you can configure services to retry connecting to dependent services if they are temporarily unavailable. This helps ensure that your application can recover from failures and continue functioning properly.  To manage startup dependencies effectively, you can use various techniques and tools.Orchestrators can help automate the process of managing dependencies and ensure that services are started in the correct order.Using docker-compose üê≥One common approach is by using docker-compose.It provides a declarative way to define the dependencies between components and ensure they are started in the correct order.To illustrate , let‚Äôs assume that the api service represents a todo application that relies on a postgres database. Before the api service can start accepting requests, it needs to ensure that the necessary database migration (migrator) and seeding operations have been completed.        graph TD;        postgres --&gt; pgAdmin;        postgres --&gt; migrator;        migrator --&gt; api;    Once the migration is successfully completed, the api service can start and begin serving requests. This approach allows for a controlled and orderly startup process, ensuring that all dependencies are satisfied before the api service becomes available.version: '3.8'services:    postgres:        image: postgres:latest    depends_on:        - migrator    pgadmin:        image: dpage/pgadmin4:latest    migrator:        image: migrate/migrate:latest        depends_on:            - postgresThis docker-compose.yml file defines the services postgres, pgadmin, and migrator. The postgres service is the database, the pgadmin service is a web-based administration tool, and the migrator service is responsible for performing the database migration.By specifying the dependencies using the depends_on keyword, the migrator service will wait for the postgres service to be up and running before executing the migration. Once the migration is completed, the api service can safely start and interact with the database.version: '3.8'services:  postgres:    image: postgres:latest  depends_on:    - migrator  pgadmin:    image: dpage/pgadmin4:latest  migrator:    image: migrate/migrate:latest    depends_on:      - postgresü§îüí≠ But, wait, is it that easy? Why do we need to using something else than?Well‚Ä¶ In practice, relying solely on the depends_on directive in a Docker Compose file may not be sufficient to ensure that a component has fully started up and is ready to accept connections or perform its intended tasks. While depends_on helps manage the startup order of services, it does not guarantee that a service is fully operational before another service starts.To address this, it is common to write custom bash scripts or use other tools to perform health checks on the dependent services. These health checks can verify that the service is in a desired state before proceeding with the startup of other components.A health check typically involves making requests to the dependent service and checking for specific responses or conditions that indicate it is ready. For example, you might check if a database service is accepting connections by attempting to connect to it and verifying that it responds with a successful connection status.It‚Äôs important to note that the specific implementation of health checks and custom scripts may vary depending on the technology stack and tools you are using.Why .NET Aspire? üöÄWhile managing startup dependencies using YAML, Dockerfile, and Bash scripts can be challenging, .NET Aspire offers a better way to handle these dependencies directly in C#.With .NET Aspire, you can define a dependency graph between components during startup using C# code. This allows for a more intuitive and seamless integration with your .NET projects. You can easily specify the order of initialization, implement graceful startup and shutdown procedures, and ensure fault tolerance and resilience.By leveraging the power of C#, you have access to the rich ecosystem of .NET libraries and frameworks, making it easier to handle complex startup scenarios. You can use the built-in dependency injection capabilities of .NET to manage and resolve dependencies, ensuring that services are initialized in the correct order.Overall, .NET Aspire offers a more developer-friendly approach to managing startup dependencies in .NET projects, making it a compelling choice for simplifying your application‚Äôs startup.Using AspireI‚Äôve prepared Nuget package called Nall.Aspire.Hosting.DependsOn to simplify the process of defining dependencies between components.Let‚Äôs see how to use it to describe dependencies for the Todo application described above.Here is our starting point, without dependencies:var builder = DistributedApplication.CreateBuilder(args);var dbServer = builder    .AddPostgres(\"db-server\");dbServer.WithPgAdmin(c =&gt; c.WithHostPort(5050));var db = dbServer.AddDatabase(\"db\");var migrator = builder    .AddProject&lt;Projects.MigrationService&gt;(\"migrator\")    .WithReference(db);var api = builder    .AddProject&lt;Projects.Api&gt;(\"api\")    .WithReference(db);builder.Build().Run();üí° I strongly recommend you checking out source code for more details, it is a fully-functional application. All you need to do - is to run it by hitting F5 - https://github.com/NikiforovAll/aspire-depends-on/tree/main/samples/TodoNow, let‚Äôs install required packages. Nall.Aspire.Hosting.DependsOn defines a WaitFor and WaitForCompletion methods used to determine startup ordering.Install core package üì¶:dotnet add package Nall.Aspire.Hosting.DependsOnAlso, we need to install specific packages for the dependencies. In our case, it is a PostgreSQL database.Install health-check-specific package üì¶:dotnet add package Nall.Aspire.Hosting.DependsOn.PostgreSQLSpecific health checks packages define WithHealthCheck extension methods. These methods are technology specific, but in essence are easy to understand and could be written on demand for any kind of dependency without using Nall.Aspire.Hosting.DependsOn. Later, I will show you how to write your own WithHealthCheck method.Let‚Äôs put everything together:var builder = DistributedApplication.CreateBuilder(args);var dbServer = builder    .AddPostgres(\"db-server\")    .WithHealthCheck(); // &lt;-- define health checkdbServer.WithPgAdmin(c =&gt; c.WithHostPort(5050)    .WaitFor(dbServer)); // &lt;-- wait for dbvar db = dbServer.AddDatabase(\"db\");var migrator = builder    .AddProject&lt;Projects.MigrationService&gt;(\"migrator\")    .WithReference(db)    .WaitFor(db); // &lt;-- wait for dbvar api = builder    .AddProject&lt;Projects.Api&gt;(\"api\")    .WithReference(db)    .WaitForCompletion(migrator); // &lt;-- wait until process is terminatedbuilder.Build().Run();Writing Custom Health Checks for AspireBy convention, WithHealthCheck method should define HealthCheckAnnotation annotation for Aspire resource.public class HealthCheckAnnotation(    Func&lt;IResource, CancellationToken, Task&lt;IHealthCheck?&gt;&gt; healthCheckFactory) : IResourceAnnotationLet‚Äôs see the implementation of Nall.Aspire.Hosting.DependsOn.Uris, the package that allows to wait for resources that expose a health check endpoint, usually it is /health endpoint that returns ‚Äú200 OK‚Äù status code if everything is fine.Here is WithHealthCheck method signature:public static IResourceBuilder&lt;T&gt; WithHealthCheck&lt;T&gt;(    this IResourceBuilder&lt;T&gt; builder,    string? endpointName = null,    string path = \"health\") where T : IResourceWithEndpointsNote, usually, you don‚Äôt really need to implement health checking logic, AspNetCore.HealthChecks.* has something to offer.In case of Nall.Aspire.Hosting.DependsOn.Uris, it depends on AspNetCore.HealthChecks.Uris.So the implementation is straight forward, just find a endpoint by name and define health check for a given path:public static IResourceBuilder&lt;T&gt; WithHealthCheck&lt;T&gt;(    this IResourceBuilder&lt;T&gt; builder,    string? endpointName = null,    string path = \"health\")    where T : IResourceWithEndpoints{    return builder.WithAnnotation(new HealthCheckAnnotation(        async (resource, ct) =&gt;        {            if (resource is not IResourceWithEndpoints resourceWithEndpoints)            {                return null;            }            var endpoint = endpointName is null                ? resourceWithEndpoints                    .GetEndpoints()                    .FirstOrDefault(e =&gt; e.Scheme is \"http\" or \"https\")                : resourceWithEndpoints.GetEndpoint(endpointName);            var url = endpoint?.Url;            var options = new UriHealthCheckOptions();            options.AddUri(new(new(url), path));            var client = new HttpClient();            return new UriHealthCheck(options, () =&gt; client);        }    ));}ConclusionIn this article, we explored the challenges of managing startup dependencies in modern applications, particularly in the context of microservices architectures..NET Aspire elevates the management of startup dependencies. By allowing developers to define dependency graphs in C#.Through practical examples, we demonstrated how to use .NET Aspire and its extensions to manage dependencies in a more intuitive and reliable manner. We also touched on the creation of custom health checks, showcasing the flexibility and extensibility of .NET Aspire.üôå In conclusion, managing startup dependencies has never been easier with .NET Aspire, and I believe every project needs it.References  https://github.com/NikiforovAll/aspire-depends-on"
    },
  
    {
      "id": "38",
      "title": "Learn .NET Aspire by example: Polyglot persistence featuring PostgreSQL, Redis, MongoDB, and Elasticsearch",
      "url": "/dotnet/aspire/2024/06/18/polyglot-persistance-with-aspire.html",
      "date": "June 18, 2024",
      "categories": ["dotnet","aspire"],
      "tags": ["dotnet","aspnetcore","aspire","databases"],
      "shortinfo": "Learn how to set up various databases using Aspire by building a simple social media application.",
      "content": "TL;DRLearn how to set up various databases using Aspire by building a simple social media application.Source code: https://github.com/NikiforovAll/social-media-app-aspireTable of Contents:  TL;DR  Introduction          Case Study - Social Media App        Application Design. Add PostgreSQL          Code        Application Design. Add Redis          Code        Application Design. Add MongoDb          Code        Application Design. Add Elasticsearch          Code                  Search          Analytics                      Putting everything together. Migration          Migration        Conclusion  ReferencesIntroductionPolyglot persistence refers to the practice of using multiple databases or data storage technologies within a single application. Instead of relying on just one database for everything, you can choose the most suitable database for each specific need or type of data. It‚Äôs like having a toolbox with different tools for different tasks, so you can use the right tool for each job.The importance of polyglot persistence lies in the fact that different databases excel in different areas. For example, relational databases like PostgreSQL are great for structured data and complex queries, while NoSQL databases like MongoDB are better suited for handling unstructured or semi-structured data. Similarly, Elasticsearch is optimized for full-text search, and Redis excels in caching and high-performance scenarios.By leveraging the strengths of different databases, developers can design more efficient and scalable systems. They can choose the right tool for the job, ensuring that each component of the application is using the most suitable database technology.The selection of the correct database depends on various factors such as the nature of the data, the expected workload, performance requirements, scalability needs, and the complexity of the queries. By carefully considering these factors, developers can ensure that the chosen databases align with the specific requirements of the application.Case Study - Social Media AppIn this post we will design social media application. It typically includes something like: Users, Posts, Follows, Likes, etc.The REST API for the application looks will something like following:graph    API[API]    Users[Users]    Posts[Posts]    Follows[Follows]    Likes[Likes]    Search[Search]    Analytics[Analytics]    API --&gt; Users    API --&gt; Posts    API --&gt; Follows    API --&gt; Likes    API --&gt; Search    API --&gt; Analytics    subgraph Users    GET_users[GET /users]    GET_users_id[GET /users/id]    POST_users[POST /users]    PUT_users_id[PUT /users/id]    DELETE_users_id[DELETE /users/id]    end    subgraph Posts    GET_posts[GET users/id/posts]    GET_posts_id[GET /posts/id]    POST_posts[POST /posts]    DELETE_posts_id[DELETE /posts/id]    end    subgraph Follows    GET_users_id_follows[GET /users/id/follows]    POST_users_id_follows[POST /users/id/follows]    DELETE_users_id_follows_followId[DELETE /users/id/follows/followId]    end    subgraph Likes    GET_posts_id_likes[GET /posts/id/likes]    POST_posts_id_likes[POST /posts/id/likes]    DELETE_posts_id_likes_userId[DELETE /posts/id/likes/userId]    end    subgraph Search    GET_search_users[GET /search/users?query=query]    GET_search_posts[GET /search/posts?query=query]    end    subgraph Analytics    GET_analytics_users[GET /analytics/leaderboard]    endOverall, these components provide essential functionality for a social media app. The search functionality allows users to find other users and relevant posts, while the analytics component provides insights into user activity and engagement.Application Design. Add PostgreSQLIn this section, we will discuss how to implement the users and follows functionality.Relational data, such as user information and their relationships (follows), can be easily represented and managed in a relational database like PostgreSQL.One of the main benefits of using a relational database is the ability to efficiently retrieve and manipulate data. They are well-suited for scenarios where data needs to be structured and related to each other, such as user information and their relationships.Relational databases scale efficiently in most cases. As the amount of data and the number of users grow, relational databases can handle the increased load by optimizing queries and managing indexes. This means that as long as the database is properly designed and optimized, it can handle a significant amount of data and user activity without performance issues.However, in some cases, when an application experiences extremely high traffic or deals with massive amounts of data, even a well-optimized relational database may face scalability challenges.In situations where the workload grows, it is apply vertical scaling. Vertical scaling involves upgrading hardware resources or optimizing the database configuration to increase the capacity of a single database instance, such as adding more memory or CPU power. This approach is typically sufficient, but sometime s it becomes practically impossible to further scale vertically.At that point, a natural transition is to employ horizontal scaling through a technique called sharding. Sharding involves distributing the data across multiple database instances or servers, where each instance or server is responsible for a subset of the data. By dividing the workload among multiple database nodes, each node can handle a smaller portion of the data, resulting in improved performance and scalability.In practice, implementing horizontal scaling with sharding in a real-world application is not as straightforward as it may seem. It requires careful planning and significant effort to ensure a smooth and successful transition.CodeWe will use Aspire PostgreSQL component. See Aspire documentation to explore what is actually means to ‚Äúconsume‚Äù Aspire Component. See .NET Aspire components overviewHere is how to configure AppHost. Add Aspire.Hosting.PostgreSQL package and configure it:var builder = DistributedApplication.CreateBuilder(args);var usersDb = builder    .AddPostgres(\"dbserver\")    .WithDataVolume()    .WithPgAdmin(c =&gt; c.WithHostPort(5050))    .AddDatabase(\"users-db\");var api = builder    .AddProject&lt;Projects.Api&gt;(\"api\")    .WithReference(usersDb);var migrator = builder    .AddProject&lt;Projects.MigrationService&gt;(\"migrator\")    .WithReference(usersDb);Add Aspire.Npgsql.EntityFrameworkCore.PostgreSQL and register UsersDbContext via AddNpgsqlDbContext method:var builder = WebApplication.CreateBuilder(args);builder.AddServiceDefaults();builder.AddNpgsqlDbContext&lt;UsersDbContext&gt;(\"users-db\");var app = builder.Build();app.MapUsersEndpoints();app.MapDefaultEndpoints();app.Run();Use DbContext regularly:var users = app.MapGroup(\"/users\");users.MapGet(        \"/{id:int}/followers\",        async Task&lt;Results&lt;Ok&lt;List&lt;UserSummaryModel&gt;&gt;, NotFound&gt;&gt; (            int id,            UsersDbContext dbContext,            CancellationToken cancellationToken        ) =&gt;        {            var user = await dbContext                .Users.Where(u =&gt; u.UserId == id)                .Include(u =&gt; u.Followers)                .ThenInclude(f =&gt; f.Follower)                .FirstOrDefaultAsync(cancellationToken);            if (user == null)            {                return TypedResults.NotFound();            }            var followers = user                .Followers.Select(x =&gt; x.Follower)                .ToUserSummaryViewModel()                .ToList();            return TypedResults.Ok(followers);        }    )    .WithName(\"GetUserFollowers\")    .WithTags(Tags)    .WithOpenApi();}Here is how to retrieve followers of a user:‚ùØ curl -X 'GET' 'http://localhost:51909/users/1/followers' -s | jq# [#   {#     \"id\": 522,#     \"name\": \"Jerome Kilback\",#     \"email\": \"Jerome_Kilback12@gmail.com\"#   },#   {#     \"id\": 611,#     \"name\": \"Ernestine Schiller\",#     \"email\": \"Ernestine_Schiller@hotmail.com\"#   }# ]üí° See source code for more details of how to represent a user and follower relationship.üí° See source code to learn how to apply database migrations and use Bogus to seed the data.Application Design. Add RedisRedis can be used as a first-level cache in an application to improve performance and reduce the load on the primary data source, such as a database.A first-level cache, also known as an in-memory cache, is a cache that resides closest to the application and stores frequently accessed data. It is typically implemented using fast and efficient data stores, such as Redis, to provide quick access to the cached data.When a request is made to retrieve data, the application first checks the first-level cache. If the data is found in the cache, it is returned immediately, avoiding the need to query the primary data source. This significantly reduces the response time and improves the overall performance of the application.To use Redis as a first-level cache, the application needs to implement a caching layer that interacts with Redis. When data is requested, the caching layer checks if the data is present in Redis. If it is, the data is returned from the cache. If not, the caching layer retrieves the data from the primary data source, stores it in Redis for future use, and then returns it to the application.By using Redis as a first-level cache, applications can significantly reduce the load on the primary data source, improve response times, and provide a better user experience.CodeIt is very easy to setup Redis Output Caching Component.Just install Aspire.StackExchange.Redis.OutputCaching and use it like this:From the AppHost:var builder = DistributedApplication.CreateBuilder(args);var redis = builder.AddRedis(\"cache\");var usersDb = builder    .AddPostgres(\"dbserver\")    .WithDataVolume()    .WithPgAdmin(c =&gt; c.WithHostPort(5050))    .AddDatabase(\"users-db\");var api = builder    .AddProject&lt;Projects.Api&gt;(\"api\")    .WithReference(usersDb)    .WithReference(redis);From the consuming service:var builder = WebApplication.CreateBuilder(args);builder.AddServiceDefaults();builder.AddNpgsqlDbContext&lt;UsersDbContext&gt;(\"users-db\");builder.AddRedisOutputCache(\"cache\"); // &lt;-- add thisvar app = builder.Build();app.MapUsersEndpoints();app.MapDefaultEndpoints();app.UseOutputCache(); // &lt;-- add thisapp.Run();Just add one-liner using CacheOutput method, meta-programming at this‚Äôs best üòé:var users = app.MapGroup(\"/users\");users    .MapGet(        \"\",        async (            UsersDbContext dbContext,            CancellationToken cancellationToken        ) =&gt;        {            var users = await dbContext                .Users.ProjectToViewModel()                .OrderByDescending(u =&gt; u.FollowersCount)                .ToListAsync(cancellationToken);            return TypedResults.Ok(users);        }    )    .WithName(\"GetUsers\")    .WithTags(Tags)    .WithOpenApi()    .CacheOutput(); // &lt;-- add thisHere is first hit to the API, it takes ~25ms:    And here is subsequent request, it takes ~6ms:    üí°Note, in the real-world scenario, the latency reduction can be quite significant when using Redis as a caching layer. For instance, consider an application that initially takes around 120ms to fetch data directly from a traditional SQL database. After implementing Redis for caching, the same data retrieval operation might only take about 15ms. This represents a substantial decrease in latency, improving the application‚Äôs responsiveness and overall user experience.Application Design. Add MongoDbNoSQL databases, such as MongoDB, are well-suited for storing unstructured or semi-structured data like posts and related likes in a social media application. Unlike relational databases, NoSQL databases do not enforce a fixed schema, allowing for flexible and dynamic data models.In a NoSQL database, posts can be stored as documents, which are similar to JSON objects. Each document can contain various fields representing different attributes of a post, such as the post content, author, timestamp, and likes. The likes can be stored as an array within the post document, where each element represents a user who liked the post.One of the key advantages of using NoSQL databases for storing posts and related likes is the ability to perform efficient point reads. Point reads refer to retrieving a single document or a specific subset of data from a database based on a unique identifier, such as the post ID.NoSQL databases like MongoDB use indexes to optimize point reads. By creating an index on the post ID field, the database can quickly locate and retrieve the desired post document based on the provided ID. This allows for fast and efficient retrieval of individual posts and their associated likes.Furthermore, NoSQL databases can horizontally scale by distributing data across multiple servers or nodes. This enables high availability and improved performance, as the workload is divided among multiple instances. As a result, point reads can be performed in parallel across multiple nodes, further enhancing the efficiency of retrieving posts and related likes.MongoDB excels at this because it was designed with built-in support for horizontal scaling. This means you can easily add more servers as your data grows, without any downtime or interruption in service.CodeAdding MongoDb Component is straightforward, the process is similar to adding other databases:Add to AppHost:var builder = DistributedApplication.CreateBuilder(args);var postsDb = builder    .AddMongoDB(\"posts-mongodb\")    .WithDataVolume()    .WithMongoExpress(c =&gt; c.WithHostPort(8081))    .AddDatabase(\"posts-db\");var api = builder    .AddProject&lt;Projects.Api&gt;(\"api\")    .WithReference(postsDb);Consuming service:var builder = WebApplication.CreateBuilder(args);builder.AddServiceDefaults();builder.AddMongoDBClient(\"posts-db\");var app = builder.Build();app.MapPostsEndpoints();app.MapDefaultEndpoints();app.Run();To use it, we define a simple wrapper client over a MongoDb collection:public class PostService{    private readonly IMongoCollection&lt;Post&gt; collection;    public PostService(        IMongoClient mongoClient,        IOptions&lt;MongoSettings&gt; settings    )    {        var database = mongoClient.GetDatabase(settings.Value.Database);        this.collection = database.GetCollection&lt;Post&gt;(settings.Value.Collection);    }    public async Task&lt;Post?&gt; GetPostByIdAsync(        string id,        CancellationToken cancellationToken = default    ) =&gt;        await this            .collection.Find(x =&gt; x.Id == id)            .FirstOrDefaultAsync(cancellationToken);}Notice, IMongoClient is added to the DI by AddMongoDBClient method.var posts = app.MapGroup(\"/posts\");posts    .MapGet(        \"/{postId}\",        async Task&lt;Results&lt;Ok&lt;PostViewModel&gt;, NotFound&gt;&gt; (            string postId,            PostService postService,            CancellationToken cancellationToken        ) =&gt;        {            var post = await postService.GetPostByIdAsync(postId, cancellationToken);            return post == null                ? TypedResults.NotFound()                : TypedResults.Ok(post.ToPostViewModel());        }    )    .WithName(\"GetPostById\")    .WithTags(Tags)    .WithOpenApi();Application Design. Add ElasticsearchElasticsearch is a powerful search and analytics engine that is commonly used for full-text search and data analysis in applications. It is designed to handle large volumes of data and provide fast and accurate search results.To use Elasticsearch for full-text search, you need to index your data in Elasticsearch. The data is divided into documents, and each document is composed of fields that contain the actual data. Elasticsearch indexes these documents and builds an inverted index, which allows for efficient searching.To search for posts using Elasticsearch, you can perform a full-text search query. This query can include various parameters, such as the search term, filters, sorting, and pagination. Elasticsearch will analyze the search term and match it against the indexed documents, returning the most relevant results.Elasticsearch is also well-suited for performing analytics tasks, such as calculating leaderboards. Elasticsearch aggregation feature allows you to group and summarize data based on certain criteria. For example, you can aggregate likes by author and calculate the number of likes for each author.One of the key advantages of Elasticsearch is its ability to scale horizontally, allowing you to handle large volumes of data and increasing the capacity of your search and analytics system.To achieve scalability, Elasticsearch uses a distributed architecture. It allows you to create a cluster of multiple nodes, where each node can hold a portion of the data and perform search and indexing operations. This distributed nature enables Elasticsearch to handle high traffic loads and provide fast response times.When you add more nodes to the cluster, Elasticsearch automatically redistributes the data across the nodes, ensuring that the workload is evenly distributed. This allows you to scale your system by simply adding more hardware resources without any downtime or interruption in service.In addition to distributing data, Elasticsearch also supports data replication. By configuring replica shards, Elasticsearch creates copies of the data on multiple nodes. This provides fault tolerance and high availability, as the system can continue to function even if some nodes fail.CodeAs for now, there is no official Elasticsearch Component for Aspire, the work is in progress. But, it is possible to define custom Aspire Component and share them as a Nuget package.üí° For the sake of simplicity, I will not explain how to add Elasticsearch Aspire Component, please see source code for more details.Assuming we already have Elasticsearch component, here is how to add it to AppHost:var builder = DistributedApplication.CreateBuilder(args);var redis = builder.AddRedis(\"cache\");var postsDb = builder    .AddMongoDB(\"posts-mongodb\")    .WithDataVolume()    .WithMongoExpress(c =&gt; c.WithHostPort(8081))    .AddDatabase(\"posts-db\");var elastic = builder    .AddElasticsearch(\"elasticsearch\", password, port: 9200)    .WithDataVolume();var api = builder    .AddProject&lt;Projects.Api&gt;(\"api\")    .WithReference(elastic);Add it to consuming services:var builder = WebApplication.CreateBuilder(args);builder.AddServiceDefaults();builder.AddElasticClientsElasticsearch(\"elasticsearch\");builder.AddMongoDBClient(\"posts-db\");var app = builder.Build();app.MapPostsEndpoints();app.MapDefaultEndpoints();app.Run();As result, we have ElasticsearchClient injected into the DI.But how do we get data into Elasticsearch? Elasticsearch isn‚Äôt typically used as a primary database. Instead, it‚Äôs used as a secondary database that‚Äôs optimized for read operations, especially search. So, we take our data from our primary database (MongoDb), break it down into smaller, simpler pieces, and then feed it into Elasticsearch. This makes it easier for Elasticsearch to search through the data quickly and efficiently.Searchüí°The process of reliable data denormalization is out of scope of this article. A straightforward implementation might be to write the data to the primary database and then to the secondary one.Assuming we already have data in Elasticsearch, let‚Äôs see how to query it:public class ElasticClient(ElasticsearchClient client){    private const string PostIndex = \"posts\";    public async Task&lt;IEnumerable&lt;IndexedPost&gt;&gt; SearchPostsAsync(        PostSearch search,        CancellationToken cancellationToken = default    )    {        var searchResponse = await client.SearchAsync&lt;IndexedPost&gt;(            s =&gt;            {                void query(QueryDescriptor&lt;IndexedPost&gt; q) =&gt;                    q.Bool(b =&gt;                        b.Should(sh =&gt;                        {                            sh.Match(p =&gt;                                p.Field(f =&gt; f.Title).Query(search.Title)                            );                            sh.Match(d =&gt;                                d.Field(f =&gt; f.Content).Query(search.Content)                            );                        })                    );                s.Index(PostIndex).From(0).Size(10).Query(query);            },            cancellationToken        );        EnsureSuccess(searchResponse);        return searchResponse.Documents;    }}In the code above we are performing full-text search on ‚ÄúTitle‚Äù and ‚ÄúContent‚Äù fields and returning top 10 results.And here is how to use it:var posts = app.MapGroup(\"/posts\");posts    .MapGet(        \"/search\",        async (            [FromQuery(Name = \"q\")] string searchTerm,            ElasticClient elasticClient,            PostService postService,            CancellationToken cancellationToken        ) =&gt;        {            var posts = await elasticClient.SearchPostsAsync(                new() { Content = searchTerm, Title = searchTerm },                cancellationToken            );            IEnumerable&lt;Post&gt; result = [];            if (posts.Any())            {                result = await postService.GetPostsByIds(                    posts.Select(x =&gt; x.Id),                    cancellationToken                );            }            return TypedResults.Ok(result.ToPostViewModel());        }    )    .WithName(\"SearchPosts\")    .WithTags(Tags)    .WithOpenApi();The typical pattern in this scenario is to search for posts in Elasticsearch and retrieve the full representation from the primary database (MongoDB).AnalyticsAnother interesting task that can be solved via Elasticsearch is analytics. Elasticsearch excels at analyzing large amounts of data. It provides real-time analytics, which means you can get insights from your data as soon as it is indexed. This is particularly useful for time-sensitive data or when you need to monitor trends, track changes, and make decisions quickly. Its powerful aggregation capabilities allow you to summarize, group, and calculate data on the fly, making it a great tool for both search and analytics.Here is how to calculate Leader Board for the social media application:public async Task&lt;AnalyticsResponse&gt; GetAnalyticsDataAsync(        AnalyticsRequest request,        CancellationToken cancellationToken = default){    const string Key = \"user_likes\";    var aggregationResponse = await client.SearchAsync&lt;IndexedLike&gt;(        s =&gt;            s.Index(LikeIndex)                .Size(0)                .Query(q =&gt;                {                    q.Range(r =&gt;                        r.DateRange(d =&gt;                        {                            d.Gte(request.start.Value.ToString(\"yyyy-MM-dd\"));                            d.Lte(request.end.Value.ToString(\"yyyy-MM-dd\"));                        })                    );                })                .Aggregations(a =&gt;                     a.Add(Key,                        t =&gt; t.Terms(f =&gt; f.Field(f =&gt; f.AuthorId).Size(request.top)))),        cancellationToken    );    EnsureSuccess(aggregationResponse);    // processes aggregation result and returns UserId to NumberOfLikes dictionary    return new AnalyticsResponse(aggregationResponse);}And here is how to retrieve the users with the highest number of likes and enrich the data from the primary database:posts    .MapPost(        \"/analytics/leaderboard\",        async (            [FromQuery(Name = \"startDate\")] DateTimeOffset? startDate,            [FromQuery(Name = \"endDate\")] DateTimeOffset? endDate,            ElasticClient elasticClient,            UsersDbContext usersDbContext,            CancellationToken cancellationToken        ) =&gt;        {            var analyticsData =                await elasticClient.GetAnalyticsDataAsync(new(startDate, endDate),cancellationToken);            var userIds = analyticsData                .Leaderboard.Keys.Select(x =&gt; x)                .ToList();            var users = await usersDbContext                .Users.Where(x =&gt; userIds.Contains(x.UserId))                .ToListAsync(cancellationToken: cancellationToken);            return TypedResults.Ok(                users                    .Select(x =&gt; new                    {                        x.UserId,                        x.Name,                        x.Email,                        LikeCount = analyticsData.Leaderboard[x.UserId],                    })                    .OrderByDescending(x =&gt; x.LikeCount)            );        }    )    .WithName(\"GetLeaderBoard\")    .WithTags(Tags)    .WithOpenApi();Putting everything together. MigrationThe AppHost serves as the composition root of a distributed system, where high-level details about the system‚Äôs architecture and components are defined and configured.In the AppHost, you can define and configure various components that make up the distributed system. These components can include databases, message brokers, caching systems, search engines, and other services that are required for the system to function.As result, AppHost looks like following:var builder = DistributedApplication.CreateBuilder(args);var usersDb = builder    .AddPostgres(\"dbserver\")    .WithDataVolume()    .WithPgAdmin(c =&gt; c.WithHostPort(5050))    .AddDatabase(\"users-db\");var postsDb = builder    .AddMongoDB(\"posts-mongodb\")    .WithDataVolume()    .WithMongoExpress(c =&gt; c.WithHostPort(8081))    .AddDatabase(\"posts-db\");var elastic = builder    .AddElasticsearch(\"elasticsearch\", port: 9200)    .WithDataVolume();var redis = builder.AddRedis(\"cache\");var messageBus = builder    .AddRabbitMQ(\"messaging\", port: 5672)    .WithDataVolume()    .WithManagementPlugin();var api = builder    .AddProject&lt;Projects.Api&gt;(\"api\")    .WithReference(usersDb)    .WithReference(postsDb)    .WithReference(elastic)    .WithReference(redis)    .WithReference(messageBus);var migrator = builder    .AddProject&lt;Projects.MigrationService&gt;(\"migrator\")    .WithReference(postsDb)    .WithReference(elastic)    .WithReference(usersDb);builder.Build().Run();As you can see, it‚Äôs quite easy to put everything together.When working on a project, it is generally a good practice to maintain consistency in how you organize and structure your code. This includes how you reference and use resources within your project.For example, we have the ‚Äúmigrator‚Äù service that references other resources in the same way as ‚Äúapi‚Äù service does.MigrationThe MigrationService is responsible for databases migration and seeding.Here is how to generate test data:private static (List&lt;Post&gt;, List&lt;IndexedLike&gt;) GeneratePosts(){    var faker = new Faker&lt;Post&gt;()        .RuleFor(p =&gt; p.Title, f =&gt; f.Lorem.Sentence())        .RuleFor(p =&gt; p.Content, f =&gt; f.Lorem.Paragraph())        .RuleFor(p =&gt; p.ExternalId, f =&gt; f.Random.AlphaNumeric(10))        .RuleFor(p =&gt; p.CreatedAt, f =&gt; f.Date.Past())        .RuleFor(p =&gt; p.AuthorId, f =&gt; f.Random.Number(1, numberOfUsers));    var posts = faker.Generate(numberOfPosts).ToList();    var likeFaker = new Faker&lt;IndexedLike&gt;()        .RuleFor(l =&gt; l.PostId, f =&gt; f.PickRandom(posts).ExternalId)        .RuleFor(l =&gt; l.LikedBy, f =&gt; f.Random.Number(1, numberOfUsers))        .RuleFor(l =&gt; l.CreatedAt, f =&gt; f.Date.Past());    var likes = likeFaker        .Generate(numberOfLikes)        .GroupBy(l =&gt; l.PostId)        .ToDictionary(g =&gt; g.Key, g =&gt; g.ToList());    foreach (var post in posts)    {        var postLikes = likes.GetValueOrDefault(post.ExternalId) ?? [];        post.Likes.AddRange(postLikes.Select(x =&gt; x.LikedBy));        foreach (var l in postLikes)        {            l.AuthorId = post.AuthorId;        }    }    return (posts, likes.Values.SelectMany(x =&gt; x).ToList());}The migration process is instrumented via OpenTelemetry and you can inspect how much it takes to execute the migration and seeding process per-database.    üí°As you can see, it takes some time for Elasticsearch to boot up. This is one of the examples that demonstrate why it is important to use resiliency patterns to build more robust and reliable systems.ConclusionPolyglot persistence is a powerful approach to designing data storage solutions for applications. By leveraging the strengths of different databases, developers can build efficient and scalable systems that meet the specific requirements of their applications.In this post, we explored how to implement polyglot persistence in a social media application using PostgreSQL, Redis, MongoDB, and Elasticsearch. Each database was used for a specific purpose, such as storing user data, caching, storing posts, and performing search and analytics tasks.By carefully selecting the appropriate databases for each use case, developers can design robust and performant applications that deliver a great user experience. The flexibility and scalability provided by polyglot persistence enable applications to handle diverse workloads and data requirements, ensuring that they can grow and evolve over time.References  https://github.com/dotnet/aspire-samples  https://github.com/NikiforovAll/social-media-app-aspire"
    },
  
    {
      "id": "39",
      "title": "A .NET Developer Guide to XUnit Test Instrumentation with OpenTelemetry and Aspire Dashboard",
      "url": "/dotnet/opentelemetry/2024/06/12/developer-guide-to-xunit-otel.html",
      "date": "June 12, 2024",
      "categories": ["dotnet","opentelemetry"],
      "tags": ["dotnet","aspnetcore","opentelemetry","aspire"],
      "shortinfo": "In this guide, we will explored how to leverage XUnit and OpenTelemetry to instrument .NET test projects. The process of setting up the XUnit.Otel.Template demonstrates the ease with which developers...",
      "content": "TL;DRIn this guide, we will explored how to leverage XUnit and OpenTelemetry to instrument .NET test projects. The process of setting up the XUnit.Otel.Template demonstrates the ease with which developers can start instrumenting their tests, making it accessible even for those new to OpenTelemetry or Aspire Dashboard.Source code: https://github.com/NikiforovAll/xunit-instrumentation-otel-template    Table of Contents:  TL;DR  Introduction  Installation  Run Tests          Explore the code      Results of Test Run      Metrics        Conclusion  ReferencesIntroductionAs discussed in my previous blog post - Automated Tests Instrumentation via OpenTelemetry and Aspire Dashboard, we can utilize OpenTelemetry and Aspire Dashboard to gain valuable insights into the execution of our tests. This allows us to collect and analyze data over time, enabling us to identify potential anomalies.Considering the positive response from the community, I have taken the initiative to enhance the existing approach and create a reusable starter template for everyone to benefit from.Installation‚ùØ dotnet new install XUnit.Otel.Template::1.0.0# The following template packages will be installed:#    XUnit.Otel.Template::1.0.0# Success: XUnit.Otel.Template::1.0.0 installed the following templates:# Template Name  Short Name  Language  Tags# -------------  ----------  --------  -------------------------# XUnit Otel     xunit-otel  [C#]      XUnit/Tests/OpenTelemetryGenerate:‚ùØ dotnet new xunit-otel -o $dev/XUnitOtelExample01 -n XUnitOtelExample# The template \"XUnit Otel\" was created successfully.Run TestsNow let‚Äôs navigate to the project directory and run test project with additional option (environment variable really) to include warmup trace. Warmup trace is a special trace that shows how much time it takes to configure dependencies:‚ùØ XUNIT_OTEL_TRACE_WARMUP=true dotnet test# Restore complete (1.2s)# You are using a preview version of .NET. See: https://aka.ms/dotnet-support-policy#   XUnitOtelExample succeeded (4.9s) ‚Üí bin\\Debug\\net8.0\\XUnitOtelExample.dll#   XUnitOtelExample test succeeded (2.8s)# Build succeeded in 9.2s# Test run succeeded. Total: 3 Failed: 0 Passed: 3 Skipped: 0, Duration: 2.8sLet‚Äôs navigate to http://localhost:18888/traces to see the results of test execution.‚òùÔ∏è Aspire Dashboard is automatically started based on Testcontainers setup as part of BaseFixture.    As you can see, there are two traces one for test run and one for warmup.Explore the codeBefore we move further let‚Äôs explore the content of exemplary test suit.[TracePerTestRun]public class ExampleTests(BaseFixture fixture) : BaseContext(fixture){    [Fact]    public async Task WaitRandomTime_Success()    {        // ...    }    [Fact]    public async Task WaitRandomTime_ProducesSubActivity_Success()    {        // ...    }    [Fact]    public async Task WaitRandomTime_AsyncWait_Success()    {       // ...    }}WaitRandomTime_Success: This test waits for a random duration between 100 and 500 milliseconds and then asserts that the operation completes successfully. Note, the special method called Runner. It is intended to wrap the asserted code so we can capture exceptions and enrich the traces with additional data such as exception message, trace, etc.[Fact]public async Task WaitRandomTime_Success(){    // Given    int waitFor = Random.Shared.Next(100, 500);    TimeSpan delay = TimeSpan.FromMilliseconds(waitFor);    // When    await Task.Delay(delay);    // Then    Runner(() =&gt; Assert.True(true));}WaitRandomTime_ProducesSubActivity_Success: Similar to the first, but it waits for a shorter random duration (between 50 and 250 milliseconds). It also starts a new activity named ‚ÄúSubActivity‚Äù, logs an event indicating a delay has been waited for, and sets a tag with the delay duration. The test asserts success after the wait. This example demonstrate how to add additional traces to test suit if needed.[Fact]public async Task WaitRandomTime_ProducesSubActivity_Success(){    // Given    using var myActivity = BaseFixture.ActivitySource.StartActivity(\"SubActivity\");    int waitFor = Random.Shared.Next(50, 250);    TimeSpan delay = TimeSpan.FromMilliseconds(waitFor);    // When    await Task.Delay(delay);    myActivity?.AddEvent(new($\"WaitedForDelay\"));    myActivity?.SetTag(\"subA_activity:delay\", waitFor);    // Then    Runner(() =&gt; Assert.True(true));}WaitRandomTime_AsyncWait_Success: This test is partially shown. Like the others, it waits for a random duration between 50 and 250 milliseconds then within a Runner method, waits for the delay again before asserting a condition that is always true. This demonstrates asynchronous Runner execution.[Fact]public async Task WaitRandomTime_AsyncWait_Success(){    // Given    int waitFor = Random.Shared.Next(50, 250);    TimeSpan delay = TimeSpan.FromMilliseconds(waitFor);    // When    await Task.Delay(delay);    // Then    await Runner(async () =&gt;    {        await Task.Delay(delay);        Assert.True(true);    });}Results of Test RunHere is the result of trace output, as you can see, every test has it‚Äôs own trace and we can see how tests are executed sequentially by XUnit:    Now, let‚Äôs modify the WaitRandomTime_AsyncWait_Success test to intentionally cause it to fail. This will allow us to observe how the test framework displays failed tests:    Below are the details of the test run. Failed tests are readily identifiable on the Aspire Dashboard, where each failed test is accompanied by an Trace Event with exception details. This event provides detailed insights into the reasons behind the test failure.    MetricsThese metrics highlight the execution time on a per-test and per-class basis, categorized by tags.    The P50 percentile, also known as the median, represents the middle value of a dataset when it‚Äôs sorted in ascending order. In the context of test execution, the P50 percentile for execution time tells you that:  50% of your tests complete faster than this time.  50% of your tests complete slower than this time.Here‚Äôs how you can use the P50 percentile for test execution:1. Performance Benchmark:  The P50 provides a good representation of the ‚Äútypical‚Äù test execution time.  You can use it as a baseline to compare performance over time. For example, if your P50 increases significantly after a code change, it might indicate a performance regression.2. Setting Realistic Expectations:  Instead of focusing on the absolute fastest or slowest tests, the P50 gives you a realistic idea of how long most tests take to execute.3. Identifying Areas for Improvement:  While the P50 represents the median, a large difference between the P50 and higher percentiles (like P90 or P95) indicates a wide spread in execution times.  This suggests that some tests are significantly slower than others, and you might want to investigate those outliers for potential optimizations.Example:Let‚Äôs say your test suite has a P50 execution time of 200 milliseconds. This means:  Half of your tests finish in under 200 milliseconds.  Half of your tests take longer than 200 milliseconds.In summary, the P50 percentile provides a valuable metric for understanding the typical performance of your tests and identifying areas for optimization. It helps you set realistic expectations, track performance trends, and make informed decisions about your testing process.ConclusionIn this guide, we‚Äôve explored how to leverage XUnit and OpenTelemetry to instrument our .NET test projects, providing a deeper insight into our test executions with the Aspire Dashboard. By integrating these tools, developers can gain valuable metrics and traces that illuminate the performance and behavior of tests in a way that traditional testing frameworks cannot match.The process of setting up the XUnit.Otel.Template demonstrates the ease with which developers can start instrumenting their tests, making it accessible even for those new to OpenTelemetry or Aspire Dashboard. The examples provided show not only how to implement basic test instrumentation but also how to enrich our tests with additional data, such as custom activities and events, to gain more detailed insights.The ability to visualize test executions and metrics on the Aspire Dashboard transforms the way we perceive and interact with our test suites. It allows us to quickly identify and address failures, understand performance bottlenecks, and improve the reliability and efficiency of our tests over time.As we continue to evolve our testing strategies, the integration of OpenTelemetry and Aspire Dashboard with XUnit represents a significant step forward in achieving more observable, reliable, and insightful test suites. This guide serves as a starting point for developers looking to enhance their testing practices with these powerful tools.References  https://github.com/NikiforovAll/xunit-instrumentation-otel-template  https://nikiforovall.github.io/dotnet/opentelemtry/2024/06/07/test-instrumentation-with-otel-aspire.html"
    },
  
    {
      "id": "40",
      "title": "Automated Tests instrumentation via OpenTelemetry and Aspire Dashboard",
      "url": "/dotnet/opentelemetry/2024/06/07/test-instrumentation-with-otel-aspire.html",
      "date": "June 07, 2024",
      "categories": ["dotnet","opentelemetry"],
      "tags": ["dotnet","aspnetcore","opentelemetry","aspire"],
      "shortinfo": "Learn how you can use OpenTelemetry to monitor your automated tests and send that data to the Aspire Dashboard for visualization.",
      "content": "TL;DRIn this post, we‚Äôll look at how you can use OpenTelemetry to monitor your automated tests and send that data to the Aspire Dashboard for visualization. The benefit of this approach is that you gain better insights into how your code works.Source code: https://github.com/NikiforovAll/tests-instrumentation-with-otel-and-aspire      TL;DR  Introduction  Running Integration Tests  Code Explained          Instrumenting Tests        Conclusion  ReferencesIntroductionIn this post, we‚Äôll look at how you can use OpenTelemetry to monitor your automated tests and send that data to the Aspire Dashboard for visualization. The benefit of this approach is that you gain better insights into how your code works.We are going to write integration tests for TodoApi application that can be defined as following:var builder = WebApplication.CreateBuilder(args);builder.AddServiceDefaults();builder.Services.AddHostedService&lt;DbInitializer&gt;();builder.AddNpgsqlDbContext&lt;TodoDbContext&gt;(\"db\");builder.Services.AddEndpointsApiExplorer();builder.Services.AddSwaggerGen();var app = builder.Build();if (app.Environment.IsDevelopment()){    app.UseSwagger();    app.UseSwaggerUI();}app.MapDefaultEndpoints();app.MapTodos();await app.RunAsync();Here is the API surface:public static RouteGroupBuilder MapTodos(this IEndpointRouteBuilder routes){    var group = routes.MapGroup(\"/todos\");    group.WithTags(\"Todos\");    group.WithParameterValidation(typeof(TodoItemViewModel));    group        .MapGet(\"/\", GetTodoItems)        .WithOpenApi();    group        .MapGet(\"/{id}\", GetTodoItem)        .WithOpenApi();    group        .MapPost(\"/\", CreateTodoItem)        .WithOpenApi();    group        .MapPut(\"/{id}\", UpdateTodoItem)        .WithOpenApi();    group        .MapDelete(\"/{id}\", DeleteTodoItem)        .WithOpenApi();    return group;}It‚Äôs a very basic CRUD application, please feel free to investigate the details in the source code.Running Integration TestsThe tests are written based on the following technologies:  XUnit - test framework  Testcontainers - test dependencies management via Docker Engine and .NET integration.  Alba - author integration tests against ASP.NET Core HTTP endpoints. Alba scenarios actually exercise the full ASP.NET Core application by running HTTP requests through your ASP.NET system in memory using the built in ASP.NET Core TestServer.I will explain how to instrument integration tests later. Right now, let‚Äôs see how it works.dotnet test --filter TodosTests --verbosity normal# Starting test execution, please wait...# A total of 1 test files matched the specified pattern.# [xUnit.net 00:00:00.00] xUnit.net VSTest Adapter v2.8.1+ce9211e970 (64-bit .NET 8.0.6)# [xUnit.net 00:00:00.17]   Discovering: Api.IntegrationTests# [xUnit.net 00:00:00.24]   Discovered:  Api.IntegrationTests# [xUnit.net 00:00:00.24]   Starting:    Api.IntegrationTests# [xUnit.net 00:00:07.51]   Finished:    Api.IntegrationTests# Test Run Successful.# Total tests: 6#      Passed: 6#  Total time: 8.3519 SecondsHere are output traces that can be found in Aspire Dashboard:    Open: http://localhost:18888/ to see the results of Test Runs.The interesting part here is a ‚ÄúWarmup‚Äù and ‚ÄúTestRun‚Äù traces. ‚ÄúWarmup‚Äù traces shows us how much time it took to setup host TestServer for TodoApi, PostgreSQL container, and Aspire Dashboard as reusable container.    As you can see, we have a lot of thing going on during automatic database migration.Let‚Äôs open ‚ÄúTestRun‚Äù traces. I‚Äôve prepared two ways you can run integration tests. By default, XUnit runs tests within the same TestCollection sequentially, but we can override this behavior by using a custom test framework.Here is sequential version:    And here is parallel version:    üí°Every test run is separated based on  service.instance.id with value assigned to test.run_id custom attribute. It is assigned to every trace to make test runs discoverable. test.run_id is generated as sequential guid to order the test runs in time. Basically, you can use Aspire Dashboard drop down for replica set to inspect every test run.    Code ExplainedAs mentioned previously, I use Alba to setup TestServer host.In the code below, we are setting up reusable WebAppFixture. It will contain everything we need to test and interact with TestSever.The other thing here we need to mention about is that we start Activity named ‚ÄúTestRun‚Äù to group sub-Activities for a test run.public class WebAppFixture : IAsyncLifetime{    public static ActivitySource ActivitySource { get; } = new(TracerName);    private const string TracerName = \"tests\";    public IAlbaHost AlbaHost { get; private set; } = default!;    public static Activity ActivityForTestRun { get; private set; } = default!;    private readonly IContainer aspireDashboard = new ContainerBuilder()        .WithImage(\"mcr.microsoft.com/dotnet/aspire-dashboard:8.0.0\")        .WithPortBinding(18888, 18888)        .WithPortBinding(18889, true)        .WithEnvironment(\"DOTNET_DASHBOARD_UNSECURED_ALLOW_ANONYMOUS\", \"true\")        .WithWaitStrategy(            Wait.ForUnixContainer().UntilHttpRequestIsSucceeded(r =&gt; r.ForPort(18888))        )        .WithReuse(true)        .WithLabel(\"aspire-dashboard\", \"aspire-dashboard-reuse-id\")        .Build();    private readonly PostgreSqlContainer db = new PostgreSqlBuilder()        .WithImage(\"postgres:16\")        .Build();    public async Task InitializeAsync()    {        await this.BootstrapAsync();        ActivityForTestRun = ActivitySource.StartActivity(\"TestRun\")!;    }    private async Task BootstrapAsync()    {        using var warmupTracerProvider = Sdk.CreateTracerProviderBuilder()            .AddSource(TracerName)            .Build();        using var activityForWarmup = ActivitySource.StartActivity(\"Warmup\")!;        await this.aspireDashboard.StartAsync();        activityForWarmup?.AddEvent(new ActivityEvent(\"AspireDashboard Started.\"));        await this.db.StartAsync();        activityForWarmup?.AddEvent(new ActivityEvent(\"PostgresSql Started.\"));        var otelExporterEndpoint =            $\"http://localhost:{this.aspireDashboard.GetMappedPublicPort(18889)}\";        using var hostActivity = ActivitySource.StartActivity(\"Start Host\")!;        this.AlbaHost = await Alba.AlbaHost.For&lt;Program&gt;(builder =&gt;        {            builder.UseEnvironment(\"Test\");            builder.UseSetting(\"OTEL_EXPORTER_OTLP_ENDPOINT\", otelExporterEndpoint);            builder.UseSetting(\"OTEL_TRACES_SAMPLER\", \"always_on\");            builder.UseSetting(\"OTEL_EXPORTER_OTLP_PROTOCOL\", \"grpc\");            builder.UseSetting(\"OTEL_DOTNET_EXPERIMENTAL_OTLP_RETRY\", \"in_memory\");            builder.UseSetting(\"OTEL_SERVICE_NAME\", \"test-host\");            builder.UseSetting(                \"Aspire:Npgsql:EntityFrameworkCore:PostgreSQL:ConnectionString\",                this.db.GetConnectionString()            );            // ordered guid to sort test runs            var testRunId = NewId.Next().ToString();            builder.ConfigureServices(services =&gt;            {                services                    .AddOpenTelemetry()                    .WithTracing(tracing =&gt;                        tracing                            .SetResourceBuilder(                                ResourceBuilder                                    .CreateDefault()                                    .AddService(TracerName, serviceInstanceId: testRunId)                            )                            .AddSource(TracerName)                            .AddProcessor(new TestRunSpanProcessor(testRunId))                    );                services.AddDbContextFactory&lt;TodoDbContext&gt;();            });        });        await this.AlbaHost.StartAsync();        activityForWarmup?.AddEvent(new ActivityEvent(\"Host Started.\"));        // wait for migration        await this            .AlbaHost.Services.GetServices&lt;IHostedService&gt;()            .FirstOrDefault(h =&gt; h.GetType() == typeof(DbInitializer))            .As&lt;DbInitializer&gt;()            .StartupTask;    }    public async Task DisposeAsync()    {        ActivityForTestRun?.Stop();        await this.AlbaHost.DisposeAsync();        await this.db.StopAsync();    }}We can define a collection of tests that will reuse same instance of WebAppFixture via Xunit.CollectionAttribute like this:[CollectionDefinition(nameof(WebAppCollection))]public sealed class WebAppCollection : ICollectionFixture&lt;WebAppFixture&gt;;[Collection(nameof(WebAppCollection))]public abstract class WebAppContext(WebAppFixture fixture){    public IAlbaHost Host { get; } = fixture.AlbaHost;}Now, we can use it by inheriting from WebAppContext:[TracePerTestRun]public class TodosTests(WebAppFixture fixture) : WebAppContext(fixture){    private static readonly Func&lt;        FluentAssertions.Equivalency.EquivalencyAssertionOptions&lt;TodoItem&gt;,        FluentAssertions.Equivalency.EquivalencyAssertionOptions&lt;TodoItem&gt;    &gt; ExcludingTodoItemFields = cfg =&gt; cfg.Excluding(p =&gt; p.Id);    [Theory, AutoData]    public async Task GetTodos_SomeTodosExist_Ok(string todoItemTitle)    {        TodoItem item = new() { Title = todoItemTitle };        await this.AddTodo(item);        var result = await this.Host.GetAsJson&lt;TodoItemViewModel[]&gt;(\"/todos\");        result.Should().NotBeNull();        result.Should().ContainEquivalentOf(item, ExcludingTodoItemFields);    }    [Fact]    public async Task PostTodos_ValidTodo_Ok()    {        var item = new TodoItem { Title = \"I want to do this thing tomorrow\" };        var result = await this.AddTodo(item);        result.Should().NotBeNull();        result!.IsComplete.Should().BeFalse();    }    [Theory, AutoData]    public async Task GetTodo_ExistingTodo_Ok(TodoItem item)    {        var dbTodo = await this.AddTodo(item);        var result = await this.Host.GetAsJson&lt;TodoItemViewModel&gt;($\"/todos/{dbTodo.Id}\");        result.Should().NotBeNull();        result.Should().BeEquivalentTo(item, ExcludingTodoItemFields);    }    [Theory, AutoData]    public async Task DeleteTodo_ExistingTodo_Ok(TodoItem item)    {        var dbTodo = await this.AddTodo(item);        await this.Host.Scenario(_ =&gt;        {            _.Delete.Url($\"/todos/{dbTodo.Id}\");            _.StatusCodeShouldBeOk();        });    }}Instrumenting TestsYou might mention special attributed called TracePerTestRun. It is responsible for OpenTelemetry test instrumentation.XUnit provides BeforeAfterTestAttribute to hookup to test execution lifecycle. We use it to start sub-Activities under parent ‚ÄúTestRun‚Äù activity.public abstract class BaseTraceTestAttribute : BeforeAfterTestAttribute { }[AttributeUsage(    AttributeTargets.Class | AttributeTargets.Method,    AllowMultiple = false,    Inherited = true)]public sealed class TracePerTestRunAttribute : BaseTraceTestAttribute{    private Activity? activityForThisTest;    public override void Before(MethodInfo methodUnderTest)    {        ArgumentNullException.ThrowIfNull(methodUnderTest);        this.activityForThisTest = WebAppFixture.ActivitySource.StartActivity(            methodUnderTest.Name,            ActivityKind.Internal,            WebAppFixture.ActivityForTestRun.Context        );        base.Before(methodUnderTest);    }    public override void After(MethodInfo methodUnderTest)    {        this.activityForThisTest?.Stop();        base.After(methodUnderTest);    }}Also, for our convenience and to group tests together, we want to add TestRunSpanProcessor. It enriches every span with test.run_id tag/attribute. It is an extension point provided by .NET OpenTelemetry instrumentation library.public class TestRunSpanProcessor(string testRunId) : BaseProcessor&lt;Activity&gt;{    private readonly string testRunId = testRunId;    public override void OnStart(Activity data) =&gt; data?.SetTag(\"test.run_id\", this.testRunId);}üôå Hooray, now you know how to instrument automated tests with OpenTelemetry and visualize the test runs via Aspire Dashboardüí°See https://www.honeycomb.io/blog/monitoring-unit-tests-opentelemetry for more details on how to instrument unit tests with OpenTelemetry. This blog post originally inspired me to write this blog post.ConclusionIn this post, we‚Äôve explored how to use OpenTelemetry to instrument our automated tests and visualize the data on the Aspire Dashboard. This approach provides valuable insights into how our code operates, helping us to identify potential bottlenecks and improve the overall performance and reliability of our applications.We‚Äôve seen how to set up integration tests for a basic CRUD application using XUnit, Testcontainers, and Alba. We‚Äôve also learned how to run these tests both sequentially and in parallel, and how to view the results in the Aspire Dashboard.By instrumenting our tests with OpenTelemetry, we can gain a deeper understanding of our code and its behavior under test conditions. This can lead to more robust, reliable, and efficient applications.Remember, the source code for this post is available on GitHub at https://github.com/NikiforovAll/tests-instrumentation-with-otel-and-aspire. Feel free to explore it further and use it as a starting point for your own test instrumentation.Happy testing! üöÄReferences  https://opentelemetry.io/docs/languages/net/instrumentation/  https://dotnet.testcontainers.org/  https://jasperfx.github.io/alba/guide/gettingstarted.html  https://www.honeycomb.io/blog/monitoring-unit-tests-opentelemetry  https://github.com/testcontainers/testcontainers-dotnet"
    },
  
    {
      "id": "41",
      "title": "Using Keycloak in .NET Aspire projects",
      "url": "/dotnet/keycloak/2024/06/02/aspire-support-for-keycloak.html",
      "date": "June 02, 2024",
      "categories": ["dotnet","keycloak"],
      "tags": ["dotnet","aspnetcore","keycloak","aspire"],
      "shortinfo": "Learn how to run Keycloak installation via Aspire",
      "content": "TL;DRYou can use Keycloak.AuthServices.Templates to add Keycloak support for .NET Aspire projects. See the docs for more details - Keycloak.AuthServices/Aspire Support.Source code: https://github.com/NikiforovAll/keycloak-aspire-starter-template  TL;DR  Introduction          Scaffold a solution      Run it        Code Explained  Conclusion  ReferencesIntroductionFrom the official docs:  .NET Aspire is designed to improve the experience of building .NET cloud-native apps. It provides a consistent, opinionated set of tools and patterns that help you build and run distributed apps.Personally, I‚Äôm a big fan of Aspire because it enables great developer experience and productivity. I recommend trying it on your own üöÄThis article will show you how to get started with Keycloak and Aspire. It is based on Keycloak.AuthServices.Templates template. Templates make it really easy to get started.üí°Here is a basic example of how the integration looks like:var builder = DistributedApplication.CreateBuilder(args);var keycloak = builder    .AddKeycloakContainer(\"keycloak\")    .WithDataVolume();var realm = keycloak.AddRealm(\"Test\");builder.AddProject&lt;Projects.Api&gt;(\"api\")    .WithReference(realm);builder.Build().Run();Scaffold a solutionInstall a templates pack:‚ùØ dotnet new install Keycloak.AuthServices.Templates# The following template packages will be installed:#    Keycloak.AuthServices.Templates::2.5.0# Success: Keycloak.AuthServices.Templates::2.5.0 installed the following templates:# Template Name            Short Name               Language  Tags# -----------------------  -----------------------  --------  -------------------------------------# Keycloak Aspire Starter  keycloak-aspire-starter  [C#]      Common/.NET Aspire/Cloud/API/Keycloak# Keycloak WebApi          keycloak-webapi          [C#]      Common/API/Keycloak‚ùØ dotnet new keycloak-aspire-starter -o $dev/keycloak-aspire-starter-template# The template \"Keycloak Aspire Starter\" was created successfully.Here is what was generated:‚ùØ tre.‚îú‚îÄ‚îÄ .gitignore‚îú‚îÄ‚îÄ Api‚îÇ   ‚îú‚îÄ‚îÄ Api.csproj‚îÇ   ‚îú‚îÄ‚îÄ Extensions.OpenApi.cs‚îÇ   ‚îú‚îÄ‚îÄ Program.cs‚îÇ   ‚îú‚îÄ‚îÄ Properties‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ launchSettings.json‚îÇ   ‚îú‚îÄ‚îÄ appsettings.Development.json‚îÇ   ‚îî‚îÄ‚îÄ appsettings.json‚îú‚îÄ‚îÄ AppHost‚îÇ   ‚îú‚îÄ‚îÄ AppHost.csproj‚îÇ   ‚îú‚îÄ‚îÄ KeycloakConfiguration‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Test-realm.json‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Test-users-0.json‚îÇ   ‚îú‚îÄ‚îÄ Program.cs‚îÇ   ‚îú‚îÄ‚îÄ Properties‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ launchSettings.json‚îÇ   ‚îú‚îÄ‚îÄ appsettings.Development.json‚îÇ   ‚îî‚îÄ‚îÄ appsettings.json‚îú‚îÄ‚îÄ Directory.Build.props‚îú‚îÄ‚îÄ Directory.Packages.props‚îú‚îÄ‚îÄ README.md‚îú‚îÄ‚îÄ ServiceDefaults‚îÇ   ‚îú‚îÄ‚îÄ Extensions.cs‚îÇ   ‚îî‚îÄ‚îÄ ServiceDefaults.csproj‚îú‚îÄ‚îÄ global.json‚îî‚îÄ‚îÄ keycloak-aspire-starter-template.slnRun it‚ùØ dotnet run --project ./AppHost/# Building...# info: Aspire.Hosting.DistributedApplication[0]#       Aspire version: 8.0.1+a6e341ebbf956bbcec0dda304109815fcbae70c9# info: Aspire.Hosting.DistributedApplication[0]#       Distributed application starting.# info: Aspire.Hosting.DistributedApplication[0]#       Application host directory is: C:\\Users\\Oleksii_Nikiforov\\dev\\keycloak-aspire-starter-template\\AppHost# info: Aspire.Hosting.DistributedApplication[0]#       Now listening on: http://localhost:15056# info: Aspire.Hosting.DistributedApplication[0]#       Distributed application started. Press Ctrl+C to shut down.Here are resources from Aspire Dashboard:    As you can see, there is a quay.io/keycloak/keycloak:24.0.3 container running. It is available on your local machine: http://localhost:8080/. Use admin:admin credentials.The template project was generated with exemplary import files. It imports Test realm, adds workspaces-client, and seeds test users:    Now, we can open Swagger UI and retrieve an access token. Note, imported realm is configured to support Implicit Flow. We can use it during the development process as demonstrated below.    To invoke the API you can use Swagger UI or other HTTP tool of your choice. Here is an example of how to use cURL:curl -X 'GET' \\  'https://localhost:51492/hello' \\  -H 'accept: text/plain' \\  -H 'Authorization: Bearer &lt;AUTH_TOKEN&gt;'# Hello World!Code ExplainedBasically, to setup Keycloak installation with Aspire we need to setup two things:  Add Keycloak Resource to Aspire AppHost.  Configure Web API to target Keycloak installationHere is how to add Keycloak as resource to Aspire:// AppHost/Program.csvar builder = DistributedApplication.CreateBuilder(args);var keycloak = builder    .AddKeycloakContainer(\"keycloak\")    .WithDataVolume()    .WithImport(\"./KeycloakConfiguration/Test-realm.json\")    .WithImport(\"./KeycloakConfiguration/Test-users-0.json\");var realm = keycloak.AddRealm(\"Test\");builder.AddProject&lt;Projects.Api&gt;(\"api\").WithReference(keycloak).WithReference(realm);builder.Build().Run();The code above does the following:  Starts a Keycloak Instance  Imports realm and test users  Reference to Keycloak adds Keycloak to service discovery  Reference to Realm adds Keycloak__Realm and Keycloak__AuthServerUrl environment variables.And here is how to configure Api to integrated with Keycloak and use workspaces-client:// Api/Program.csusing Api;using Keycloak.AuthServices.Authentication;var builder = WebApplication.CreateBuilder(args);var services = builder.Services;var configuration = builder.Configuration;builder.AddServiceDefaults();services.AddApplicationOpenApi(configuration);services.AddKeycloakWebApiAuthentication(    configuration,    options =&gt;    {        options.Audience = \"workspaces-client\";        options.RequireHttpsMetadata = false;    });services.AddAuthorization();var app = builder.Build();app.UseHttpsRedirection();app.UseApplicationOpenApi();app.UseAuthentication();app.UseAuthorization();app.MapGet(\"/hello\", () =&gt; \"Hello World!\").RequireAuthorization();app.Run();ConclusionThe integration of Keycloak with .NET Aspire projects provides a first class support for building distributed, cloud native systems. By leveraging the Keycloak.AuthServices.Templates template, developers can easily scaffold a solution and configure their APIs to work with Keycloak.üôå Keycloak.AuthServices.Templates is under development. Please, feel free to submit PRs.  üôåReferences  https://github.com/NikiforovAll/keycloak-authorization-services-dotnet  https://nikiforovall.github.io/keycloak-authorization-services-dotnet/devex/aspire.html  https://learn.microsoft.com/en-us/dotnet/aspire/"
    },
  
    {
      "id": "42",
      "title": "Claim-Check Pattern with AWS Message Processing Framework for .NET and Aspire",
      "url": "/dotnet/aws/2024/05/27/aws-claim-check-dotnet.html",
      "date": "May 27, 2024",
      "categories": ["dotnet","aws"],
      "tags": ["dotnet","aspnetcore","aws","cloud","aws-s3","aws-sns","aws-sqs","aspire"],
      "shortinfo": "Learn how to use AWS.Messaging and Aspire to implement the Claim-Check pattern",
      "content": "TL;DRLearn how to use AWS.Messaging by implementing Claim-Check pattern.  The Claim-Check pattern allows workloads to transfer payloads without storing the payload in a messaging system. The pattern stores the payload in an external data store and uses a ‚Äúclaim check‚Äù to retrieve the payload. The claim check is a unique, obscure token or key. To retrieve the payload, applications need to present the claim-check token to the external data store.Source code: https://github.com/NikiforovAll/aws-claim-check-dotnet  TL;DR  Introduction          When to use Claim-Check pattern?        What is AWS.Messaging?  Implementation          Goal      Code                  File Upload via API          File Processing via Worker          OpenTelemetry support                      Conclusion  ReferencesIntroduction  ‚òùÔ∏èThe blog post will focus on code implementation and usage of AWS.Messaging and Aspire and not on the details of the Claim-Check pattern.For more details I highly recommend seeing Azure/Architecture Center/Claim-Check patternTraditional messaging systems are optimized to manage a high volume of small messages and often have restrictions on the message size they can handle. Large messages not only risk exceeding these limits but can also degrade the performance of the entire system when the messaging system stores them.The solution to this problem is to use the Claim-Check pattern, and don‚Äôt send large messages to the messaging system. Instead, send the payload to an external data store and generate a claim-check token for that payload. The messaging system sends a message with the claim-check token to receiving applications so these applications can retrieve the payload from the data store. The messaging system never sees or stores the payload.      Payload  Save payload in data store.  Generate claim-check token and send message with claim-check token.  Receive message and read claim-check token.  Retrieve the payload.  Process the payload.When to use Claim-Check pattern?The following scenarios are use cases for the Claim-Check pattern:  Messaging system limitations: Use the Claim-Check pattern when message sizes surpass the limits of your messaging system. Offload the payload to external storage. Send only the message with its claim-check token to the messaging system.  Messaging system performance: Use the Claim-Check pattern when large messages are straining the messaging system and degrading system performance.For example, AWS SQS has a message size limit of 256 KiB. See Amazon SQS message quotas for more details.What is AWS.Messaging?The AWS Message Processing Framework for .NET is an AWS-native framework that simplifies the development of .NET message processing applications that use AWS services such as Amazon Simple Queue Service (SQS), Amazon Simple Notification Service (SNS), and Amazon EventBridge. The framework reduces the amount of boiler-plate code developers need to write, allowing you to focus on your business logic when publishing and consuming messages.The Message Processing Framework supports the following activities and features:      Sending messages to SQS and publishing events to SNS and EventBridge.    Receiving and handling messages from SQS by using a long-running poller, which is typically used in background services. This includes managing the visibility timeout while a message is being handled to prevent other clients from processing it.  Handling messages in AWS Lambda functions.  FIFO (first-in-first-out) SQS queues and SNS topics.  OpenTelemetry for logging.For a good introductory blog post see AWS Developer Tools Blog / Introducing the AWS Message Processing Framework for .NET (Preview)ImplementationGoalüéØ Assume we want to process pdf documents provided by users to extract key phrases and store these key phrases for further processing.Here‚Äôs a step-by-step explanation of the process:  File Submission: A user submits a file through an API Gateway. This could be any document that needs to be processed, such as a PDF file.  File Storage: The API Gateway forwards the file to a REST API running on an EC2 instance. The API then stores the file in an S3 bucket and sends a message to an SNS (Simple Notification Service) topic with the location of the file in S3 (this is known as a claim-check pattern).  Message Queuing: The SNS topic puts the message into an SQS (Simple Queue Service) queue for processing. The API returns a 201 response to the API Gateway, which then returns a response to the user indicating that the file was successfully submitted.  File Processing: A background service running on another EC2 instance consumes the message from the SQS queue. This service retrieves the file from S3 and sends it to Amazon Textract for parsing.  Text Extraction: Amazon Textract loads the document, extracts the text, and returns the parsed content to the background service.  Key Phrase Extraction: The background service then sends the parsed content to Amazon Comprehend to extract key phrases.  Result Storage: The key phrases are then stored back in S3 by the background service. The service acknowledges the message in the SQS queue, removing it from the queue.This workflow allows for the asynchronous processing of documents at scale. The user gets a quick response when they submit a file, and the heavy processing is done in the background, allowing the system to handle a large number of file submissions.    ‚ùóü§î Arguably, in the world of AWS, there are cloud-native alternatives to the canonical claim check pattern. For example, you can subscribe to S3 events from a lambda function, but my goal is to demonstrate how to use AWS.Messaging and Claim-Check pattern implementation, and not to provide reference solution to this problem.CodeThe solution consists of Api, Processor (Worker) components, and AWS resources defined via CloudFormation.This application is based on Aspire integration for AWS. Basically, it bootstraps the the CloudFormation stack for your application during the AppHost startup.// AppHost/Program.csusing Amazon;var builder = DistributedApplication.CreateBuilder(args);var awsConfig = builder.AddAWSSDKConfig().WithProfile(\"default\").WithRegion(RegionEndpoint.USEast1);var awsResources = builder    .AddAWSCloudFormationTemplate(\"DocumentSubmissionAppResources\", \"aws-resources.template\")    .WithReference(awsConfig);builder.AddProject&lt;Projects.Api&gt;(\"api\").WithReference(awsResources);builder.AddProject&lt;Projects.Processor&gt;(\"processor\").WithReference(awsResources);builder.Build().Run();The code above is based on couple of NuGet packages:  Aspire.Hosting.AWS - Provides extension methods and resources definition for a .NET Aspire AppHost to configure the AWS SDK for .NET and AWS application resources.  Aspire.Hosting.AppHost - Provides the core APIs and MSBuild logic for .NET Aspire AppHost projects.To glue everything together, we need to take a look at CloudFormation template - ‚Äúaws-resources.template‚Äù. The interesting part here is the Outputs section. It serves as a contract between your application and infrastructure defined through Aspire.{    \"AWSTemplateFormatVersion\": \"2010-09-09\",    \"Parameters\": {},    \"Resources\": {        // skipped content, see source code for more details.    },    \"Outputs\": {        \"DocumentQueueUrl\": {            \"Value\": {                \"Ref\": \"DocumentQueue\"            }        },        \"DocumentTopicArn\": {            \"Value\": {                \"Ref\": \"DocumentTopic\"            }        },        \"DocumentBucketName\": {            \"Value\": {                \"Ref\": \"DocumentBucket\"            }        }    }}In order to reference Outputs in our code I added next code:// ServiceDefaults/Extensions.cspublic static AwsResources AddAwsResources(this IHostApplicationBuilder builder){    var awsResources = builder.Configuration.GetSection(\"AWS:Resources\").Get&lt;AwsResources&gt;()!;    // validate, consume at runtime via IOptions if needed.    builder        .Services.AddOptions&lt;AwsResources&gt;()        .Configure(options =&gt; builder.Configuration.Bind(\"AWS:Resources\", options))        .ValidateOnStart();    return awsResources;}And the model:// ServiceDefaults/AwsResources.cspublic class AwsResources{    [Required]    [Url]    public string DocumentQueueUrl { get; set; } = default!;    [Required]    public string? DocumentTopicArn { get; set; }    [Required]    public string? DocumentBucketName { get; set; }}Now, once we have the infrastructure ready, we can take a look at the components.File Upload via APIIt is very intuitive and easy to work with AWS.Messaging, all we need is to define a publisher:// Program.csvar builder = WebApplication.CreateBuilder(args);builder.AddServiceDefaults();var awsResources = builder.AddAwsResources();builder.Services.AddAWSService&lt;IAmazonS3&gt;();builder.Services.AddAWSMessageBus(messageBuilder =&gt;{    messageBuilder.AddMessageSource(\"DocumentSubmissionApi\");    messageBuilder.AddSNSPublisher&lt;DocumentSubmission&gt;(awsResources.DocumentTopicArn);});var app = builder.Build();app.MapUploadEndpoint();app.Run();Here is how to use it:app.MapPost(    \"/upload\",    async Task&lt;Results&lt;Created, BadRequest&lt;string&gt;&gt;&gt; (        IFormFile file,        [FromServices] IOptions&lt;AwsResources&gt; resources,        [FromServices] IAmazonS3 s3Client,        [FromServices] IMessagePublisher publisher,        [FromServices] TimeProvider timeProvider,        [FromServices] ILogger&lt;Program&gt; logger    ) =&gt;    {        if (file is null or { Length: 0 })        {            return TypedResults.BadRequest(\"No file uploaded.\");        }        using var stream = file.OpenReadStream();        var bucketName = resources.Value.DocumentBucketName;        var key = Guid.NewGuid().ToString();        await s3Client.PutObjectAsync(            new PutObjectRequest            {                BucketName = bucketName,                Key = key,                InputStream = stream            }        );        var response = await publisher.PublishAsync(            new DocumentSubmission { CreatedAt = timeProvider.GetLocalNow(), Location = key }        );        logger.LogInformation(\"Published message with id {MessageId}\", response.MessageId);        return TypedResults.Created();    });File Processing via WorkerNote, in this case we need to provide an SQS Queue Url to listen to.// Program.csvar builder = Host.CreateApplicationBuilder(args);builder.AddServiceDefaults();var awsResources = builder.AddAwsResources();builder.Services.AddAWSService&lt;IAmazonTextract&gt;();builder.Services.AddAWSService&lt;IAmazonComprehend&gt;();builder.Services.AddAWSService&lt;IAmazonS3&gt;();builder.Services.AddAWSMessageBus(builder =&gt;{    builder.AddSQSPoller(awsResources.DocumentQueueUrl);    builder.AddMessageHandler&lt;DocumentSubmissionHandler, DocumentSubmission&gt;();});builder.Build().Run();Here is the handler:public class DocumentSubmissionHandler(    IAmazonTextract amazonTextractClient,    IAmazonComprehend amazonComprehendClient,    IAmazonS3 s3Client,    IOptions&lt;AwsResources&gt; resources,    ILogger&lt;DocumentSubmissionHandler&gt; logger) : IMessageHandler&lt;DocumentSubmission&gt;{    public async Task&lt;MessageProcessStatus&gt; HandleAsync(        MessageEnvelope&lt;DocumentSubmission&gt; messageEnvelope,        CancellationToken token = default    )    {        logger.LogInformation(\"Received message - {MessageId}\", messageEnvelope.Id);        var bucketName = resources.Value.DocumentBucketName;        var key = messageEnvelope.Message.Location;        var textBlocks = await this.AnalyzeDocumentAsync(bucket, key, token);        var keyPhrases = await this.DetectKeyPhrasesAsync(textBlocks, token);        await this.StorKeyPhrases(keyPhrases, bucket, key, token);        return MessageProcessStatus.Success();    }}OpenTelemetry supportThe awesome thing about Aspire and AWS.Messaging is the native OpenTelemetry support. Here is how to add AWS.Messaging instrumentation:// ServiceDefaults/Extensions.cspublic static IHostApplicationBuilder ConfigureOpenTelemetry(        this IHostApplicationBuilder builder){    builder.Logging.AddOpenTelemetry(logging =&gt;    {        logging.IncludeFormattedMessage = true;        logging.IncludeScopes = true;    });    builder        .Services.AddOpenTelemetry()        .WithMetrics(metrics =&gt;        {            metrics                .AddAspNetCoreInstrumentation()                .AddHttpClientInstrumentation()                .AddRuntimeInstrumentation();        })        .WithTracing(tracing =&gt;        {            tracing                .AddAspNetCoreInstrumentation()                .AddHttpClientInstrumentation()                .AddAWSInstrumentation() // &lt;-- add this                .AddAWSMessagingInstrumentation(); // &lt;-- and this        });    builder.AddOpenTelemetryExporters();    return builder;}The result of file upload from Aspire Dashboard:    üí° Aspire is great for investigating how distributed systems work. We can use it to deepen our understanding of Claim-Check pattern in our case.ConclusionIn conclusion, leveraging the power of AWS.Messaging coupled with Aspire can significantly streamline the process of .NET and AWS Cloud development. These tools simplify the complexities associated with development of distributed systems.References  https://github.com/awslabs/aws-dotnet-messaging  https://aws.amazon.com/blogs/developer/introducing-the-aws-message-processing-framework-for-net-preview/  https://learn.microsoft.com/en-us/azure/architecture/patterns/claim-check  https://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/msg-proc-fw.html  https://www.enterpriseintegrationpatterns.com/patterns/messaging/StoreInLibrary.html  https://docs.aws.amazon.com/prescriptive-guidance/latest/automated-pdf-analysis-solution/welcome.html"
    },
  
    {
      "id": "43",
      "title": "Mastering AWS Batch: A .NET Developer Guide to Batch File Processing",
      "url": "/dotnet/aws/2024/05/26/aws-batch-dotnet.html",
      "date": "May 26, 2024",
      "categories": ["dotnet","aws"],
      "tags": ["dotnet","aws","cloud","aws-s3","aws-batch"],
      "shortinfo": "Learn how to leverage AWS Batch to efficiently process files in S3 using .NET",
      "content": "TL;DRIn this blog post, we will explore how to leverage AWS Batch and Amazon S3 to efficiently process files using .NETSource code: https://github.com/NikiforovAll/aws-batch-dotnet  TL;DR  Introduction  Part 1: Understanding AWS Batch and S3          AWS Batch                  Components of AWS Batch                          Jobs              Job Definitions              Job Queues              Compute Environments                                          S3        Part 2: Building a .NET CLI for AWS Batch operations          Building the CLI                  Define Commands                    Creating a Docker Image        Part 3: Setting up AWS with Terraform IaC  Part 4: Running AWS Batch Jobs with CLI Commands  Conclusion  ReferencesIntroductionIn many scenarios, it is common to have a task that needs to process a large number of files. A real-world example of tasks involving processing a large number of files:  A deep learning model used for natural language processing (NLP). The model might be trained on a dataset consisting of millions of text files, each containing a piece of text, such as a book, an article, or a conversation. Each of these text files would need to be processed and fed into the model for it to learn and understand the structure of the language.  Genomics researchers often have to process massive amounts of data. For instance, they might need to analyze the genomes of thousands of individuals to identify patterns and variations. This task involves processing and analyzing a large number of files, each containing the genetic information of an individual.  A financial institution that needs to process transaction data for millions of their customers for fraud detection. Each transaction would be a separate file, and sophisticated algorithms would need to process these files to detect any irregular patterns that could indicate fraudulent activity.Part 1: Understanding AWS Batch and S3AWS Batch allows you to efficiently distribute the workload across multiple compute resources. By leveraging AWS Batch, you can easily scale your file processing tasks to handle any number of files, while taking advantage of the automatic resource provisioning and management provided by the service.AWS Batch is a powerful service that allows you to distribute computing workloads across multiple resources in the AWS Cloud. By combining it with Amazon S3, we can easily scale our file processing tasks and take advantage of automatic resource provisioning and management.With AWS Batch, you can easily parallelize the processing of your files, significantly reducing the overall processing time. This is particularly useful when dealing with large datasets or computationally intensive tasks. By distributing the workload across multiple compute resources, AWS Batch enables you to process multiple files simultaneously, maximizing the throughput of your file processing pipeline.AWS BatchAWS Batch helps you to run batch computing workloads on the AWS Cloud. Batch computing is a common way for developers, scientists, and engineers to access large amounts of compute resources. AWS Batch removes the undifferentiated heavy lifting of configuring and managing the required infrastructure, similar to traditional batch computing software. This service can efficiently provision resources in response to jobs submitted in order to, you can easily parallelize the processing of your files, which can significantly reduce the overall processing time. Additionally, AWS Batch integrates seamlessly with other AWS services, such as Amazon S3, allowing you to easily access and process your files stored in the cloud.    For more information on how to use AWS Batch you can refer to the AWS Batch documentation.    Components of AWS BatchAWS Batch simplifies running batch jobs across multiple Availability Zones within a Region. You can create AWS Batch compute environments within a new or existing VPC. After a compute environment is up and associated with a job queue, you can define job definitions that specify which Docker container images to run your jobs. Container images are stored in and pulled from container registries, which may exist within or outside of your AWS infrastructure.mindmap  root((AWS Batch))    VPC      AvailabilityZones    Compute Environment      Amazon EC2      Amazon EC2 Spot Instances      AWS Fargate      Amazon ECS      Amazon EKS    Job Queues    JobDefinitions      Memory Requirements      CPU Requirements      Container Registries - ECR    Execution      Running Batch Jobs        API/SDK        AWS CLI        Step FunctionsJobsA unit of work (such as a shell script, a Linux executable, or a Docker container image) that you submit to AWS Batch. It has a name, and runs as a containerized application on AWS Fargate, Amazon ECS container instances, Amazon EKS, or Amazon EC2 resources in your compute environment, using parameters that you specify in a job definition.When you submit a job to an AWS Batch job queue, the job enters the SUBMITTED state. It then passes through the following states until it succeeds (exits with code 0) or fails. See Job states - Documentation.When you submit a job request to AWS Batch, you have the option of defining a dependency on a previously submitted job.# Submit job Aaws batch submit-job --job-name jobA --job-queue myQueue --job-definition jdA# Output {    \"jobName\": \"example\",    \"jobId\": \"876da822-4198-45f2-a252-6cea32512ea8\"}# Submit job Baws batch submit-job --job-name jobB --job-queue myQueue --job-definition jdB --depends-on jobId=\"876da822-4198-45f2-a252-6cea32512ea8\"    Job DefinitionsA job definition is a template that describes various parameters for running a job in AWS Batch. It includes information such as the Docker image to use, the command to run, the amount of CPU and memory to allocate, and more. AWS Batch job definitions specify how jobs are to be run. While each job must reference a job definition, many of the parameters that are specified in the job definition can be overridden at runtime.Job QueuesJobs are submitted to a job queue where they reside until they can be scheduled to run in a compute environment.Compute EnvironmentsJob queues are mapped to one or more compute environments. Compute environments contain the Amazon ECS container instances that are used to run containerized batch jobs. A specific compute environment can also be mapped to one or more than one job queue. Within a job queue, the associated compute environments each have an order that‚Äôs used by the scheduler to determine where jobs that are ready to be run will run.S3Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can use Amazon S3 to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics.    For more information on how to use AWS S3 you can refer to the AWS Batch documentation.Part 2: Building a .NET CLI for AWS Batch operationsLet‚Äôs say we want to build a pipeline that analyzes files from an S3 bucket for word frequency and finds the most used words in the bucket. This pipeline can be implemented using AWS Batch and a CLI application.The process of migration consists of three stages - initialize a migration plan, run a migration for each item in the plan, and aggregate the results.USAGE:    BatchMigration.dll [OPTIONS] &lt;COMMAND&gt;EXAMPLES:    BatchMigration.dll plan --source s3://source-bucket --destination s3://destination-bucket/output --plans3://destination-bucket/plan.json    BatchMigration.dll migrate --plan s3://destination-bucket/plan.json --index 1    BatchMigration.dll merge --source s3://destination-bucket/outputOPTIONS:    -h, --help    Prints help informationCOMMANDS:    plan       Prepares migration plan for a bucket    migrate    Run a migration based on migration plan and index    merge      Merge the results results      Initialize Migration Plan:          Use the plan command to prepare a migration plan for a bucket.      Specify the source bucket using the --source option.      Specify the destination bucket for the output using the --destination option.      Use the --plan option to generate a migration plan file in JSON format, such as s3://destination-bucket/plan.json.        Run Migration:          Use the migrate command to run the migration for each item in the plan.      Specify the migration plan file using the --plan option.      Use the --index option to specify the index of the item in the plan to migrate.        Aggregate Results:          Use the merge command to merge the migration results.      Specify the source bucket for the results using the --source option.      By following these steps, you can migrate files from the source bucket to the destination bucket using AWS Batch and the provided CLI commands.Here is a relationship between jobs:Basically, we start one job to build a plan, than based on the number of files we need to process, we run N migration jobs, and process the results based:    graph LR      Plan[Plan] --&gt;|Initiate| Migrate{Migrate}      Migrate --&gt; Node1[Node 1]      Migrate --&gt; Node2[Node 2]      Migrate --&gt; Node3[Node 3]      Node1 --&gt; Merge[Merge]      Node2 --&gt; Merge      Node3 --&gt; Merge  Building the CLIWe will be using Spectre.Console to build a CLI application. This library provides a convenient way to create command-line interfaces with rich text formatting and interactive features.var services = ConfigureServices();var app = new CommandApp(new TypeRegistrar(services));app.Configure(config =&gt;{    config        .AddCommand&lt;PlanCommand&gt;(\"plan\")        .WithDescription(\"Prepares migration plan for a bucket\")        .WithExample(            \"plan\",            \"--source s3://source-bucket\",            \"--destination s3://destination-bucket/output\",            \"--plan s3://destination-bucket/plan.json\"        );    config        .AddCommand&lt;MigrateCommand&gt;(\"migrate\")        .WithDescription(\"Run a migration based on migration plan and index\")        .WithExample(\"migrate\", \"--plan s3://destination-bucket/plan.json\", \"--index 1\");    config        .AddCommand&lt;MergeCommand&gt;(\"merge\")        .WithDescription(\"Merge the results results\")        .WithExample(\"merge\", \"--source s3://destination-bucket/output\");});var result = app.Run(args);Define CommandsThe basic idea is to scan the content of S3 bucket and put the plan back to S3, by doing this we can distribute the work between jobs. As you will see later, there is a concept of array job. An array job is a job that shares common parameters, such as the job definition, vCPUs, and memory. It runs as a collection of related yet separate basic jobs that might be distributed across multiple hosts and might run concurrently. At runtime, the AWS_BATCH_JOB_ARRAY_INDEX environment variable is set to the container‚Äôs corresponding job array index number. You can use this index value to control how your array job children are differentiated. In our case, we start the subsequent migrate job based on total number of items in migration plan.üí° The examples below are abbreviated and modified for simplicity, please refer to the source code https://github.com/NikiforovAll/aws-batch-dotnet for details.Here is a plan command:public class PlanCommand(IAmazonS3 s3) : CancellableAsyncCommand&lt;PlanCommand.Settings&gt;{    private static readonly JsonSerializerOptions JsonSerializerOptions =        new() { WriteIndented = true };    public class Settings : CommandSettings    {        [CommandOption(\"-s|--source &lt;SourcePath&gt;\")]        public string Source { get; set; } = default!;        [CommandOption(\"-d|--destination &lt;DestinationPath&gt;\")]        public string Destination { get; set; } = default!;        [CommandOption(\"-p|--plan &lt;PlanPath&gt;\")]        public string Plan { get; set; } = default!;    }    public override async Task&lt;int&gt; ExecuteAsync(        CommandContext context,        Settings settings,        CancellationToken cancellation    )    {        var (source, destination, plan) = (            S3Path.Parse(settings.Source),            S3Path.Parse(settings.Destination),            S3Path.Parse(settings.Plan)        );        var files = await this.GetFilesAsync(source.Bucket, source.Key, cancellation);        var migrationPlan = new MigrationPlan(            new(source, destination, plan, files.Count),            files        );        await this.StoreMigrationPlan(migrationPlan, cancellation);        AnsiConsole.MarkupLine($\"Running scanning for {source}\");        AnsiConsole.MarkupLine($\"Result of the scan will be saved to {destination}\");        AnsiConsole.MarkupLine($\"Plan can be found here {plan}\");        return 0;    }Here is migrate command:Here is what it does:  Loads migration plan based on AWS_BATCH_JOB_ARRAY_INDEX  Get‚Äôs corresponding file to migrate based on index  Calculate word occurrences for the file  Put the result to destination bucket, the file name is copied from the source file.public class MigrateCommand(IAmazonS3 s3, IConfiguration configuration)    : CancellableAsyncCommand&lt;MigrateCommand.Settings&gt;{    public class Settings : CommandSettings    {        [CommandOption(\"-p|--plan &lt;PlanPath&gt;\")]        public string Plan { get; set; } = default!;        [CommandOption(\"-i|--index &lt;Index&gt;\")]        public int? Index { get; set; } = default!;    }    public override async Task&lt;int&gt; ExecuteAsync(        CommandContext context,        Settings settings,        CancellationToken cancellation    )    {        var plan = S3Path.Parse(settings.Plan);        var index = settings.Index ?? configuration.GetValue&lt;int&gt;(\"JOB_ARRAY_INDEX\");        var migrationPlan = await this.GetPlanAsync(plan, cancellation);        var file = migrationPlan!.Items[index];        var fileSourcePath = new S3Path(            migrationPlan.Metadata.Source.Bucket,            Path.Combine(migrationPlan.Metadata.Source.Key, file)        );        var fileDestinationPath = new S3Path(            migrationPlan.Metadata.Destination.Bucket,            Path.Combine(migrationPlan.Metadata.Destination.Key, file)        );        var sourceText = await this.GetTextAsync(fileSourcePath, cancellation);        var destinationText = CalculateWordsOccurrences(sourceText!);        var stream = new MemoryStream(Encoding.UTF8.GetBytes(destinationText));        await s3.PutObjectAsync(            new PutObjectRequest()            {                BucketName = fileDestinationPath.Bucket,                Key = fileDestinationPath.Key,                InputStream = stream            },            cancellation        );        AnsiConsole.MarkupLine($\"Plan: {plan}\");        AnsiConsole.MarkupLine($\"Migrating file([blue]{index}[/]) - {fileSourcePath}\");        AnsiConsole.MarkupLine($\"Migrating file([blue]{index}[/]) - {fileDestinationPath}\");        return 0;    }}Here is merge command:public class MergeCommand(IAmazonS3 s3, ILogger&lt;MergeCommand&gt; logger)    : CancellableAsyncCommand&lt;MergeCommand.Settings&gt;{    public class Settings : CommandSettings    {        [CommandOption(\"-s|--source &lt;SourcePath&gt;\")]        public string Source { get; set; } = default!;    }    public override async Task&lt;int&gt; ExecuteAsync(        CommandContext context,        Settings settings,        CancellationToken cancellation    )    {        ArgumentNullException.ThrowIfNull(settings.Source);        var sourcePath = S3Path.Parse(settings.Source);        var files = await this.GetFilesAsync(sourcePath.Bucket, sourcePath.Key, cancellation);        // E.g: (word1, 10), (word2: 3)        var occurrences = await this.AggregateFiles(files, cancellation);        var top = occurrences            .Where(x =&gt;x.Key is { Length: &gt; 2 })            .Where(x =&gt; x.Value &gt;= 3)            .OrderByDescending(x =&gt; x.Value)            .Take(100);        WriteTable(top);        return 0;    }}Creating a Docker ImageIn order to run our task in AWS Batch we need to push our image to Amazon Elastic Container Registry.FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build-envWORKDIR /AppCOPY . ./# Restore as distinct layersRUN dotnet restore# Build and publish a releaseRUN dotnet publish -c Release -o out# Build runtime imageFROM mcr.microsoft.com/dotnet/aspnet:8.0WORKDIR /AppCOPY --from=build-env /App/out .ENTRYPOINT [\"dotnet\", \"BatchMigration.dll\"]CMD [\"--help\"]And here is how to build it an push to the public ECR repository:docker build -t aws-batch-dotnet-demo-repository .aws ecr-public get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin public.ecr.awsdocker tag aws-batch-dotnet-demo-repository:latest public.ecr.aws/t7c5r3b7/aws-batch-dotnet-demo-repository:latestdocker push public.ecr.aws/t7c5r3b7/aws-batch-dotnet-demo-repository:latestüí° Note, you will need to create a public repository and get instructions from repository page in AWS Management Console.Part 3: Setting up AWS with Terraform IaCI‚Äôve decided to prepare a Terraform example of how to provision a full AWS Batch setup because configuring it from management console can be somewhat tedious. The code below demonstrates main parts of AWS Batch configuration, the code is redacted, once again, please consult source code for precise configuration.Below you can find a Terraform script that sets up an AWS Batch environment. Here‚Äôs a breakdown of what it does:  It specifies the required provider, in this case, AWS, and the version of the provider and configures the AWS provider with the region ‚Äúus-east-1‚Äù.  It creates two S3 buckets, one named ‚Äúaws-batch-demo-dotnet-source-bucket‚Äù and the other ‚Äúaws-batch-demo-dotnet-destination-bucket‚Äù.  It uploads all files from the local ‚Äúdocuments‚Äù directory to the source bucket.  It creates an AWS Batch environment using the ‚Äúterraform-aws-modules/batch/aws‚Äù module. This environment includes:          A compute environment named ‚Äúmain_ec2‚Äù with a type of ‚ÄúEC2‚Äù, a maximum of 8 vCPUs, and a desired number of 2 vCPUs. The instances launched by this environment will be of type ‚Äúm4.large‚Äù.      A job queue named ‚ÄúMainQueue‚Äù that is enabled and has a priority of 1. This queue uses the ‚Äúmain_ec2‚Äù compute environment.      Three job definitions named ‚Äúplan‚Äù, ‚Äúmigrate‚Äù, and ‚Äúmerge‚Äù. Each job runs a different command in a container that uses the latest image from the ‚Äúpublic_ecr‚Äù repository. Each job requires 1 vCPU and 1024 units of memory.      terraform {  required_providers {    aws = {      source  = \"hashicorp/aws\"      version = \"~&gt; 5.0\"    }  }}# Configure the AWS Providerprovider \"aws\" {  region = \"us-east-1\"}resource \"aws_s3_bucket\" \"source_bucket\" {  bucket = \"aws-batch-demo-dotnet-source-bucket\"}resource \"aws_s3_bucket\" \"destination_bucket\" {  bucket = \"aws-batch-demo-dotnet-destination-bucket\"}resource \"aws_s3_object\" \"documents\" {  for_each = fileset(\"./documents\", \"**/*\")  bucket = aws_s3_bucket.source_bucket.bucket  key    = each.value  source = \"./documents/${each.value}\"}locals {  region = \"us-east-1\"  name   = \"aws-batch-dotnet\"  tags = {    Name    = local.name    Example = local.name  }}module \"batch\" {  source = \"terraform-aws-modules/batch/aws\"  compute_environments = {    main_ec2 = {      name_prefix = \"ec2\"      compute_resources = {        type = \"EC2\"        min_vcpus      = 0        max_vcpus      = 8        desired_vcpus  = 2        instance_types = [\"m4.large\"]        security_group_ids = [module.vpc_endpoint_security_group.security_group_id]        subnets            = module.vpc.private_subnets        tags = {          # This will set the name on the Ec2 instances launched by this compute environment          Name = \"${local.name}-ec2\"          Type = \"Ec2\"        }      }    }  }  # Job queus and scheduling policies  job_queues = {    main_queue = {      name     = \"MainQueue\"      state    = \"ENABLED\"      priority = 1      compute_environments = [\"main_ec2\"]      tags = {        JobQueue = \"Job queue\"      }    }  }  job_definitions = {    plan = {      name           = \"${local.name}-plan\"      propagate_tags = true      container_properties = jsonencode({        command = [\"plan\"]        image   = \"${module.public_ecr.repository_url}:latest\"        resourceRequirements = [          { type = \"VCPU\", value = \"1\" },          { type = \"MEMORY\", value = \"1024\" }        ]      })      tags = {        JobDefinition = \"Plan\"      }    },    migrate = {      name           = \"${local.name}-migrate\"      propagate_tags = true      container_properties = jsonencode({        command = [\"migrate\"]        image   = \"${module.public_ecr.repository_url}:latest\"        resourceRequirements = [          { type = \"VCPU\", value = \"1\" },          { type = \"MEMORY\", value = \"1024\" }        ]      })      tags = {        JobDefinition = \"Migrate\"      }    },    merge = {      name           = \"${local.name}-merge\"      propagate_tags = true      container_properties = jsonencode({        command = [\"merge\"]        image   = \"${module.public_ecr.repository_url}:latest\"        resourceRequirements = [          { type = \"VCPU\", value = \"1\" },          { type = \"MEMORY\", value = \"1024\" }        ]      })      tags = {        JobDefinition = \"Merge\"      }    }  }  tags = local.tags}üí° Note, you don‚Äôt need to know terraform to try to use it. Simply, run terraform init and terraform apply to provision the environment.Part 4: Running AWS Batch Jobs with CLI Commandsaws batch submit-job \\    --job-name aws-batch-dotnet-plan-01 \\    --job-queue MainQueue  \\    --job-definition aws-batch-dotnet-plan \\    --share-identifier \"demobatch*\" \\    --scheduling-priority-override 1 \\    --container-overrides '{        \"command\": [            \"plan\",            \"--source\",            \"s3://aws-batch-demo-dotnet-source-bucket\",            \"--destination\",            \"s3://aws-batch-demo-dotnet-destination-bucket/output/\",            \"--plan\",            \"s3://aws-batch-demo-dotnet-destination-bucket/plans/plan-01.json\"        ]    }'Here is an example of produced migration plan:{  \"Metadata\": {    \"Source\": {      \"Bucket\": \"aws-batch-demo-dotnet-source-bucket\",      \"Key\": \"\"    },    \"Destination\": {      \"Bucket\": \"aws-batch-demo-dotnet-destination-bucket\",      \"Key\": \"output/\"    },    \"Plan\": {      \"Bucket\": \"aws-batch-demo-dotnet-destination-bucket\",      \"Key\": \"plans/plan-01.json\"    },    \"TotalItems\": 2  },  \"Items\": [    \"file1.txt\",    \"file2.txt\"  ]}Run the migration:aws batch submit-job \\    --job-name aws-batch-dotnet-migrate-01 \\    --job-queue MainQueue  \\    --job-definition aws-batch-dotnet-migrate \\    --share-identifier \"demobatch*\" \\    --scheduling-priority-override 1 \\    --array-properties size=2 \\    --container-overrides '{        \"command\": [            \"migrate\",            \"--plan\",            \"s3://aws-batch-demo-dotnet-destination-bucket/plans/plan-01.json\"        ]    }'üí° Note, --array-properties size=2 because we need to process two files. In this case, array job is scheduled to main queue, once processed, it will produce sub-jobs that are processed concurrently.Here is an example of file processing:and:12batch:11aws:7# the list goes onAggregate results:aws batch submit-job \\    --job-name aws-batch-dotnet-merge-01 \\    --job-queue MainQueue  \\    --job-definition aws-batch-dotnet-merge \\    --share-identifier \"demobatch*\" \\    --scheduling-priority-override 1 \\    --container-overrides '{        \"command\": [            \"merge\",            \"--source\",            \"s3://aws-batch-demo-dotnet-destination-bucket/output/\"        ]    }'And here is the that was output to console:‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ Key       ‚îÇ Value ‚îÇ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§‚îÇ batch     ‚îÇ  11   ‚îÇ‚îÇ aws       ‚îÇ   8   ‚îÇ‚îÇ service   ‚îÇ   6   ‚îÇ‚îÇ amazon    ‚îÇ   5   ‚îÇ‚îÇ storage   ‚îÇ   5   ‚îÇ‚îÇ computing ‚îÇ   4   ‚îÇ‚îÇ jobs      ‚îÇ   4   ‚îÇ‚îÇ data      ‚îÇ   4   ‚îÇ‚îÇ services  ‚îÇ   3   ‚îÇ‚îÇ simple    ‚îÇ   3   ‚îÇ‚îÇ workloads ‚îÇ   3   ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚òùÔ∏è We can define dependencies between jobs by providing --depends-on. Alternatively, we can use AWS Step Functions to orchestrate the submission of jobs.    ConclusionIn this post, we have journeyed through the process of implementing file processing pipeline. We have covered everything from creating a .NET CLI for AWS Batch operations and setting up AWS with Terraform IaC, to running AWS Batch jobs with CLI commands.Through these guides, I hope to have provided you with a comprehensive understanding and practical skills to confidently navigate AWS Batch in a .NET environment.Remember, the best way to consolidate your learning is through practice. So, don‚Äôt hesitate to apply these concepts in your projects and see the magic happen!References  https://github.com/NikiforovAll/aws-batch-dotnet  https://docs.aws.amazon.com/batch/latest/userguide/example_array_job.html  https://docs.aws.amazon.com/batch/latest/APIReference/API_SubmitJob.html  https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html  https://aws.amazon.com/blogs/hpc/encoding-workflow-dependencies-in-aws-batch/"
    },
  
    {
      "id": "44",
      "title": "Announcement - Keycloak.AuthServices v2.3.0 is out üéâ!",
      "url": "/dotnet/keycloak/2024/05/10/keycloak-v2-3-0.html",
      "date": "May 10, 2024",
      "categories": ["dotnet","keycloak"],
      "tags": ["aspnetcore","dotnet","keycloak","auth"],
      "shortinfo": "Protected Resource Builder, Improved Logging and Observability including OpenTelemetry support, and more! üôå",
      "content": "AnnouncementI‚Äôm happy to announce the release of Keycloak.AuthServices 2.3.0 packages üéâ. The release includes Protected Resource Builder, Improved Observability including OpenTelemetry support, and more! üôåCheck out the documentation to see all the cool new features and improvements: https://nikiforovall.github.io/keycloak-authorization-services-dotnet/  Join us on Discord:             Package      Version      Description                  Keycloak.AuthServices.Authentication            Keycloak Authentication for API and Web Apps              Keycloak.AuthServices.Authorization            Authorization Services.  Keycloak Authorization Server integration ‚ú®              Keycloak.AuthServices.Sdk            HTTP API integration with Admin API and Protection API              Keycloak.AuthServices.Sdk.Kiota            HTTP API integration for Admin API based on OpenAPI              Keycloak.AuthServices.OpenTelemetry            OpenTelemetry support      Changelog  üë∑‚Äç‚ôÇÔ∏è Protected Resource Builder          Minimal API      MVC        üé≠ Improved Role Claim Transformation  üÜï Added OpenTelemetry support Keycloak.AuthServices.OpenTelemetry  üóíÔ∏è Added improved docs - added ‚ÄúMaintenance‚Äù section and more!Protected Resource Builder - Minimal APIProtected Resource Builder approach provides a convenient way to authorize resources, making it easier to manage and maintain authorization rules using Authorization Server. No need to register policies!var builder = WebApplication.CreateBuilder(args);var services = builder.Services;services    .AddAuthentication(JwtBearerDefaults.AuthenticationScheme)    .AddKeycloakWebApi(builder.Configuration);services    .AddAuthorization()    .AddKeycloakAuthorization()    .AddAuthorizationServer(builder.Configuration);var app = builder.Build();app.UseAuthentication();app.UseAuthorization();app.MapGet(\"/workspaces\", () =&gt; \"Hello World!\")     .RequireProtectedResource(\"workspaces\", \"workspace:read\"); app.Run();Protected Resource Builder - MVC// Program.csvar builder = WebApplication.CreateBuilder(args);var services = builder.Services;services.AddControllers(options =&gt; options.AddProtectedResources()); services    .AddAuthentication(JwtBearerDefaults.AuthenticationScheme)    .AddKeycloakWebApi(context.Configuration);services    .AddAuthorization()    .AddKeycloakAuthorization()    .AddAuthorizationServer(context.Configuration);var app = builder.Build();app.UseHttpsRedirection();app.UseAuthorization();app.MapControllers();app.Run();Here is how to use it in Controllers:[ApiController][Route(\"workspaces\")][ProtectedResource(\"workspaces\")]public class WorkspacesController : ControllerBase{    [HttpGet]    [ProtectedResource(\"workspaces\", \"workspace:list\")]    public ActionResult&lt;IEnumerable&lt;string&gt;&gt; GetWorkspacesAsync() =&gt; this.Ok(Array.Empty&lt;string&gt;());    [HttpGet(\"public\")]    [IgnoreProtectedResource]    public IActionResult GetPublicWorkspaceAsync() =&gt; this.Ok(new { Id = \"public\" });    [HttpGet(\"{id}\")]    [ProtectedResource(\"{id}\", \"workspace:read\")]    public IActionResult GetWorkspaceAsync(string id) =&gt; this.Ok(new { id });    [HttpDelete(\"{id}\")]    [ProtectedResource(\"{id}\", \"workspace:delete\")]    public IActionResult DeleteWorkspaceAsync(string id) =&gt;        string.IsNullOrWhiteSpace(id) ? this.BadRequest() : this.NoContent();}See: https://nikiforovall.github.io/keycloak-authorization-services-dotnet/authorization/protected-resource-builder.htmlImproved Claims TransformationYou can now map both Realm Roles and Client Roles.See the docs for more details.OpenTelemetry supportvar builder = WebApplication.CreateBuilder(args);var services = builder.Services;builder.Logging.AddOpenTelemetry(logging =&gt;{    logging.IncludeFormattedMessage = true;    logging.IncludeScopes = true;});services    .AddOpenTelemetry()    .WithMetrics(metrics =&gt;        metrics            .AddAspNetCoreInstrumentation()            .AddHttpClientInstrumentation()            .AddKeycloakAuthServicesInstrumentation() // &lt;-- add this    )    .WithTracing(tracing =&gt;        tracing            .AddAspNetCoreInstrumentation()            .AddHttpClientInstrumentation()            .AddKeycloakAuthServicesInstrumentation() // &lt;-- add this    )    .UseOtlpExporter();See the docs for more details.Maintenance DocumentationIncludes common recipes and troubleshooting guidelines.See the docs for more details.FeedbackI‚Äôm excited to hear your thoughts and suggestions! Please let me know which aspects of the functionality you‚Äôd like me to explore in the next blog post. Leave your recommendations in the comments section below. Your feedback is greatly appreciated! üôè"
    },
  
    {
      "id": "45",
      "title": "Announcement - Keycloak.AuthServices v2.0.0 is out üéâ!",
      "url": "/dotnet/keycloak/2024/05/05/keycloak-v2-0-0.html",
      "date": "May 05, 2024",
      "categories": ["dotnet","keycloak"],
      "tags": ["aspnetcore","dotnet","keycloak","auth"],
      "shortinfo": "Improved Authentication, Authorization, Documentation, and many other features added. üôå",
      "content": "AnnouncementI‚Äôm happy to announce the release of Keycloak.AuthServices 2.0.0 packages üéâ. The new release contains improved Authentication, Authorization, Documentation, and many other features added. üôåCheck out the documentation to see all the cool new features and improvements: https://nikiforovall.github.io/keycloak-authorization-services-dotnet/            Package      Version      Description                  Keycloak.AuthServices.Authentication            Keycloak Authentication for API and Web Apps              Keycloak.AuthServices.Authorization            Authorization Services.  Keycloak Authorization Server integration ‚ú®              Keycloak.AuthServices.Sdk            HTTP API integration with Admin API and Protection API              Keycloak.AuthServices.Sdk.Kiota            HTTP API integration for Admin API based on OpenAPI      Changelog  üì¶ Refactored Authentication, Authorization and SDK packages  üë∑‚Äç‚ôÇÔ∏è Added Authentication Builders  üóíÔ∏è Added improved docs  üîë Improved RBAC capabilities  ‚öôÔ∏è Improved Options pattern support  üß™ Added integration tests  üÜï Added Keycloak.AuthServices.Sdk.KiotaThe full changelog can be found at: https://github.com/NikiforovAll/keycloak-authorization-services-dotnet/releases/tag/2.0.0.  Tip üí°: See Getting Started Guide to get the idea of how to use Keycloak.AuthServices.Authentication:2.0.0.I‚Äôm excited to hear your thoughts and suggestions! Please let me know which aspects of the functionality you‚Äôd like me to explore in the next blog post. Leave your recommendations in the comments section below. Your feedback is greatly appreciated! üôè"
    },
  
    {
      "id": "46",
      "title": "Job Offloading Pattern with System.Threading.Channels. A way to deal with long-running tasks in .NET",
      "url": "/dotnet/async/2024/04/21/job-offloading-pattern.html",
      "date": "April 21, 2024",
      "categories": ["dotnet","async"],
      "tags": ["dotnet","aspnetcore","pipelines","async","opentelemetry"],
      "shortinfo": "This post shows you how to implement Job Offloading Pattern using a pipeline built with System.Threading.Channels",
      "content": "Table of Contents:  TL;DR  Introduction  Defining Pipeline  Adding OpenTelemetry  Demo          Running single request      Adding custom metrics and using Bombardier      Running multiple requests        Conclusion  ReferencesTL;DRThis post shows you how to implement Job Offloading Pattern using an a pipeline built with System.Threading.Channels  Source code: https://github.com/NikiforovAll/channels-composition-and-otelIntroductionThe job offloading pattern is a technique for improving the performance and responsiveness of a system by delegating tasks to be completed outside of the main thread or process. This frees up the main thread to focus on critical tasks and keeps the user interface responsive.Consider a Web API that receives requests to perform data-intensive operations, such as generating reports from large datasets. If these operations are performed synchronously within the API request, it could lead to long response times and potentially timeout errors. This would result in a poor user experience and could also impact the scalability of the API.By using the job offloading pattern, these data-intensive operations can be offloaded to a background worker. When a request is received, the API can quickly respond with a message indicating that the operation is in progress. The actual operation is then performed in the background, without blocking the API from handling other requests.Once the operation is complete, the result can be stored in a location from where the client can retrieve it, such as a database or a cloud storage. The client can periodically check for the result or the API can provide a notification mechanism to inform the client when the operation is complete.This approach not only improves the responsiveness of the API, but also allows it to scale better by freeing up resources to handle more incoming requests while the background workers are performing the data-intensive operations.There are several benefits to using the job offloading pattern:  Improved responsiveness: By offloading long-running or resource-intensive tasks, the main thread is free to handle user interactions and keep the application responsive.  Increased scalability: Offloading jobs allows the system to distribute workload across multiple workers or machines, improving scalability as demand increases.  Enhanced reliability: If a job fails, it can be retried without impacting the main application.Here are some common ways to implement job offloading:  Background Jobs üéØ(this article): These are tasks that are submitted to a queue and processed asynchronously by a background worker. This is a popular approach for tasks like sending emails, processing images, or updating databases.  Work Queues: A work queue is a data structure that stores jobs to be processed. The worker process retrieves jobs from the queue and executes them. There are various message queueing systems available to implement work queues.  Outbox Pattern: This pattern involves storing job information in a database table before it‚Äôs submitted to a queue. This ensures data consistency and allows for retries in case of queue failures.In this article we will focus on Background Jobs because this option is suitable for many scenarios and doesn‚Äôt require additional dependencies like databases, message queues, etc.Defining PipelineAs discussed in my previous post we can make use of System.Threading.Channels to build pipelines.Assume we have a pipeline like following:graph LR    A[QueueBackgroundWorkItemAsync] --&gt; B[InitPipeline]    B --&gt; C[Step1]    C --&gt; D[Step2]    D --&gt; E[FinishPipeline]Take a look at code below, it shows some concepts like payload transformation, accessing a DI container and concurrent processing:public class ProcessorChannel : IBackgroundProcessor{    private readonly Channel&lt;Payload&gt; _pipeline;    private const int MinDelay = 500;    private const int MaxDelay = 1000;    public ProcessorChannel(        IServiceProvider serviceProvider,        IOptions&lt;ProcessorChannelSettings&gt; processorOptions    )    {        var options = processorOptions.Value;        _pipeline = Channel.CreateBounded&lt;Payload&gt;(options.Capacity);        Writer = _pipeline;        Reader = _pipeline            .Pipe(                payload =&gt;                    InitPipeline(payload, options.UseUnifiedSpanForAllPipelines, serviceProvider),                options.Capacity            )            .PipeAsync(                maxConcurrency: options.Step1MaxConcurrency,                Step1,                capacity: options.Step1Capacity            )            .PipeAsync(                maxConcurrency: options.Step2MaxConcurrency,                Step2,                capacity: options.Step2Capacity            )            .Pipe(FinishPipeline);    }    public async ValueTask QueueBackgroundWorkItemAsync(        Payload payload,        CancellationToken cancellationToken = default    )    {        ArgumentNullException.ThrowIfNull(payload);        await Writer.WriteAsync(payload, cancellationToken);    }    private static PayloadWithScope&lt;Payload&gt; InitPipeline(        Payload payload,        bool useUnifiedSpanForAllPipelines,        IServiceProvider serviceProvider    )    {        var context = new PayloadWithScope&lt;Payload&gt;(            payload,            serviceProvider.CreateScope()        );        return context;    }    private PayloadResult FinishPipeline(PayloadWithScope&lt;PayloadResult&gt; context)    {        context.ServiceScope?.Dispose();        return context.Payload;    }    private async ValueTask&lt;PayloadWithScope&lt;Payload2&gt;&gt; Step1(PayloadWithScope&lt;Payload&gt; context)    {        var timeProvider = context.GetRequiredService&lt;TimeProvider&gt;();        var delay = Random.Shared.Next(MinDelay, MaxDelay);        await Task.Delay(delay);        return new(            new(context.Payload.Name, timeProvider.GetUtcNow(), $\"Waited {delay} ms.\"),            context.ServiceScope        );    }    private async ValueTask&lt;PayloadWithScope&lt;PayloadResult&gt;&gt; Step2(        PayloadWithScope&lt;Payload2&gt; context    )    {        var payload = context.Payload;        var timeProvider = context.GetRequiredService&lt;TimeProvider&gt;();        var delay = Random.Shared.Next(MinDelay, MaxDelay);        await Task.Delay(delay);        return new(            new(payload.Name, payload.CreatedAt, payload.Message, timeProvider.GetUtcNow()),            context.ServiceScope        );    }    public ChannelWriter&lt;Payload&gt; Writer { get; }    public ChannelReader&lt;PayloadResult&gt; Reader { get; }}In order to use the pipeline, we can register IBackgroundProcessor:public interface IBackgroundProcessor{    ValueTask QueueBackgroundWorkItemAsync(        Payload payload,        CancellationToken cancellationToken = default    );}Here is how DI registration looks like:var builder = WebApplication.CreateBuilder(args);builder.Services.AddHostedService&lt;ProcessorBackgroundService&gt;();builder.Services.AddSingleton&lt;ProcessorChannel&gt;();builder.Services.AddSingleton&lt;IBackgroundProcessor, ProcessorChannel&gt;();builder.Services.AddSingleton&lt;TimeProvider&gt;(_ =&gt; TimeProvider.System);builder.Services.AddSingleton&lt;ChannelReader&lt;PayloadResult&gt;&gt;(sp =&gt;    sp.GetRequiredService&lt;ProcessorChannel&gt;().Reader);builder.Services.Configure&lt;ProcessorChannelSettings&gt;(options =&gt;{    options.Capacity = 25;    options.Step1Capacity = 10;    options.Step1MaxConcurrency = 5;    options.Step2Capacity = 10;    options.Step2MaxConcurrency = 2;});var app = builder.Build();Here is how to offload (dispatch) a task to the pipeline from an endpoint:app.MapPost(    \"/start\",    async (        [FromBody] Payload payload,        IBackgroundProcessor writer,        CancellationToken cancellationToken    ) =&gt;    {        await writer.QueueBackgroundWorkItemAsync(payload, cancellationToken);        return TypedResults.Ok(new { Success = true });    })On a receiving end, we have ProcessorBackgroundService:public class ProcessorBackgroundService(    ChannelReader&lt;PayloadResult&gt; reader,    ILogger&lt;ProcessorBackgroundService&gt; logger) : BackgroundService{    protected override async Task ExecuteAsync(CancellationToken stoppingToken)    {        await foreach (var _ in reader.ReadAllAsync(stoppingToken))        {            logger.LogProcessedByWorker();        }    }}The processing of item in the pipeline starts based on queue and ProcessorBackgroundService is only used as a receiving end (consumer) to retrieve the results.Adding OpenTelemetryOpenTelemetry is a powerful observability framework that helps you understand what happens inside your application‚Äôs pipeline. It provides a standardized way to collect, instrument, and export telemetry data, such as metrics, traces, and logs.By integrating OpenTelemetry into your application, you gain visibility into the internal workings of your code. This visibility allows you to monitor and analyze the performance, behavior, and dependencies of your application components. Here‚Äôs how OpenTelemetry helps in understanding the pipeline:      Metrics: OpenTelemetry allows you to collect and analyze various metrics, such as CPU usage, memory consumption, request latency, and error rates. These metrics provide insights into the overall health and performance of your application. By monitoring these metrics, you can identify bottlenecks, optimize resource utilization, and detect anomalies.    Tracing: OpenTelemetry enables distributed tracing, which helps you understand the flow of requests across different services and components in your application. With distributed tracing, you can track the path of a request as it traverses through various microservices, databases, and external dependencies. This helps you identify performance bottlenecks, latency issues, and problematic dependencies. Traces also provide a detailed timeline of events, allowing you to pinpoint the exact location of issues and optimize the critical path.  Logging: OpenTelemetry allows you to capture and analyze logs from your application. Logs provide valuable information about the execution flow, error messages, and other important events. By aggregating and analyzing logs, you can troubleshoot issues, detect errors, and gain insights into the behavior of your application.  Context Propagation: OpenTelemetry provides context propagation mechanisms that allow you to pass contextual information, such as request IDs and correlation IDs, across different services and components. This helps in correlating logs, traces, and metrics related to a specific request or transaction, making it easier to understand the end-to-end flow and troubleshoot issues.‚ùóüí°For this blog post I will not explain how to instrument pipelines using OpenTelemetry for the sake of simplicity, but I strongly recommend you taking a look at source code for more details: https://github.com/NikiforovAll/channels-composition-and-otel/tree/mainDemodotnet run --project ./src/AppHostRunning single requestLet‚Äôs run a single request:POST /startContent-Type: application/json{    \"name\": \"Task 1\"}Here is the trace result for an operation:The visualization shows the sequence and timing of each span in the trace, indicating the flow of the request through different components of the service. The longer bars represent spans that took more time to complete, and the gaps between bars indicate idle time or time spent waiting for a response from another service or component.  Trace ID: ff2e438d-a6d6-4264-b06d-3a2b035a79d9  Endpoint: POST /start  Duration: 1.67s  Depth: 3  Total Spans: 5Spans:  POST /start          Duration: 114.82ms        Queue item          Duration: 652us        ProcessPipeline          Duration: 1.56s        Step1          Duration: 724.35ms        Step2          Duration: 827.59ms      Adding custom metrics and using BombardierAs mentioned above, I will not show use the details of OpenTelemetry instrumentation in this blog post, but in full-implementation I‚Äôve added a couple of metrics that allows us to inspect how many requests are under processing by the pipeline:public static class ServiceMetrics{    private static Meter _meter = new Meter(\"MyService.Pipelines\");    public static Counter&lt;int&gt; StartedCounter { get; } =        _meter.CreateCounter&lt;int&gt;(\"pipelines.started\");    public static UpDownCounter&lt;int&gt; InProgressCounter { get; } =        _meter.CreateUpDownCounter&lt;int&gt;(\"pipelines.in-progress\");    public static UpDownCounter&lt;int&gt; InProgressStep1Counter { get; } =        _meter.CreateUpDownCounter&lt;int&gt;(\"pipelines.step1.in-progress\");    public static UpDownCounter&lt;int&gt; InProgressStep2Counter { get; } =        _meter.CreateUpDownCounter&lt;int&gt;(\"pipelines.step2.in-progress\");    public static Counter&lt;int&gt; FinishedCounter { get; } =        _meter.CreateCounter&lt;int&gt;(\"pipelines.finished\");}Here is how to inspect the metrics during the runtime (see the results in demo below):dotnet-counters monitor -n API --counters MyService.PipelinesBombardier is a powerful HTTP load testing tool. It‚Äôs a command-line utility that allows you to generate high volumes of HTTP traffic to test the performance and scalability of your web services. Here‚Äôs how you can use it:#!/bin/bashurl=\"http://localhost:5046/start\"requests=${1:-50}# Invoke bombardier with additional optionsbombardier -m POST -H 'Content-Type: application/json' -b '{\"name\": \"Task\"}' -n $requests $urlRunning multiple requestsHere is an example of making additional 50 simultaneous requests via Bombardier (the image below also contains the result of previous 50 requests run). Note, on the right side we have dotnet-counters launched, it allows us to see the MyService.Pipelines metrics../scripts/bombardier.sh 50Also, I‚Äôve changed the way we collect traces for the demo (using ProcessorChannelSettings.UseUnifiedSpanForAllPipelines=true parameter), the traces are collected under ProcessorBackgroundService root Span for visualization purposes. Please don‚Äôt put huge number of traces under one Span in production scenarios.In the image above, we can see how items are concurrently processed by the pipeline. It is interesting how various settings can impact the way our pipeline behaves.I recommend you to play with it on your own:public class ProcessorChannelSettings{    [Range(1, 100, ErrorMessage = \"Capacity must be at least 1\")]    public int Capacity { get; set; }    [Range(1, 100)]    public int Step1Capacity { get; set; }    [Range(1, 10)]    public int Step1MaxConcurrency { get; set; }    [Range(1, 100)]    public int Step2Capacity { get; set; }    [Range(1, 10)]    public int Step2MaxConcurrency { get; set; }    [Range(1, 10)]    public int MaxConcurrency { get; set; } = 5;    public bool UseUnifiedSpanForAllPipelines { get; set; }}Here is how configuration looks like:builder.Services.Configure&lt;ProcessorChannelSettings&gt;(options =&gt;{    options.Capacity = 25;    options.Step1Capacity = 10;    options.Step1MaxConcurrency = 5;    options.Step2Capacity = 10;    options.Step2MaxConcurrency = 2;    options.UseUnifiedSpanForAllPipelines = true;});ConclusionIn conclusion, the job offloading pattern is a crucial design pattern in modern software development. It allows for efficient distribution of tasks, leading to improved performance and scalability of applications. By offloading tasks to background services or other systems, the main application thread remains unblocked, ensuring a smooth and responsive user experience.Moreover, the importance of OpenTelemetry instrumentation cannot be overstated. It provides invaluable insights into the performance and behavior of your applications. With it, you can trace requests as they flow through various components, measure latencies, and gather other critical metrics. This data is essential for identifying bottlenecks, understanding system behavior, and making informed decisions to optimize your applications.By combining the job offloading pattern with OpenTelemetry instrumentation, you can build robust, scalable, and observable pipelines that can handle high volumes of traffic and deliver superior performance.References  https://devblogs.microsoft.com/dotnet/an-introduction-to-system-threading-channels/  https://github.com/Open-NET-Libraries/Open.ChannelExtensions  https://blog.maartenballiauw.be/post/2020/08/26/producer-consumer-pipelines-with-system-threading-channels.html  https://deniskyashif.com/2019/12/08/csharp-channels-part-1/  https://github.com/martinjt/dotnet-background-otel/tree/main  https://learn.microsoft.com/en-us/dotnet/core/diagnostics/distributed-tracing-instrumentation-walkthroughs  https://opentelemetry.io/docs/languages/net/instrumentation/  https://github.com/open-telemetry/opentelemetry-dotnet/tree/main/examples/MicroserviceExample  https://learn.microsoft.com/en-us/dotnet/core/diagnostics/metrics-instrumentation"
    },
  
    {
      "id": "47",
      "title": "Building pipelines with System.Threading.Channels",
      "url": "/dotnet/async/2024/04/21/channels-composition.html",
      "date": "April 21, 2024",
      "categories": ["dotnet","async"],
      "tags": ["dotnet","aspnetcore","async","pipelines"],
      "shortinfo": "This post shows you how to build pipelines based on System.Threading.Channels and Open.ChannelExtensions",
      "content": "Table of Contents:  TL;DR  Introduction  Building Pipeline. Pipeline Primitives          Generator aka Producer      Transformer aka Producer/Consumer      Multiplexer and Demultiplexer        Use Open.ChannelExtensions  Conclusion  ReferencesTL;DRThis post shows you how to build pipelines based on System.Threading.Channels and Open.ChannelExtensions.  Source code: https://github.com/NikiforovAll/channels-composition-and-otel/tree/main/src/ConsoleIntroductionConcurrent programming challenges can be effectively addressed using channels. Channels, as part of the System.Threading.Channels namespace, provide a powerful tool for efficient inter-component communication in your application, particularly when dealing with data streams. Channels are essentially a modern take on the classic producer-consumer problem, offering a robust, thread-safe solution for handling data flow. They are designed to be composed, allowing you to build complex, multi-stage pipelines that can process data concurrently.The relationship between concurrency and parallelism is commonly misunderstood. In fact, two procedures being concurrent doesn‚Äôt mean that they‚Äôll run in parallel. Concurrency is something that enables parallelism. On a single processor, two procedures can be concurrent, yet they won‚Äôt run in parallel. A concurrent program deals with a lot of things at once, whereas a parallel program does a lot of things at once.A channel is a data structure that allows one thread to communicate with another thread. In .NET, this was usually done by using a synchronization/locking mechanism. Channels, on the other hand, can be used to send messages directly between threads without any external synchronization or locking required.Example:The code below demonstrates the basic usage of Channel&lt;T&gt;. It prints numbers from 0 to 4 in random order.var channel = Channel.CreateUnbounded&lt;string&gt;();var consumer = Task.Run(async () =&gt;{    await foreach (var message in channel.Reader.ReadAllAsync())    {        Console.WriteLine(message);    }});var producer = Task.Run(async () =&gt;{    for (int i = 0; i &lt; 5; i++)    {        await Task.Delay(TimeSpan.FromSeconds(Random.Shared.Next(3)));        await channel.Writer.WriteAsync($\"Message {i}\");    }    channel.Writer.Complete();});await Task.WhenAll(producer, consumer);Think about this code in terms of the quote below:  Don‚Äôt communicate by sharing memory; share memory by communicating. (R. Pike)Building Pipeline. Pipeline PrimitivesIn software development, a pipeline is a sequence of steps or stages through which data or tasks flow. Each step in the pipeline performs a specific operation on the data or tasks and passes the result to the next step. Pipelines are commonly used to process data or perform a series of operations in a structured and efficient manner.While the Open.ChannelExtensions library already contains the necessary components for building concurrent pipelines, we will be building a naive implementation from scratch for learning purposes. This will allow us to gain a deeper understanding of the underlying concepts and mechanisms.Generator aka ProducerA generator is responsible for producing data that will be consumed by other parts of the pipeline. A generator is a crucial part of a pipeline based on System.Threading.Channels. It is the starting point of the pipeline, producing data that will be processed by subsequent stages.The generator method Generate&lt;T&gt; in the provided code is a simple example of a generator. It takes an array of items of type T and produces an IAsyncEnumerable&lt;T&gt; sequence from them. This sequence can then be used as the source of data for a channel.public static async IAsyncEnumerable&lt;T&gt; Generate&lt;T&gt;(params T[] array){    foreach (var item in array)    {        yield return item;    }}The Source&lt;TOut&gt; method creates a channel from an IAsyncEnumerable&lt;TOut&gt; source. It creates an unbounded channel and returns the ChannelReader part of it, effectively turning the IAsyncEnumerable into a readable channel.public static ChannelReader&lt;TOut&gt; Source&lt;TOut&gt;(IAsyncEnumerable&lt;TOut&gt; source){    var channel = Channel.CreateUnbounded&lt;TOut&gt;();    Task.Run(async () =&gt;    {        await foreach (var item in source)        {            await channel.Writer.WriteAsync(item);        }        channel.Writer.Complete();    });    return channel.Reader;}As result, we have a channel based on IAsyncEnumerable which is quite useful for many scenarios:var pipeline = Source(Generate(1, 2, 3));await foreach (var item in pipeline.ReadAllAsync()){    System.Console.WriteLine(item);}Let‚Äôs print the elements using a reusable method that traverses elements of the ChannelReader:public static async Task ForEach&lt;TRead&gt;(this ChannelReader&lt;TRead&gt; source, Action&lt;TRead&gt; action){    await foreach (var item in source.ReadAllAsync())    {        action(item);    }}So, the our simple example of pipeline looks like this:var pipeline = Source(Generate(1, 2, 3));await pipeline.ForEach(System.Console.WriteLine);And the output:123Assume we want to generate a sequence from 1 to 100, for that case I would suggest writing custom generator that simplifies working with source.The GenerateRange method is another example of a generator. It generates a sequence of integers within a specified range and produces an IAsyncEnumerable&lt;int&gt; sequence from them.public static async IAsyncEnumerable&lt;int&gt; GenerateRange(Range range){    int count = range.End.Value - range.Start.Value + 1;    foreach (var item in Enumerable.Range(range.Start.Value, count))    {        yield return item;    }}var pipeline = Source(GenerateRange(1..100));await pipeline.ForEach(System.Console.WriteLine);Transformer aka Producer/ConsumerThe pipeline can be explained as a series of consumer/producer tasks, forming a stream of steps that may be executed concurrently or sequentially.Let‚Äôs say we want to square a sequence of elements:var pipeline = Source(GenerateRange(1..10))    .CustomPipe(x =&gt; x*x);await pipeline.ForEach(System.Console.WriteLine);This code defines an extension method CustomPipe over ChannelReader&lt;TRead&gt; class. The CustomPipe method takes two type parameters, TRead and TOut. TRead is the type of data that the source channel reader reads, and TOut is the type of data that the resulting channel reader will produce. For each item read from the source, the transform function is called to transform the item to type TOut, and the transformed item is written to the new channel.public static ChannelReader&lt;TOut&gt; CustomPipe&lt;TRead, TOut&gt;(        this ChannelReader&lt;TRead&gt; source,        Func&lt;TRead, TOut&gt; transform    )    {        var channel = Channel.CreateUnbounded&lt;TOut&gt;();        Task.Run(async () =&gt;        {            await foreach (var item in source.ReadAllAsync())            {                await channel.Writer.WriteAsync(transform(item));            }            channel.Writer.Complete();        });        return channel.Reader;    }As you may have noticed, it‚Äôs not necessary for the pipeline step execution to return the same type as its result.The code snippet below creates a pipeline that generates a range of numbers, applies a custom pipe operation to calculate the square of each number, and then formats the results into a string.var pipeline = Source(GenerateRange(1..10))    .CustomPipe(x =&gt; (item: x, square: x * x))    .CustomPipe(x =&gt; $\"{x.item,2}^2 = {x.square,4}\");await pipeline.ForEach(System.Console.WriteLine);And here is the result of the execution: 1^2 =    1 2^2 =    4 3^2 =    9 4^2 =   16 5^2 =   25 6^2 =   36 7^2 =   49 8^2 =   64 9^2 =   8110^2 =  100In real-world scenarios, we often need to perform asynchronous tasks. Let‚Äôs write a modification of CustomPipe method - CustomPipeAsync that takes Func&lt;TRead, ValueTask&lt;TOut&gt; transform as parameter instead:public static ChannelReader&lt;TOut&gt; CustomPipeAsync&lt;TRead, TOut&gt;(    this ChannelReader&lt;TRead&gt; source,    Func&lt;TRead, ValueTask&lt;TOut&gt;&gt; transform){    var channel = Channel.CreateUnbounded&lt;TOut&gt;();    Task.Run(async () =&gt;    {        await foreach (var item in source.ReadAllAsync())        {            await channel.Writer.WriteAsync(await transform(item));        }        channel.Writer.Complete();    });    return channel.Reader;}var pipeline = Source(GenerateRange(1..10))    .CustomPipe(x =&gt; (item: x, square: x * x))    .CustomPipeAsync(async x =&gt;    {        await Task.Delay(x.square * 10); // some async work        return x;    })    .CustomPipe(x =&gt; $\"{x.item, 2}^2 = {x.square, 4}\");await pipeline.ForEach(System.Console.WriteLine);The output remains the same (although with some delay between items during the output process): 1^2 =    1 2^2 =    4 3^2 =    9 4^2 =   16 5^2 =   25 6^2 =   36 7^2 =   49 8^2 =   64 9^2 =   8110^2 =  100For our task it is not necessary to perform tasks one by one. So we need some way to process tasks concurrently.Multiplexer and DemultiplexerA multiplexer (or muxer) is a concept that combines multiple input signals into one output signal. In the context of our pipeline, we can think of it as a function that takes multiple ChannelReader&lt;T&gt; instances and combines them into a single ChannelReader&lt;T&gt;.On the other hand, a demultiplexer (or demuxer) is a concept that takes a single input and distributes it over several outputs. In our pipeline, it would be a function that takes a single ChannelReader&lt;T&gt; and splits it into multiple ChannelReader&lt;T&gt; instances.Here‚Äôs an example of how you might implement a multiplexer (remember naive approach üôà):static ChannelReader&lt;T&gt; Merge&lt;T&gt;(params ChannelReader&lt;T&gt;[] inputs){    var output = Channel.CreateUnbounded&lt;T&gt;();    Task.Run(async () =&gt;    {        async Task Redirect(ChannelReader&lt;T&gt; input)        {            await foreach (var item in input.ReadAllAsync())                await output.Writer.WriteAsync(item);        }        await Task.WhenAll(inputs.Select(i =&gt; Redirect(i)).ToArray());        output.Writer.Complete();    });    return output;}This code defines a method Merge that merges multiple input channels into a single output channel. Once all items have been read from all input channels and written to the output channel, the writer of the output channel is completed. This indicates that no more data will be written to the channel.And here is how you might implement a demultiplexer:static ChannelReader&lt;T&gt;[] Split&lt;T&gt;(ChannelReader&lt;T&gt; ch, int n){    var outputs = Enumerable.Range(0, n)        .Select(_ =&gt; Channel.CreateUnbounded&lt;T&gt;())        .ToArray();    Task.Run(async () =&gt;    {        var index = 0;        await foreach (var item in ch.ReadAllAsync())        {            await outputs[index].Writer.WriteAsync(item);            index = (index + 1) % n;        }        foreach (var output in outputs)            output.Writer.Complete();    });    return outputs.Select(output =&gt; output.Reader).ToArray();}This code defines a method Split that splits the data from a single input channel into multiple output channels. For each item read from the input channel, the item is written to one of the output channels. The output channel that the item is written to is determined by the index variable, which is incremented for each item and then wrapped around to zero when it reaches n. This ensures that the items are distributed evenly across the output channels.The interesting part is that we can add concurrent processing to CustomPipeAsync by introducing maxConcurrency parameter and using Split and Merge methods together like this:public static ChannelReader&lt;TOut&gt; CustomPipeAsync&lt;TRead, TOut&gt;(    this ChannelReader&lt;TRead&gt; source,    int maxConcurrency,    Func&lt;TRead, ValueTask&lt;TOut&gt;&gt; transform){    var bufferChannel = Channel.CreateUnbounded&lt;TOut&gt;();    var channel = Merge(Split(source, maxConcurrency));    Task.Run(async () =&gt;    {        await foreach (var item in channel.ReadAllAsync())        {            await bufferChannel.Writer.WriteAsync(await transform(item));        }        bufferChannel.Writer.Complete();    });    return bufferChannel.Reader;}The source channel is split into multiple channels using the Split method. The Split method divides the items from the source channel into multiple channels based on the specified maxConcurrency. This allows for concurrent processing of the items.Here is the usage of maxConcurrency parameter:var pipeline = Source(GenerateRange(1..10))    .CustomPipe(x =&gt; (item: x, square: x * x))    .CustomPipeAsync(        maxConcurrency: 2,        async x =&gt;        {            await Task.Delay(x.square * 10);            return x;        }    )    .CustomPipe(x =&gt; $\"{x.item,2}^2 = {x.square,4}\");await pipeline.ForEach(System.Console.WriteLine);And the output, note the order is no longer sequential, which is what we wanted: 1^2 =    1 3^2 =    9 5^2 =   25 7^2 =   49 9^2 =   81 2^2 =    4 4^2 =   16 6^2 =   36 8^2 =   6410^2 =  100Use Open.ChannelExtensionsLuckily, we don‚Äôt need to worry about full-fledged implementation of pipeline primitives, Open.ChannelExtensions already has everything we need for building production-ready pipelines. Let‚Äôs see how we can use it to reproduce the demo above:var pipeline = Source(GenerateRange(1..10))    .Pipe(x =&gt; (item: x, square: x * x))    .PipeAsync(        maxConcurrency: 2,        async x =&gt;        {            await Task.Delay(x.square * 10);            return x;        }    )    .Pipe(x =&gt; $\"{x.item,2}^2 = {x.square,4}\");await pipeline.ForEach(System.Console.WriteLine);Basically, Open.ChannelExtensions provides Pipe and PipeAsync methods identical to the onces we‚Äôve implemented above (but definitely more efficient).The output: 1^2 =    1 2^2 =    4 3^2 =    9 4^2 =   16 5^2 =   25 6^2 =   36 7^2 =   49 8^2 =   64 9^2 =   8110^2 =  100ConclusionIn this blog post, we‚Äôve learned a lot by building custom pipelines from scratch based on System.Threading.Channels. However, for real-world use, I recommend using Open.ChannelExtensions as it provides efficient and production-ready pipeline primitives.References  https://devblogs.microsoft.com/dotnet/an-introduction-to-system-threading-channels/  https://deniskyashif.com/2019/12/08/csharp-channels-part-1/  https://github.com/Open-NET-Libraries/Open.ChannelExtensions"
    },
  
    {
      "id": "48",
      "title": "Polymorphic serialization via System.Text.Json in ASP.NET Core Minimal API",
      "url": "/dotnet/aspnetcore/2024/04/06/openapi-polymorphism.html",
      "date": "April 06, 2024",
      "categories": ["dotnet","aspnetcore"],
      "tags": ["dotnet","aspnetcore","openapi","minimal-api"],
      "shortinfo": "In this article, we will explore the process of serializing a model hierarchy using `System.Text.Json` and how to accurately represent this serialized data in OpenAPI 3.0.",
      "content": "Table of Contents:  TL;DR  Introduction  Serialization via System.Text.Json          Use JsonDerivedTypeAttribute      Use DefaultJsonTypeInfoResolver in situations when you can‚Äôt apply attributes        Configure OpenAPI  Conclusion  ReferencesTL;DRIn this article, we will explore the process of serializing a model hierarchy using System.Text.Json and how to accurately represent this serialized data in OpenAPI 3.0.  Source code: https://github.com/NikiforovAll/openapi-polymorphismIntroductionIn the context of our demonstration, we are dealing with a composite object. The composite pattern allows us to treat individual objects and compositions of objects uniformly.This pattern is particularly useful when dealing with a hierarchy of objects where you might need to work with a single instance of an object, or a whole group of them in a similar manner.In our case, we want to serialize a composite object using System.Text.Json and represent this serialized data accurately in OpenAPI 3.0.public abstract record Component(string Name);public record Leaf(string Name) : Component(Name);public record Node(string Name, IList&lt;Component&gt;? Children = default) : Component(Name){    public IList&lt;Component&gt; Children { get; init; } = Children ?? [];    public void Add(Component component) =&gt; Children.Add(component);};For example, we would like to serialize something like following:private static Component GetNode(){    Node node = new(\"Root1\")    {        Children =        [            new Node(\"N1\")            {                Children =                [                    new Leaf(\"L1\"),                    new Leaf(\"L2\")                ]            },            new Node(\"N2\")            {                Children =                [                    new Node(\"N3\")                    {                        Children =                        [                            new Leaf(\"L3\"),                            new Leaf(\"L4\")                        ]                    }                ]            }        ]    };    return node;}Serialization via System.Text.JsonLet‚Äôs start the naive way - just use the model as it is:public static IEndpointRouteBuilder MapBasedOnAttribute(this IEndpointRouteBuilder app){    app.MapGet(\"/v{version:apiVersion}/composite\", () =&gt; GetNode() )    .WithTags(\"Composite\")    .HasApiVersion(1)    .WithOpenApi();    return app;}The results are disappointing ü•≤:{  \"children\": [    {      \"name\": \"N1\"    },    {      \"name\": \"N2\"    }  ],  \"name\": \"Root1\"}Fortunately, we can serialize hierarchies properly starting from .NET 7. See https://learn.microsoft.com/en-us/dotnet/standard/serialization/system-text-json/polymorphism for more details.Use JsonDerivedTypeAttributeAll you need to do is to apply JsonDerivedTypeAttribute to the base class of the hierarchy you want to handle.Let‚Äôs try to do exactly that:[JsonDerivedType(typeof(Node))][JsonDerivedType(typeof(Leaf))]public abstract record Component(string Name);public record Leaf(string Name) : Component(Name);public record Node(string Name, IList&lt;Component&gt;? Children = default) : Component(Name){    public IList&lt;Component&gt; Children { get; init; } = Children ?? [];    public void Add(Component component) =&gt; Children.Add(component);};Let‚Äôs see the results:{  \"children\": [    {      \"children\": [        {          \"name\": \"L1\"        },        {          \"name\": \"L2\"        }      ],      \"name\": \"N1\"    },    {      \"children\": [        {          \"children\": [            {              \"name\": \"L3\"            },            {              \"name\": \"L4\"            }          ],          \"name\": \"N3\"        }      ],      \"name\": \"N2\"    }  ],  \"name\": \"Root1\"}This is better, but there is still room for improvement. We want our clients to be able properly deserialize the hierarchy. To achieve that, we need to add a type discriminator - a special JSON property containing the exact type.Simply add typeDiscriminator:// public JsonDerivedTypeAttribute(Type derivedType, string typeDiscriminator);[JsonDerivedType(typeof(Node), typeDiscriminator: nameof(Node))][JsonDerivedType(typeof(Leaf), typeDiscriminator: nameof(Leaf))]public abstract record Component(string Name);public record Leaf(string Name) : Component(Name);public record Node(string Name, IList&lt;Component&gt;? Children = default) : Component(Name){    public IList&lt;Component&gt; Children { get; init; } = Children ?? [];    public void Add(Component component) =&gt; Children.Add(component);};Let‚Äôs see the results:{  \"$type\": \"Node\",  \"children\": [    {      \"$type\": \"Node\",      \"children\": [        {          \"$type\": \"Leaf\",          \"name\": \"L1\"        },        {          \"$type\": \"Leaf\",          \"name\": \"L2\"        }      ],      \"name\": \"N1\"    },    {      \"$type\": \"Node\",      \"children\": [        {          \"$type\": \"Node\",          \"children\": [            {              \"$type\": \"Leaf\",              \"name\": \"L3\"            },            {              \"$type\": \"Leaf\",              \"name\": \"L4\"            }          ],          \"name\": \"N3\"        }      ],      \"name\": \"N2\"    }  ],  \"name\": \"Root1\"}Awesome, now our model serialized the way I wanted. üôåUse DefaultJsonTypeInfoResolver in situations when you can‚Äôt apply attributesThe application of attributes is done during design time, which means that you specify the attributes in your code before compiling and running it. Once the attributes are applied, they become a permanent part of the program element‚Äôs definition. There are many situations when you can‚Äôt apply attributes to a model.For example, cross-assembly hierarchies, third-party dependencies, etc.From official docs:  For use cases where attribute annotations are impractical or impossible, to configure polymorphism use the contract model. The contract model is a set of APIs that can be used to configure polymorphism in a type hierarchy by creating a custom DefaultJsonTypeInfoResolver subclass that dynamically provides polymorphic configuration per type.public class PolymorphicTypeResolver : DefaultJsonTypeInfoResolver{    public override JsonTypeInfo GetTypeInfo(Type type, JsonSerializerOptions options)    {        JsonTypeInfo jsonTypeInfo = base.GetTypeInfo(type, options);        Type baseType = typeof(Component);        if (jsonTypeInfo.Type == baseType)        {            jsonTypeInfo.PolymorphismOptions = new JsonPolymorphismOptions            {                TypeDiscriminatorPropertyName = \"$type\",                IgnoreUnrecognizedTypeDiscriminators = true,                UnknownDerivedTypeHandling = JsonUnknownDerivedTypeHandling.FailSerialization,                DerivedTypes =                {                    new JsonDerivedType(typeof(Node), nameof(Node)),                    new JsonDerivedType(typeof(Leaf), nameof(Leaf)),                }            };        }        return jsonTypeInfo;    }}Here is how to add it to Minimal API:// Program.csbuilder.Services.Configure&lt;JsonOptions&gt;(options =&gt;{    options.SerializerOptions.TypeInfoResolver = new PolymorphicTypeResolver();});namespace Microsoft.AspNetCore.Http.Json{    /// &lt;summary&gt;    /// Options to configure JSON serialization settings for Microsoft.AspNetCore.Http.HttpRequestJsonExtensions    /// and Microsoft.AspNetCore.Http.HttpResponseJsonExtensions.    /// &lt;/summary&gt;    public class JsonOptions    {        public JsonOptions();        /// &lt;summary&gt;        /// Gets the System.Text.Json.JsonSerializerOptions.        /// &lt;/summary&gt;        public JsonSerializerOptions SerializerOptions { get; }    }}The output is the same:{  \"$type\": \"Node\",  \"children\": [    {      \"$type\": \"Node\",      \"children\": [        {          \"$type\": \"Leaf\",          \"name\": \"L1\"        },        {          \"$type\": \"Leaf\",          \"name\": \"L2\"        }      ],      \"name\": \"N1\"    },    {      \"$type\": \"Node\",      \"children\": [        {          \"$type\": \"Node\",          \"children\": [            {              \"$type\": \"Leaf\",              \"name\": \"L3\"            },            {              \"$type\": \"Leaf\",              \"name\": \"L4\"            }          ],          \"name\": \"N3\"        }      ],      \"name\": \"N2\"    }  ],  \"name\": \"Root1\"}Configure OpenAPIIn the modern world OpenAPI documents has become a necessity. These documents serve as a contract, allowing other systems to integrate with yours seamlessly.Let‚Äôs add additional OpenAPI metadata to the endpoint for demonstration purposes and see how the Swagger looks like:public static IEndpointRouteBuilder MapBasedOnAttribute(this IEndpointRouteBuilder app, ApiVersionSet versionSet){    app.MapGet(\"/v{version:apiVersion}/composite\", ExecuteAsync)    .WithName(\"GetCompositeForAttributeAnnotatedModels\")    .WithTags(\"Composite\")    .WithApiVersionSet(versionSet)    .HasApiVersion(1)    .WithOpenApi(operation =&gt; new(operation)    {        Summary = \"Polymorphism via JsonDerivedTypeAttribute\",        Description = \"Composite based on polymorphic serialization with attributes\",    })    .ProducesProblem(StatusCodes.Status401Unauthorized);    return app;}If we open a Swagger endpoint, we can see that the schema doesn‚Äôt contain complete information about the model. That‚Äôs a pity üôÇ// http://localhost:5077/swagger/v1/swagger.json{  \"openapi\": \"3.0.1\",  \"info\": {    \"title\": \"Composite V1\",    \"version\": \"v1\"  },  \"paths\": {    \"/v1/composite\": {      \"get\": {        \"tags\": [          \"Composite\"        ],        \"summary\": \"Polymorphism via JsonDerivedTypeAttribute\",        \"description\": \"Composite based on polymorphic serialization with attributes\",        \"operationId\": \"GetCompositeForAttributeAnnotatedModels\",        \"requestBody\": {          \"content\": {                      }        },        \"responses\": {          \"200\": {            \"description\": \"OK\",            \"content\": {              \"application/json\": {                \"schema\": {                  \"$ref\": \"#/components/schemas/Component\"                }              }            }          }        }      }    }  },  \"components\": {    \"schemas\": {      \"Component\": {        \"type\": \"object\",        \"properties\": {          \"name\": {            \"type\": \"string\",            \"nullable\": true          }        },        \"additionalProperties\": false      }    }  }}Luckily, we can fix it by configuring OpenAPI document generation as part of Swashbuckle.AspNetCore NuGet package:services.AddSwaggerGen(options =&gt;{    options.UseOneOfForPolymorphism(); // &lt;-- add this    options.UseAllOfForInheritance(); // &lt;-- add this    options.SwaggerDoc(\"v1\", new() { Title = \"Composite V1\", Version = \"v1\" });    options.SwaggerDoc(\"v2\", new() { Title = \"Composite V2\", Version = \"v2\" });    options.OperationFilter&lt;SwaggerDefaultValues&gt;();});// SwaggerGenOptionsExtensions.cs/// &lt;summary&gt;/// Enables polymorphic schema generation. If enabled, request and response schemas/// will contain the oneOf construct to describe sub types as a set of alternative/// schemas./// &lt;/summary&gt;public static void UseOneOfForPolymorphism(this SwaggerGenOptions swaggerGenOptions){    swaggerGenOptions.SchemaGeneratorOptions.UseOneOfForPolymorphism = true;}/// &lt;summary&gt;/// Enables composite schema generation. If enabled, subtype schemas will contain/// the allOf construct to incorporate properties from the base class instead of/// defining those properties inline./// &lt;/summary&gt;public static void UseAllOfForInheritance(this SwaggerGenOptions swaggerGenOptions){    swaggerGenOptions.SchemaGeneratorOptions.UseAllOfForInheritance = true;}// http://localhost:5077/swagger/v1/swagger.json{  \"openapi\": \"3.0.1\",  \"info\": {    \"title\": \"Composite V1\",    \"version\": \"v1\"  },  \"paths\": {    \"/v1/composite\": {      \"get\": {        \"tags\": [          \"Composite\"        ],        \"summary\": \"Polymorphism via JsonDerivedTypeAttribute\",        \"description\": \"Composite based on polymorphic serialization with attributes\",        \"operationId\": \"GetCompositeForAttributeAnnotatedModels\",        \"requestBody\": {          \"content\": {                      }        },        \"responses\": {          \"200\": {            \"description\": \"OK\",            \"content\": {              \"application/json\": {                \"schema\": {                  \"oneOf\": [                    {                      \"$ref\": \"#/components/schemas/Leaf\"                    },                    {                      \"$ref\": \"#/components/schemas/Node\"                    }                  ]                }              }            }          }        }      }    }  },  \"components\": {    \"schemas\": {      \"Component\": {        \"type\": \"object\",        \"properties\": {          \"name\": {            \"type\": \"string\",            \"nullable\": true          }        },        \"additionalProperties\": false      },      \"Leaf\": {        \"type\": \"object\",        \"allOf\": [          {            \"$ref\": \"#/components/schemas/Component\"          }        ],        \"additionalProperties\": false      },      \"Node\": {        \"type\": \"object\",        \"allOf\": [          {            \"$ref\": \"#/components/schemas/Component\"          }        ],        \"properties\": {          \"children\": {            \"type\": \"array\",            \"items\": {              \"oneOf\": [                {                  \"$ref\": \"#/components/schemas/Leaf\"                },                {                  \"$ref\": \"#/components/schemas/Node\"                }              ]            },            \"nullable\": true          }        },        \"additionalProperties\": false      }    }  }}ConclusionIn this article, we‚Äôve explored how to serialize a model hierarchy using System.Text.Json in ASP.NET Core Minimal API. We‚Äôve seen how to use JsonDerivedTypeAttribute and DefaultJsonTypeInfoResolver to handle polymorphic serialization, allowing us to accurately represent complex object hierarchies in JSON format. We‚Äôve also discussed how to configure OpenAPI to provide a clear and accurate representation of our serialized data.References  https://learn.microsoft.com/en-us/dotnet/standard/serialization/system-text-json/polymorphism  https://github.com/dotnet/aspnet-api-versioning  https://github.com/dotnet/aspnet-api-versioning/blob/main/examples/AspNetCore/WebApi/MinimalOpenApiExample/Program.cs  https://learn.microsoft.com/en-us/aspnet/core/fundamentals/minimal-apis/handle-errors?view=aspnetcore-8.0#problem-details  https://github.com/dotnet/aspnetcore/issues/54599"
    },
  
    {
      "id": "49",
      "title": "Transactional Outbox in .NET Cloud Native Development via Aspire",
      "url": "/dotnet/aspnetcore/aspire/2024/03/30/aspire-cap.html",
      "date": "March 30, 2024",
      "categories": ["dotnet","aspnetcore","aspire"],
      "tags": ["dotnet","aspnetcore","aspire","microservices"],
      "shortinfo": "This post provides an example of the Outbox pattern implementation using Aspire, DotNetCore.CAP, Azure Service Bus, Azure SQL, Bicep, and azd.",
      "content": "Table of Contents:  TL;DR  Introduction to Outbox Pattern  Implementation of the Outbox Pattern          Introduction to DotNetCore.CAP      Example        Adding Aspire          Provision Infrastructure      Configure for Local Development        Demo          Locally      Azure                  Outbox Tables                    Cleanup        Conclusion  ReferencesTL;DRThis post provides an example of the Outbox pattern implementation using Aspire, DotNetCore.CAP, Azure Service Bus, Azure SQL, Bicep, and azd.  Source code: https://github.com/NikiforovAll/cap-aspireIntroduction to Outbox PatternThe Outbox pattern is a crucial component in the world of distributed systems. As we move towards a more distributed and decoupled architecture in modern software development, ensuring reliable message delivery becomes increasingly important.In a distributed system, different components need to communicate with each other, often through asynchronous messaging. The Outbox pattern provides a reliable way to handle these messages. It ensures that even if a system fails after performing a local transaction but before sending out the message, the message is not lost. Instead, it is temporarily stored in an ‚Äúoutbox‚Äù and can be retrieved and sent when the system recovers.By using the Outbox pattern, we can ensure that all components of the system receive the necessary messages in a reliable manner, thereby maintaining the integrity and consistency of the entire system.In a distributed system without the Outbox pattern, there are several things that could go wrong, leading to data inconsistency or message loss. Here are a few examples:  Transaction Commit and Message Send are not Atomic: In a typical scenario, a service might first commit a transaction to its database and then send a message to a message broker. If the service crashes after the transaction commit but before the message is sent, the message will be lost. Other services will not be aware of the changes that were committed to the database.  Message Send Failure: Even if the service doesn‚Äôt crash, sending the message could fail due to a network issue or a problem with the message broker. If the message send operation is not retried, the message will be lost.  Duplicate Messages: If the service retries the message send operation after a failure, it could end up sending the same message multiple times if the first send actually succeeded but the acknowledgement was lost. This could lead to duplicate processing if the message consumers are not idempotent.  Ordering Issues: If multiple messages are sent as a result of a single transaction, and the sends are not atomic, then the messages could be received out of order. This could lead to incorrect processing if the order of the messages is important.The Outbox pattern addresses these issues by ensuring that the transaction commit and the message send operations are atomic, and by providing a mechanism for reliably sending messages even in the face of failures.Here is a sequence diagram that illustrates the problem with a system without the Outbox pattern:sequenceDiagram    participant A as Component A    participant DB as Database    participant MB as Message Broker    participant B as Component B    A-&gt;&gt;DB: Transaction    Note right of DB: Transaction is committed    DB--&gt;&gt;A: Acknowledgement    Note left of A: üí•A crashes before message is delivered    A--xMB: Message crash    Note right of MB: Message delivery fails due to A's crashIdempotent consumers play a significant role in the Outbox pattern. In the context of distributed systems, idempotency refers to the ability of a system to produce the same outcome, regardless of how many times a particular operation is performed. This is crucial in ensuring data consistency and reliability in a distributed environment.However, this could potentially lead to the same message being delivered more than once, especially in scenarios where the system fails after sending the message but before it could mark the message as sent in the outbox. This is where idempotent consumers come into play.Idempotent consumers are designed to handle duplicate messages gracefully. They ensure that the side effects of receiving the same message more than once are eliminated. This is typically achieved by keeping track of the IDs of all processed messages. When a message is received, the consumer checks if it has already processed a message with the same ID. If it has, it simply ignores the message.Here is a sequence diagram that illustrates how the Outbox pattern solves the problem:sequenceDiagram    participant A as Component A    participant DB as Database    participant OB as Outbox    participant MB as Message Broker    participant B as Component B    A-&gt;&gt;DB: Transaction    A-&gt;&gt;DB: Message    Note right of DB: Message is stored in Database    DB--&gt;&gt;A: Acknowledgement    Note right of DB: Transaction is committed    Note left of A: üí•A crashes before message is delivered    A--xMB: Message    Note right of MB: Message delivery fails due to A's crash    loop Every X time        OB--&gt;&gt;DB: Check        Note over OB: Outbox polls Database for new messages        OB--&gt;&gt;MB: Message        Note right of MB: Message is delivered to Message Broker    end    MB--&gt;&gt;B: Message    Note right of B: Message is processed by Component B    Note over B: If B has already processed a message with the same ID, it simply ignores the message (Idempotent Consumer)Implementation of the Outbox PatternNow that you understand the importance and benefits of the Outbox pattern, let‚Äôs delve into what it takes to implement:The implementation of the Outbox pattern involves the following steps:  Create an Outbox Table: The first step is to create an Outbox table in your database. This table will store all the messages that need to be sent. Each message should have a unique ID and a status field that indicates whether the message has been sent or not.  Modify the Application Code: The next step is to modify your application code. Whenever your application needs to send a message as part of a transaction, it should add the message to the Outbox table as part of the same transaction.  Implement an Outbox Publisher: The Outbox publisher is a separate component that polls the Outbox table for unsent messages. When it finds an unsent message, it sends the message and updates the status of the message in the Outbox table to ‚Äòsent‚Äô.Introduction to DotNetCore.CAPLuckily, there is a .NET library called DotNetCore.CAP that simplifies the implementation of the Outbox pattern for us.DotNetCore.CAP is an open-source library that provides a set of APIs that allow developers to easily send messages as part of a database transaction, store them in an outbox, and ensure they are reliably delivered to all interested consumers, even in the face of failures.The library also supports idempotent consumers, which are crucial for ensuring data consistency and reliability in a distributed environment. This means that even if the same message is delivered more than once, the side effects of receiving the same message are eliminated.By using DotNetCore.CAP, developers can focus on the business logic of their applications, while the library takes care of the complexities of ensuring reliable message delivery in a distributed system.ExampleThis code demonstrates how to use the CAP library in an ASP.NET Core application for event publishing and handling.In the producer:  A route handler for the ‚Äú/send‚Äù endpoint is defined.  It starts a transaction, executes a SQL command to get the current server time, and publishes a message with this time to the ‚Äútest.show.time‚Äù topic.  The message is published with a delay of 500 milliseconds.  If all operations are successful, the transaction is committed and a response is returned.// Producer/Program.csapp.MapGet(\"/send\", async (    SqlConnection connection,    ICapPublisher capPublisher,    TimeProvider timeProvider) =&gt;{    var startTime = timeProvider.GetTimestamp();    using var transaction = connection        .BeginTransaction(capPublisher, autoCommit: false);    var command = connection.CreateCommand();    command.Transaction = (SqlTransaction)transaction;    command.CommandText = \"SELECT GETDATE()\";    var serverTime = await command.ExecuteScalarAsync();    await capPublisher.PublishDelayAsync(        TimeSpan.FromMilliseconds(500),        \"test.show.time\",        new MyMessage(serverTime?.ToString()!));    transaction.Commit();    return TypedResults.Ok(new    {        Status = \"Published\",        Duration = timeProvider.GetElapsedTime(startTime)    });});üí°Note, BeginTransaction is an extensions method defined in DotNetCore.CAP.SqlServer. It responsible for Outbox table management.public static IDbTransaction BeginTransaction(    this IDbConnection dbConnection,    ICapPublisher publisher,    bool autoCommit = false)In the consumer:  A class SampleSubscriber is defined that implements ICapSubscribe.  It has a method HandleAsync that is decorated with the CapSubscribe attribute, specifying it as a handler for the ‚Äútest.show.time‚Äù topic.  When a message is received on this topic, it logs the message content after a delay of 300 milliseconds.// Consumer/Program.cspublic class SampleSubscriber(    TimeProvider timeProvider,    ILogger&lt;SampleSubscriber&gt; logger) : ICapSubscribe{    public record MyMessage(string CreatedAt);        [CapSubscribe(\"test.show.time\")]    public async Task HandleAsync(MyMessage message)    {        await Task.Delay(TimeSpan.FromMilliseconds(300), timeProvider);                logger.LogInformation(\"Message received: {CreatedAt}\", message.CreatedAt);    }}Adding AspireTo fully demonstrate the demo, we need to setup some real infrastructure components - Message Broker and Database..NET Aspire provides a curated suite of NuGet packages (Components) specifically selected to facilitate the integration of cloud-native applications. Each component furnishes essential cloud-native functionalities through either automatic provisioning or standardized configuration patterns.Add Message Broker:The .NET Aspire Service Bus component handles the following concerns to connect your app to Azure Service Bus. It adds ServiceBusClient to the DI container for connecting to Azure Service Bus.dotnet add package Aspire.Azure.Messaging.ServiceBus --prereleaseAdd Database:.NET Aspire provides two built-in configuration options to streamline SQL Server deployment on Azure:  Provision a containerized SQL Server database using Azure Container Apps  Provision an Azure SQL Database instance (We will go with this one)dotnet add package Aspire.Microsoft.Data.SqlClient --prereleaseHere is how we can setup Aspire Host based on installed components:// CapExample.AppHost/Program.csvar builder = DistributedApplication.CreateBuilder(args);var sqlServer = builder.ExecutionContext.IsPublishMode    ? builder.AddSqlServer(\"sqlserver\").PublishAsAzureSqlDatabase().AddDatabase(\"sqldb\")    : builder.AddConnectionString(\"sqldb\");var serviceBus = builder.ExecutionContext.IsPublishMode    ? builder.AddAzureServiceBus(\"serviceBus\")    : builder.AddConnectionString(\"serviceBus\");builder.AddProject&lt;Projects.Consumer&gt;(\"consumer\")    .WithReference(sqlServer)    .WithReference(serviceBus);builder.AddProject&lt;Projects.Producer&gt;(\"producer\")    .WithReference(sqlServer)    .WithReference(serviceBus);builder.Build().Run();The idea is to use connection strings during development time and provision Azure resources on publish time.Aspire allows us to develop locally and deploy to the cloud without source code changes by providing a flexible configuration system. We can use connection strings managed by Aspire Components, which can be easily switched between local development and cloud deployment environments. This allows us to seamlessly transition between different deployment scenarios without modifying our source code.Down below you can find how to configure DotNetCore.CAP based on Aspire Components:// Consumer/Program.cs// Producer/Program.csvar builder = WebApplication.CreateBuilder(args);builder.AddServiceDefaults();builder.AddAzureServiceBus(\"serviceBus\");builder.AddSqlServerClient(\"sqldb\");builder.Services.AddCap(x =&gt;{    var dbConnectionString = builder.Configuration.GetConnectionString(\"sqldb\")!;    var serviceBusConnection = builder.Configuration.GetConnectionString(\"serviceBus\")!;    x.UseAzureServiceBus(serviceBusConnection);    x.UseSqlServer(x =&gt; x.ConnectionString = dbConnectionString);});Provision InfrastructureThe Azure Developer CLI (azd) has been enhanced to support the deployment of .NET Aspire applications. The azd init workflow offers tailored support for .NET Aspire projects. I utilized this approach while developing this application and it proved to be quite nice. It improved by productivity.When azd targets a .NET Aspire application it starts the AppHost with a special command (dotnet run --project AppHost.csproj -- --publisher manifest), which produces the Aspire manifest file.The manifest file is interrogated by the azd provision sub-command logic to generate Bicep files in-memory only (by default).See https://learn.microsoft.com/en-us/dotnet/aspire/deployment/azure/aca-deployment-azd-in-depth?tabs=linux#how-azure-developer-cli-integration-works for more instructions.Personally, I find it more convenient to generate Bicep files explicitly. This can be accomplished by executing the following command:‚ùØ azd infra synthHere is visualization of what would be provisioned:To provision resources we need to run next command:‚ùØ azd provisionHere is created resource group - rg-cap-dev:Configure for Local DevelopmentTo retrieve connection string for Azure Service Bus:#!/bin/bashresourceGroup=\"rg-cap-dev\"namespace=$(    az servicebus namespace list \\        --resource-group $resourceGroup \\        --output tsv \\        --query '[0].name')azbConnectionString=$(    az servicebus namespace authorization-rule keys list \\        --namespace-name \"$namespace\" \\        --name RootManageSharedAccessKey \\        --resource-group $resourceGroup \\        --output tsv \\        --query 'primaryConnectionString')dotnet user-secrets --project ./src/CapExample.AppHost \\    set ConnectionStrings:serviceBus $azbConnectionStringTo retrieve connection string for Azure SQL Database:#!/bin/bash# read server addressif [ -f .azure/cap-dev/.env ]then    export $(cat .azure/cap-dev/.env | sed 's/#.*//g' | xargs)fi# read server passworddb_password=$(jq -r '.inputs.sql.password' .azure/cap-dev/config.json)dotnet user-secrets --project ./src/CapExample.AppHost set ConnectionStrings:sqldb \\    \"Server=$SQLSERVER_SQLSERVERFQDN;Initial Catalog=sqldb;Persist Security Info=False;User ID=CloudSA;Password=$db_password;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;\"To check the secrets were stored successfully:‚ùØ dotnet user-secrets --project ./src/CapExample.AppHost list# ConnectionStrings:sqldb = Server=sqlserver-gopucer6dsl5q.database.windows.net;Initial Catalog=sqldb;Persist Security Info=False;User ID=CloudSA;Password=&lt;your-password&gt;;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;# ConnectionStrings:serviceBus = Endpoint=sb://servicebus-gopucer6dsl5q.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=&lt;your-key&gt;DemoAs you might know, Aspire adds OpenTelemetry support out-of-the-box and you can configure it in ServiceDefaults/Extensions.cs.DotNetCore.CAP has a package that adds additional OpenTelemetry capabilities.Distributed spans in OpenTelemetry help you track operations over a message broker by providing a way to trace the flow of messages across different components of your system.When using a message broker like DotNetCore.CAP, messages are typically sent from a producer to a consumer through an intermediary broker. Each step in this process can be considered a span, which represents a unit of work or an operation.By instrumenting your code with OpenTelemetry, you can create spans that capture information about each step of the message flow. These spans can include details such as the time taken, any errors encountered, and additional contextual information.With distributed spans, you can visualize the entire journey of a message as it moves through your system, from the producer to the broker and finally to the consumer. This allows you to gain insights into the performance and behavior of your message processing pipeline.By analyzing the distributed spans, you can identify bottlenecks, latency issues, and potential errors in your message processing flow. This information is invaluable for troubleshooting and optimizing the performance of your distributed systems.Here is what you need to do to install and configure OpenTelemetry for DotNetCore.CAP:Install NuGet package:‚ùØ dotnet add ./src/CapExample.ServiceDefaults/ package DotNetCore.Cap.OpenTelemetryAdjust ServiceDefaults:builder.Services.AddOpenTelemetry()    .WithMetrics(metrics =&gt;    {        metrics            .AddAspNetCoreInstrumentation()            .AddHttpClientInstrumentation()            .AddProcessInstrumentation()            .AddRuntimeInstrumentation();    })    .WithTracing(tracing =&gt;    {        if (builder.Environment.IsDevelopment())        {            tracing.SetSampler(new AlwaysOnSampler());        }        tracing            .AddAspNetCoreInstrumentation()            .AddGrpcClientInstrumentation()            .AddHttpClientInstrumentation()            .AddCapInstrumentation(); // &lt;-- add this code    });LocallyNow, let‚Äôs run the demo:‚ùØ dotnet run --project ./src/CapExample.AppHost# Building...# Restore complete (0.6s)#   CapExample.ServiceDefaults succeeded (0.2s) ‚Üí src\\CapExample.ServiceDefaults\\bin\\Debug\\net9.0\\CapExample.ServiceDefaults.dll#   Consumer succeeded (0.3s) ‚Üí src\\Consumer\\bin\\Debug\\net9.0\\Consumer.dll#   Producer succeeded (0.5s) ‚Üí src\\Producer\\bin\\Debug\\net9.0\\Producer.dll#   CapExample.AppHost succeeded (0.2s) ‚Üí src\\CapExample.AppHost\\bin\\Debug\\net9.0\\CapExample.AppHost.dll# Build succeeded in 2.6s# info: Aspire.Hosting.DistributedApplication[0]#       Aspire version: 9.0.0-preview.2.24162.2+eaca163f7737934020f1102a9d11fdf790cccdc0# info: Aspire.Hosting.DistributedApplication[0]#       Distributed application starting.# info: Aspire.Hosting.DistributedApplication[0]#       Application host directory is: C:\\Users\\Oleksii_Nikiforov\\dev\\cap-aspire\\src\\CapExample.AppHost# info: Aspire.Hosting.DistributedApplication[0]#       Now listening on: http://localhost:15118# info: Aspire.Hosting.DistributedApplication[0]#       Distributed application started. Press Ctrl+C to shut down.Let‚Äôs generate some load:‚ùØ curl -s http://localhost:5288/send | jq# {#   \"status\": \"Published\",#   \"duration\": \"00:00:00.5255861\"# }Navigate to Aspire Dashboard to see some traces:Here is the first request, as you can see, we need some time to establish initial connection with Azure Service Bus:Subsequent requests take less time:üí°I recommend you to delve into the source code and trace examples to enhance your understanding of how the Outbox Pattern works.AzureLet‚Äôs deploy to Azure Container Apps by running azd deployDuring the initial configuration (azd init) I specified that I want a public address for the Producer. We can utilize it for dev testing:Let‚Äôs generate some load and see the metrics for Azure Service Bus.‚ùØ curl -s https://producer.mangoforest-17799c26.eastus.azurecontainerapps.io/send | jq# {#   \"status\": \"Published\",#   \"duration\": \"00:00:00.0128251\"# }Outbox TablesUnder the hood DotNetCore.CAP create two tables to manage an outbox.In the Outbox Pattern, the Published and Received tables are used to manage the messages that need to be published or have been received by a messaging system. Let‚Äôs take a closer look at the purpose of each table:Published Table: The Published table is responsible for storing the messages that need to be published to an external messaging system. When an application generates a message that needs to be sent out, it is stored in the Published table. This table acts as a buffer or a queue, ensuring that the messages are not lost if the messaging system is temporarily unavailable or if there are any failures during the publishing process. By using the Published table, the application can continue to generate messages without being blocked by the availability or performance of the messaging system. The messages in the Published table can be processed asynchronously by a separate component or background process, which takes care of publishing them to the external messaging system. Once a message is successfully published, it can be removed from the Published table.Received Table: The Received table is used to track the messages that have been received by the application from the external messaging system. When the application receives a message, it stores the necessary information about the message in the Received table. This information can include the message content, metadata, and any other relevant details. The Received table allows the application to keep a record of the messages it has processed, enabling it to handle duplicate messages.CleanupOnce you are done with the development you can delete the resource group rg-cap-dev:‚ùØ azd down# Deleting all resources and deployed code on Azure (azd down)# Local application code is not deleted when running 'azd down'.#   Resource group(s) to be deleted:#     ‚Ä¢ rg-cap-dev: https://portal.azure.com/#@/resource/subscriptions/0b252e02-9c7a-4d73-8fbc-633c5d111ebc/resourceGroups/rg-cap-dev/overview# ? Total resources to delete: 10, are you sure you want to continue? Yes# Deleting your resources can take some time.#   (‚úì) Done: Deleting resource group: rg-cap-dev# SUCCESS: Your application was removed from Azure in 10 minutes 53 seconds.ConclusionIn this post, we‚Äôve explored the Outbox Pattern, a crucial component in the world of distributed systems. We‚Äôve seen how it ensures reliable message delivery in a distributed system, maintaining the integrity and consistency of the entire system.We‚Äôve also looked at how the .NET library DotNetCore.CAP simplifies the implementation of the Outbox Pattern, allowing developers to focus on the business logic of their applications while the library takes care of the complexities of ensuring reliable message delivery.Finally, we‚Äôve seen how .NET Aspire provides a curated suite of NuGet packages that facilitate the integration of cloud-native applications, providing essential cloud-native functionalities through either automatic provisioning or standardized configuration patterns.In summary, the combination of the Outbox Pattern, DotNetCore.CAP, and .NET Aspire provides a powerful toolset for building reliable, cloud-native applications in .NET. By understanding and leveraging these tools, developers can build applications that are robust, scalable, and ready for the challenges of the modern, distributed world.References  https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/  https://cap.dotnetcore.xyz/  https://learn.microsoft.com/en-us/dotnet/aspire/deployment/azure/aca-deployment-azd-in-depth"
    },
  
    {
      "id": "50",
      "title": "A Guide to OpenAPI Client Generation with Kiota. Deep dive (Part 2)",
      "url": "/dotnet/aspnetcore/2024/03/24/kiota-guide-deep-dive.html",
      "date": "March 24, 2024",
      "categories": ["dotnet","aspnetcore"],
      "tags": ["dotnet","aspnetcore","openapi","aspire","cli"],
      "shortinfo": "This post provides a deep dive into OpenAPI client generation with Kiota, covering topics such as SDK generation, dependency injection, typed HTTP clients, cross-cutting concerns, and testing.",
      "content": "  Source code: https://github.com/NikiforovAll/kiota-getting-startedTable of Contents:  Introduction  Setup  Generate HTTP Client for ASP.NET Core applications          Generate OpenAPI specification automatically      Use OpenAPI Specification      Demo        Dependency Injection, Typed HTTP Clients, and IHttpClientFactory  Cross-Cutting Concerns and Resilience  Testing  Conclusion  ReferencesIntroductionIn the previous blog post, we learned the basics of Kiota. In this post, I want to share more details on how to apply it in production scenarios in a more sophisticated manner.I will show you the following aspects of successful SDK development:  Generation of SDKs for ASP.NET Core applications  Dependency Injection, Typed HTTP Clients, and IHttpClientFactory  Cross-Cutting Concerns  TestingSetupWe want to create an App.Client API project that calls the App trending API, which we‚Äôve built in a previous post. We will use .NET Aspire to glue everything together.Aspire is a powerful library for .NET applications that simplifies the process of service discovery, configuration, and registration. It provides a set of tools and abstractions that allow developers to easily connect their services and clients in a decoupled manner.In the context of our App.Client API project, we can use Aspire to automatically discover and register the App trending API. This means that our client application doesn‚Äôt need to know the exact location or configuration of the App API - Aspire will handle this for us.graph LR    App --&gt; NewsSearchSdk[\"NewsSearch.Sdk\"]    App --&gt; AppServiceDefaults[\"App.ServiceDefaults\"]    AppClient --&gt; AppSdk[\"App.Sdk\"]    AppClient[\"App.Client\"] --&gt; AppServiceDefaults    AppAppHost[\"App.AppHost\"] --&gt; App    AppAppHost --&gt; AppClientHere is App/Program.csvar builder = WebApplication.CreateBuilder(args);builder.AddServiceDefaults();var services = builder.Services;services.AddEndpointsApiExplorer();services.AddSwaggerGen();var app = builder.Build();app.MapDefaultEndpoints();app.MapTrendingEndpoints();app.Run();And here is App.Client/Program.csvar builder = WebApplication.CreateBuilder(args);builder.AddServiceDefaults();var services = builder.Services;services.AddEndpointsApiExplorer();services.AddSwaggerGen();var app = builder.Build();app.MapDefaultEndpoints();app.MapGet(\"/my/trending\", () =&gt;{  // TODO:});app.Run();Generate HTTP Client for ASP.NET Core applicationsASP.NET Core has built-in support for OpenAPI, also known as Swagger. This support allows developers to generate API documentation directly from their code. The quality of the generated OpenAPI specification largely depends on the amount of metadata provided by the developer in the form of attributes and comments. The more detailed and accurate this metadata is, the more precise and useful the generated OpenAPI specification will be. This, in turn, improves the quality of the client SDKs generated from the OpenAPI specification.Generate OpenAPI specification automaticallyWe can generate OpenAPI specification at build time from the code in ASP.NET Core by using Microsoft.Extensions.ApiDescription.Serverdotnet add ./src/App package Microsoft.Extensions.ApiDescription.ServerAnd add configuration to App.csproj:&lt;PropertyGroup&gt;  &lt;OpenApiDocumentsDirectory&gt;$(MSBuildProjectDirectory)/../App.Sdk/OpenApi&lt;/OpenApiDocumentsDirectory&gt;  &lt;OpenApiGenerateDocuments&gt;true&lt;/OpenApiGenerateDocuments&gt;  &lt;OpenApiGenerateDocumentsOnBuild&gt;true&lt;/OpenApiGenerateDocumentsOnBuild&gt;&lt;/PropertyGroup&gt;Note, I decided to set up the output location outside of the project - src/App.Sdk/OpenApi. We can combine the OpenAPI generation with Kiota App.Sdk client generation:Add the next Target to App.csproj. Every time we change App, the App.Sdk is regenerated. This makes the process fully automatic. Personally, I like this developer experience because I can always see the changed files in the source code.&lt;Target Name=\"OpenAPI\" AfterTargets=\"Build\" Condition=\"$(Configuration)=='Debug'\"&gt;      &lt;Exec Command=\"dotnet kiota generate -l CSharp --output ../App.Sdk --namespace-name App.Sdk --class-name AppApiClient --exclude-backward-compatible --openapi ../App.Sdk/OpenApi/App.json\" WorkingDirectory=\"$(ProjectDir)\" /&gt;&lt;/Target&gt;Use OpenAPI SpecificationThe AppApiClient class is a central part of the generated SDK. It provides methods for making HTTP requests to the API endpoints defined in our application. In the example below, we are using the AppApiClient to make a GET request to the /trending/{country} endpoint.The AppApiClient takes an IRequestAdapter as a parameter in its constructor. This adapter is responsible for sending HTTP requests and receiving HTTP responses. In this example, we are using the HttpClientRequestAdapter.We also provide an IAuthenticationProvider to the HttpClientRequestAdapter. This provider is responsible for providing the necessary authentication credentials for the API requests. In this example, we are using the AnonymousAuthenticationProvider, which does not provide any authentication credentials.Finally, we set the BaseUrl of the HttpClientRequestAdapter. This URL is used as the base for all API requests made by the AppApiClient.// App.Client/Program.csapp.MapGet(\"/my/trending\", async () =&gt;{    var authProvider = new AnonymousAuthenticationProvider();    var requestAdapter = new HttpClientRequestAdapter(authProvider, httpClient: httpClient)    {        BaseUrl = \"http://app\"    };    var client = new AppApiClient(requestAdapter);    var response = await client.Trending[\"US\"].GetAsync();    return response.Value.Select(topic =&gt; topic.Query.Text);});üí°Note, the BaseUrl is based on Aspire convention. Here is App.AppHost:// App.AppHost/Program.csvar builder = DistributedApplication.CreateBuilder(args);var appProject = builder.AddProject&lt;Projects.App&gt;(\"app\");builder.AddProject&lt;Projects.App_Client&gt;(\"app-client\")    .WithReference(appProject);builder.Build().Run();Demodotnet run --project ./src/App.AppHostcurl -s http://localhost:5102/my/trending | jqgraph LR    User(fa:fa-user) --&gt; AppClient[\"App.Client\"]    AppClient --&gt; AppSdk[\"App.Sdk\"]    AppSdk --&gt; App    App --&gt; NewsSearchSdk[\"NewsSearch.Sdk\"]    NewsSearchSdk --&gt; BingApi[\"Bing Search API\"]    style AppSdk fill:#add8e6    style NewsSearchSdk fill:#add8e6    style BingApi fill:#ffff99And here is trace example from Aspire.Dashboard:üí°Note, Http instrumentation/tracing works only for clients resolved through DI container. Later, I will show you how to properly add AppApiClient so it uses HttpClient from IHttpClientFactory.Dependency Injection, Typed HTTP Clients, and IHttpClientFactoryAs you may already know, IHttpClientFactory in .NET provides a better way to work with HttpClient, as it addresses well-known client lifetime issues. In this guide, I will demonstrate how to use a client generated by Kiota as a typed client. For more information, refer to the Typed-client approach.// App.Client/Program.csservices.AddSingleton&lt;IAuthenticationProvider, AnonymousAuthenticationProvider&gt;(    _ =&gt; new AnonymousAuthenticationProvider());services.AddHttpClient&lt;AppApiClient&gt;()    .AddTypedClient((httpClient, sp) =&gt;    {        var authenticationProvider = sp.GetRequiredService&lt;IAuthenticationProvider&gt;();        var requestAdapter = new HttpClientRequestAdapter(authProvider , httpClient: httpClient)        {            BaseUrl = \"http://app\"        };        return new AppApiClient(requestAdapter);    })    .ConfigurePrimaryHttpMessageHandler(_ =&gt;    {        var defaultHandlers = KiotaClientFactory.CreateDefaultHandlers();        var defaultHttpMessageHandler = KiotaClientFactory.GetDefaultHttpMessageHandler();        return KiotaClientFactory.ChainHandlersCollectionAndGetFirstLink(            defaultHttpMessageHandler, [.. defaultHandlers])!;    });The code above is configuring an HttpClient for the AppApiClient in a .NET application. The AddTypedClient method is used to further configure the HttpClient instance. The advantage of using typed clients is that they provide a clear contract for HTTP interactions and can be easily mocked for testing.By default, Kiota provides the default list of DelegatingHandlers and HttpMessageHandler. It is good idea to include them, but you can definitely opt-out if it interferes with your code.public static IList&lt;DelegatingHandler&gt; CreateDefaultHandlers(){    return new List&lt;DelegatingHandler&gt;    {        new RetryHandler(),        new RedirectHandler(),        new ParametersNameDecodingHandler(),        new UserAgentHandler(),        new HeadersInspectionHandler()    };}The ConfigurePrimaryHttpMessageHandler method is used to set up the primary HttpMessageHandler for the HTTP client. This handler is responsible for sending HTTP requests and receiving HTTP responses.public static IHttpClientBuilder ConfigurePrimaryHttpMessageHandler(    this IHttpClientBuilder builder,    Func&lt;IServiceProvider, HttpMessageHandler&gt; configureHandler);Here‚Äôs how it works:      Create a list of default DelegatingHandler instances using KiotaClientFactory.CreateDefaultHandlers(). A DelegatingHandler is a special type of HttpMessageHandler that can be used to process or manipulate HTTP requests and responses in some way before they are sent or after they are received.        Get the default HttpMessageHandler using KiotaClientFactory.GetDefaultHttpMessageHandler(). This handler is the one that will actually send the HTTP request and receive the response.        Chain these handlers together using KiotaClientFactory.ChainHandlersCollectionAndGetFirstLink(). This method takes the default HttpMessageHandler and the list of DelegatingHandler instances, and chains them together so that each request or response will pass through each handler in turn. The method returns the first link in this chain, which is then used as the primary HttpMessageHandler for the HTTP client.  Finally, here is how to use AppApiClient from DI:// App.Client/Program.csapp.MapGet(\"/my/trending\", async (AppApiClient client) =&gt;{    var response = await client.Trending[\"US\"].GetAsync();    return response.Value.Select(topic =&gt; topic.Query.Text);});Cross-Cutting Concerns and ResilienceIn distributed applications, communication between services is a critical aspect. However, this communication is not always reliable. Network issues, high latency, or the unavailability of a service can lead to failures. This is where resilience comes into play.Polly is a .NET resilience and transient-fault-handling library that allows developers to express policies such as Retry, Circuit Breaker, Timeout, Bulkhead Isolation, and Fallback in a fluent and thread-safe manner. It is a crucial tool for building reliable applications that can withstand the unpredictable nature of the network..NET 8, .NET team has made substantial advancements to simplify the integration of resilience into your applications - meet new resilience packages:# Extensions to the Polly libraries to enrich telemetry with metadata and exception summariesdotnet add package Microsoft.Extensions.Resilience# Resilience mechanisms for HttpClient built on the Polly frameworkdotnet add package Microsoft.Extensions.Http.ResilienceFor an out-of-the-box experience, use the AddStandardResilienceHandler extension on the IHttpClientBuilder like this:IHttpStandardResiliencePipelineBuilder resilienceBuilder = services    .AddHttpClient(\"my-client\")    .AddStandardResilienceHandler(options =&gt;    {        // Configure standard resilience options here    });IHttpStandardResiliencePipelineBuilder allows to configure underlying multiple resilience strategies with options to send the requests and handle any transient errors.In the context of our example, it‚Äôs worth noting that it already has the StandardResilienceHandler built-in. This is due to the fact that Aspire has opinionated defaults on how to build distributed applications. This means that it comes with a set of pre-configured settings that are designed to handle common scenarios in a distributed environment.The StandardResilienceHandler is a part of these defaults. It is a resilience strategy that includes a combination of retry, circuit breaker, and timeout policies. These policies are designed to handle transient faults in a graceful manner, ensuring that your application remains responsive and reliable in the face of network issues, high latency, or service unavailability.The StandardResilienceHandler is automatically applied to all HTTP clients that are created through the IHttpClientFactory. This means that you don‚Äôt have to manually configure these resilience policies for each client. Instead, they are applied consistently across your application, ensuring that all HTTP communication is resilient.Here is partial content from App.ServiceDefaults/Extensions.cs:public static IHostApplicationBuilder AddServiceDefaults(this IHostApplicationBuilder builder){    builder.ConfigureOpenTelemetry();    builder.AddDefaultHealthChecks();    builder.Services.AddServiceDiscovery();    builder.Services.ConfigureHttpClientDefaults(http =&gt;    {        // Turn on resilience by default        http.AddStandardResilienceHandler();        http.UseServiceDiscovery();    });    return builder;}Here is an example of how to use the AddStandardResilienceHandler method on top of IHttpClientBuilder returned by AddTypedClient method for fine-grained control and customization of resiliency per-client:services.AddHttpClient&lt;AppApiClient&gt;().AddTypedClient()    .AddStandardResilienceHandler().Configure(cfg =&gt;    {        cfg.Retry.MaxRetryAttempts = 3;        cfg.Retry.UseJitter = true;        cfg.Retry.BackoffType = Polly.DelayBackoffType.Exponential;    });üí°Note, Microsoft.Extensions.Http.Resilience allows to build custom pipelines and gives you full control over how to manage resiliency in your applications.TestingFor unit testing, it‚Äôs suggested to use mock versions of the HTTP transport layer to manage API responses. In Kiota API clients, this layer is in a request adapter. By mocking the request adapter, you can control the API responses.public class TrendingTopicTests{    [Fact]    public async Task TrendingTopic_GetUS_SuccessAsync()    {        // Arrange        var adapter = Substitute.For&lt;IRequestAdapter&gt;();        adapter.SetupSendAsyncWithResponse(new TrendingTopics() { Value = [] });        var newsSearchApiClient = new NewsSearchApiClient(adapter);        // Act        var response = await newsSearchApiClient            .News            .Trendingtopics            .GetAsync(r =&gt; r.QueryParameters.Cc = \"US\");        // Assert        Assert.NotNull(response);    }}public static class Utils{    public static void SetupSendAsyncWithResponse&lt;T&gt;(        this IRequestAdapter adapter, T response) where T : IParsable    {        adapter.SendAsync&lt;T&gt;(            Arg.Any&lt;RequestInformation&gt;(),            Arg.Any&lt;ParsableFactory&lt;T&gt;&gt;(),            Arg.Any&lt;Dictionary&lt;string, ParsableFactory&lt;IParsable&gt;&gt;&gt;(),            Arg.Any&lt;CancellationToken&gt;())            .ReturnsForAnyArgs(response);    }}üí°The process of mocking IRequestAdapter can be somewhat complex, particularly as it requires reliance on models generated by Kiota. To streamline testing, I recommend encapsulating the use of generated clients within a simple interface and then mocking this interface. This approach not only simplifies testing but also aligns well with the principles of Clean Architecture. By doing so, we avoid the need to mock the Kiota code directly, enhancing the maintainability and readability of our tests.ConclusionIn conclusion, Kiota is not just a powerful tool, but a practical solution for modern development challenges. It‚Äôs ready to be integrated into your production code. It‚Äôs time to embrace Kiota and let it transform your development workflow.References  https://nikiforovall.github.io/dotnet/aspnetcore/2024/03/22/kiota-guide-introduction.html  https://www.meziantou.net/generate-openapi-specification-at-build-time-from-the-code-in-asp-net-core.htm  https://devblogs.microsoft.com/dotnet/introducing-dotnet-aspire-simplifying-cloud-native-development-with-dotnet-8/  https://github.com/microsoft/kiota-http-dotnet/blob/main/src/KiotaClientFactory.cs  https://devblogs.microsoft.com/dotnet/building-resilient-cloud-services-with-dotnet-8/  https://learn.microsoft.com/en-us/openapi/kiota/testing"
    },
  
    {
      "id": "51",
      "title": "A Guide to OpenAPI Client Generation with Kiota. Introduction (Part 1)",
      "url": "/dotnet/aspnetcore/2024/03/22/kiota-guide-introduction.html",
      "date": "March 22, 2024",
      "categories": ["dotnet","aspnetcore"],
      "tags": ["dotnet","aspnetcore","openapi","aspire","cli"],
      "shortinfo": "Introduces Kiota, explaining its purpose and relevance in the context of OpenAPI client generation. Highlights its features and benefits.",
      "content": "  Source code: https://github.com/NikiforovAll/kiota-getting-startedIntroductionKiota is a powerful command line tool developed by Microsoft that simplifies the process of generating API clients for calling any OpenAPI-described API.It gains traction, for example, GitHub teams decided to move away from the static landscape of the traditional Octokit, and now they are shipping SDKs using¬†Kiota (ref: github/Our move to generated SDKs)Purpose and GoalKiota aims to eliminate the need for developers to rely on different API SDKs for each API they interact with. So When you need to call multiple APIs, you can use Kiota to generate a consistent, strongly typed API client without having to learn a new library for every HTTP API.Key Features  Language Agnostic: Kiota provides support for a wide range of languages, including C#, CLI, Go, Java, PHP, Python, Ruby, Swift, and TypeScript.  Full OpenAPI Capabilities: It leverages the complete capabilities of OpenAPI descriptions.  Minimal Code Generation: It generates only the necessary source code by building on a core library.  Reduced External Dependencies: Kiota minimizes external dependencies.  JSON Schema Integration: It uses JSON Schema descriptions to generate primitive-based model serialization and deserialization code.  IDE Autocomplete: The generated code supports IDE autocomplete, aiding in API resource discovery.  Full Access to HTTP Capabilities: Kiota ensures you have full access to HTTP features.  Fine-Tuned Generation: Need only a specific part of the API? No problem. Kiota lets you filter the generation to the exact surface area you‚Äôre interested in.Use KiotaVisit https://learn.microsoft.com/en-us/openapi/kiota/install to see various installation options, in our case, we will use dotnet global-tooldotnet tool install --global Microsoft.OpenApi.KiotaA noteworthy feature of the Kiota CLI is its endeavor to enhance discoverability by incorporating the search command kiota search &lt;searchTerm&gt;.$ kiota search news# Key                                                     Title                           Description# apisguru::gov.bc.ca:news                                BC Gov News API Service 1.0     News API# apisguru::microsoft.com:cognitiveservices-NewsSearch    News Search Client              The News Search API lets you send a# apisguru::sportsdata.io:mlb-v3-rotoballer-premium-news  MLB v3 RotoBaller Premium News# apisguru::sportsdata.io:nba-v3-rotoballer-premium-news  NBA v3 RotoBaller Premium News# apisguru::sportsdata.io:nfl-v3-rotoballer-premium-news  NFL v3 RotoBaller Premium NewsLet‚Äôs say we want to know more about: microsoft.com:cognitiveservices-NewsSearch.$ kiota search apisguru::microsoft.com:cognitiveservices-NewsSearch# Key: apisguru::microsoft.com:cognitiveservices-NewsSearch# Title: News Search Client# Description: The News Search API lets you send a search query to Bing and get back a list of news that are relevant to the search query. This section provides technical details about the query parameters and headers that you use to request news and the JSON response objects that contain them. For examples that show how to make requests, see [Searching the web for news](https://docs.microsoft.com/en-us/azure/cognitive-services/bing-news-search/search-the-web).# Service:# OpenAPI: https://raw.githubusercontent.com/APIs-guru/openapi-directory/gh-pages/v2/specs/microsoft.com/cognitiveservices-NewsSearch/1.0/swagger.jsonWe can check structure of the API by using kiota show command, but first, we need to download OpenAPI specification.$ kiota download apisguru::microsoft.com:cognitiveservices-NewsSearch \\    --output ./src/NewsSearch.Sdk/OpenApi/NewsSearch.json$ kiota show \\    --openapi ./src/NewsSearch.Sdk/OpenApi/NewsSearch.json# /#  ‚îî‚îÄnews#     ‚îú‚îÄsearch#     ‚îî‚îÄtrendingtopics# Hint: use the --include-path and --exclude-path options with glob patterns to filter the paths displayed.# Example: kiota show -d \"C:\\Users\\Oleksii_Nikiforov\\dev\\kiota-getting-started\\.\\src\\NewsSearch.Sdk\\OpenApi\\NewsSearch.json\" --include-path \"**/foo\"DemoNow, we are ready to see how it works, but before we start I want to introduce you the demo application, note, it is intentionally complicated just to demonstrate various aspects of using Kiota:We have an application (App.Client) that calls our application‚Äôs API (App API). This application integrates with the Bing REST API. The unique aspect of this demo is that every HTTP client is automatically generated based on OpenAPI and Kiota.ComponentsAs part of this post, we don‚Äôt need to know about all components. You will learn more in future blog posts. For this post, focus on components marked with üéØ.            Component      Description                  üéØApp      Integrates with Bing REST API              üéØNewsSearch.Sdk      Generated OpenAPI HTTP Client by Kiota. It‚Äôs based on externally provided OpenAPI specification              App.Sdk      Generated OpenAPI HTTP Client by Kiota              App.Client      Invokes App via App.Sdk              App.Client.Sdk      Generated OpenAPI HTTP Client by Kiota              App.Client.Cli      Generated CLI Client by Kiota. Convention-based commands based on App.Client OpenAPI specification              App.AppHost      Aspire Host              App.ServiceDefaults      Reasonable service defaults      graph LR    App --&gt; NewsSearchSdk[\"NewsSearch.Sdk\"]    App --&gt; AppServiceDefaults[\"App.ServiceDefaults\"]    AppClient --&gt; AppSdk[\"App.Sdk\"]    AppClient[\"App.Client\"] --&gt; AppServiceDefaults    AppAppHost[\"App.AppHost\"] --&gt; App    AppAppHost --&gt; AppClientOur goal is to add an endpoint to fetch trending topics by Country Code.var builder = WebApplication.CreateBuilder(args);var services = builder.Services;var app = builder.Build();app.MapGet(\"trending/{country:minlength(2):maxlength(2)}\", (string? country) =&gt;{    // TODO:});app.Run();Generate ClientSo, first, we need to generate the client SDK based on OpenApi specification stored previously. Note, --class-name parameter specifies the name of the generated client - NewsSearchApiClient.üí° As mentioned earlier, Kiota supports partial client generation by using --include-path option.kiota generate -l CSharp \\    --log-level trace \\    --output ./src/NewsSearch.Sdk \\    --namespace-name NewsSearch.Sdk \\    --class-name NewsSearchApiClient \\    --include-path \"**/trendingtopics\" \\    --exclude-backward-compatible \\    --openapi ./src/NewsSearch.Sdk/OpenApi/NewsSearch.jsonAfter that, we want to add required dependencies. Luckily, Kiota help with it by providing friendly instructions as part of kiota info -l CSharp$ kiota info  -l CSharp# The language CSharp is currently in Stable maturity level.# Hint: use the install command to install the dependencies.# Example:#    dotnet add package Microsoft.Kiota.Abstractions --version 1.7.11#    dotnet add package Microsoft.Kiota.Authentication.Azure --version 1.1.4#    dotnet add package Microsoft.Kiota.Http.HttpClientLibrary --version 1.3.7#    dotnet add package Microsoft.Kiota.Serialization.Form --version 1.1.5#    dotnet add package Microsoft.Kiota.Serialization.Json --version 1.1.8#    dotnet add package Microsoft.Kiota.Serialization.Multipart --version 1.1.3#    dotnet add package Microsoft.Kiota.Serialization.Text --version 1.1.4Here is how NewsSearch.Sdk.csproj looks like:&lt;Project Sdk=\"Microsoft.NET.Sdk\"&gt;    &lt;ItemGroup&gt;        &lt;PackageReference Include=\"Microsoft.Kiota.Abstractions\" /&gt;        &lt;PackageReference Include=\"Microsoft.Kiota.Authentication.Azure\" /&gt;        &lt;PackageReference Include=\"Microsoft.Kiota.Http.HttpClientLibrary\" /&gt;        &lt;PackageReference Include=\"Microsoft.Kiota.Serialization.Form\" /&gt;        &lt;PackageReference Include=\"Microsoft.Kiota.Serialization.Json\" /&gt;        &lt;PackageReference Include=\"Microsoft.Kiota.Serialization.Multipart\" /&gt;        &lt;PackageReference Include=\"Microsoft.Kiota.Serialization.Text\" /&gt;    &lt;/ItemGroup&gt;&lt;/Project&gt;Add a project NewsSearch.Sdk reference to App and instantiate instance of NewsSearchApiClient.dotnet add ./src/App reference ./src/NewsSearch.Sdk/var requestAdapter = Mock&lt;IRequestAdapter&gt;;var client = new NewsSearchApiClient(requestAdapter);The request adapter interface IRequestAdapter is the primary point where Kiota service libraries will trigger the creation of a HTTP request. The default implementation in .NET has the name HttpClientRequestAdapter and it takes IAuthenticationProvider as a dependency.Most REST APIs are protected through some kind of authentication and authorization scheme. The default HTTP core services provided by Kiota require an authentication provider to be passed to handle authentication concerns.NewsSearch API has ApiKey Authentication. We can use standard class Microsoft.Kiota.Abstractions.Authentication.ApiKeyAuthenticationProviderApiKeyAuthenticationProvider authenticationProvider = new (    apiKey, \"Ocp-Apim-Subscription-Key\", KeyLocation.Header);HttpClientRequestAdapter requestAdapter = new (authenticationProvider){    BaseUrl = \"https://api.bing.microsoft.com/v7.0\"};Let‚Äôs put everything together:app.MapGet(    \"trending/{country:minlength(2):maxlength(2)}\",    async (IConfiguration configuration, string? country) =&gt;{    ApiKeyAuthenticationProvider authenticationProvider = new (        configuration[\"ApiKey\"], \"Ocp-Apim-Subscription-Key\", KeyLocation.Header);    HttpClientRequestAdapter requestAdapter = new (authenticationProvider)    {        BaseUrl = \"https://api.bing.microsoft.com/v7.0\"    };    var client = new NewsSearchApiClient(requestAdapter);    var response = await client        .News        .Trendingtopics.GetAsync(r =&gt; r.QueryParameters.Cc = country);    return response;});Before we test it, we need to set ApiKey. Navigate to Azure portal and create resource of type microsoft.bing/accounts and copy ApiKey.dotnet user-secrets --project ./src/App set ApiKey \"&lt;key&gt;\"Finally, we are ready to Run our demo:dotnet run --project ./src/Appcurl -s http://localhost:5103/trending/US | jq '.value[].name' | head -3The setup, as demonstrated, is quite straightforward. The hierarchical approach to the client builder is particularly commendable as it simplifies the discovery of REST APIs, making it intuitive and user-friendly.Automatic GenerationSince kiota ships as dotnet global-tool we can easily rely on it as dependency and therefore make it as part of build process. Here is how to build SDK client every time we build the project. This way we don‚Äôt need to use Kiota CLI everytime.Add this to NewsSearch.Sdk.csproj:&lt;Target Name=\"GenerateClient\" AfterTargets=\"Build\" Condition=\"$(Configuration)=='Debug'\"&gt;    &lt;Exec Command=\"dotnet kiota generate -l CSharp --output ./ --namespace-name NewsSearch.Sdk --class-name NewsSearchApiClient --include-path **/trendingtopics --exclude-backward-compatible --openapi ./OpenApi/NewsSearch.json\" WorkingDirectory=\"$(ProjectDir)\" /&gt;&lt;/Target&gt;ConclusionIn conclusion, this blog post has provided a comprehensive introduction to Kiota, a powerful tool developed by Microsoft for generating API clients. We‚Äôve learned how Kiota can simplify the process of interacting with multiple APIs by generating consistent, strongly typed API clients.We‚Äôve also explored Kiota‚Äôs key features, including its language-agnostic nature, minimal code generation, reduced external dependencies, and full access to HTTP capabilities.The post has demonstrated how to install and use Kiota, and provided a detailed walkthrough of generating a client SDK based on an OpenAPI specification. We‚Äôve seen how Kiota can enhance discoverability and how it can be integrated into an application to fetch trending topics from the Bing REST API.Overall, Kiota is a promising tool that can significantly streamline the process of working with APIs, making it a valuable addition to any developer‚Äôs toolkit.References  https://learn.microsoft.com/en-us/openapi/kiota/overview  https://github.com/darrelmiller/KiotaApp  https://github.com/NikiforovAll/kiota-getting-started"
    },
  
    {
      "id": "52",
      "title": "Building a federated search engine from scratch. An introduction.",
      "url": "/dotnet/2023/05/19/federated-search-intro.html",
      "date": "May 19, 2023",
      "categories": ["dotnet"],
      "tags": ["dotnet","api"],
      "shortinfo": "Federated search, also known as distributed information retrieval, is the simultaneous search of multiple searchable resources. Learn more about it in this blog post.",
      "content": "IntroductionFederated search, also known as distributed information retrieval, is the simultaneous search of multiple searchable resources. A user makes a single query request, which is distributed to the search engines, databases, or other query engines participating in the federation. The federated search then aggregates the results that are received from the search engines for presentation to the user.Usually, we need to access a vast array of information sources. However, as the number of these information sources grow, so does the complexity of finding the right piece of information at the right time. Furthermore, the API and contracts changes from source to source and it makes the integration with downstream service tedious task.Why Federated Search Matters: Federated search eliminates the need for users to search multiple databases/sources individually, saving time and effort. It also allows for a more comprehensive search, as it can access and retrieve information from diverse sources that a user may not have thought to check or did not have direct access to.Building blocksHere are the fundamental building blocks you‚Äôll need to consider:  Query Interface: This is the entry point for the user to ask their query. This could be a simple text box or a more complex interface with advanced options.  Query Translation: Since the search is federated, it means you‚Äôll be searching over multiple databases or search engines, each potentially with their own query language. Your federated search engine will need to translate the user‚Äôs query into the appropriate language for each underlying system.  Search Connector / Adapter: These components connect to the external databases or search engines to send the translated queries and fetch the results. The nature of the connectors will vary based on the APIs and interfaces provided by the underlying systems.  Result Aggregation: Once results are fetched from all the underlying systems, they need to be combined in a meaningful way. This can be quite complex because different systems may rank their results differently.  Result Presentation: This component is responsible for presenting the combined results to the user. It should be able to handle different types of results (text, images, etc.) and paginate them for ease of browsing.  Performance Optimization: Given that a federated search has to interact with multiple systems, it can potentially be quite slow. Techniques like caching, parallel queries, and preemptive queries can help to improve performance.  Error Handling: Your engine should be able to handle errors gracefully. This could include timeouts from slow underlying systems, systems going offline, or corrupt data in the results.  Logging and Analytics: You‚Äôll need a way to log errors and track the performance of your search engine. This data can be used to continually improve the engine.  Security and Privacy: Since the system will be dealing with potentially sensitive data, it‚Äôs important to ensure that it‚Äôs secure and respects the privacy of its users.Building a federated search engine from scratch is a major undertaking. There are numerous edge cases to handle, and performance and accuracy can be major challenges. It‚Äôs often more practical to build on top of existing search platforms and libraries, which can handle many of these concerns out of the box.graph TD    FSE[Federated Search Engine]    QI[Query Interface]    QT[Query Translation]    SCA[Search Connector / Adapter]    RA[Result Aggregation]    RP[Result Presentation]    PO[Performance Optimization]    EH[Error Handling]    LA[Logging and Analytics]    SP[Security and Privacy]    FSE --&gt; QI    FSE --&gt; QT    FSE --&gt; SCA    FSE --&gt; RA    FSE --&gt; RP    FSE --&gt; PO    FSE --&gt; EH    FSE --&gt; LA    FSE --&gt; SPQuery InterfaceDesigning a query interface for a federated search engine poses unique requirements and challenges because the interface must support search across multiple databases or search engines, each potentially having different data structures, query languages, and APIs. Here are some key requirements:  Ease of Use: Regardless of the complexity of the underlying databases and systems, the query interface should be straightforward to use. It should accept user queries in a simple and easy-to-understand format. A common approach is to allow plain text queries, but more complex interfaces could allow for advanced search options, Boolean operators, and other search enhancements.  Query Translation: The query interface needs to translate user queries into the specific query language or API calls required by each underlying system. This can be a complex task, as it requires an understanding of each system‚Äôs capabilities, syntax, and semantics.  Support for Multiple Data Types: The query interface should be flexible enough to accommodate different types of data - text, numbers, dates, geographical data, images, etc. This might require special syntax or options for each data type.  Scalability: The query interface should be designed in a way that it can easily accommodate the addition of new databases or search engines. This might involve modular design principles, where each database or search engine has its own adapter or connector.  Error Handling and Feedback: The query interface should provide meaningful feedback to the user. If a query cannot be executed for some reason, the interface should provide an error message that helps the user understand and rectify the problem. Additionally, it should provide some form of feedback while the search is being conducted, such as a progress indicator, particularly for long-running searches.  Security and Privacy: The query interface should ensure that user queries do not expose sensitive information and that they do not inadvertently allow for SQL injection or other forms of attacks. This might involve careful validation and sanitization of user inputs.These requirements emphasize the need for a thoughtful and careful design process when creating a query interface for a federated search engine. By meeting these requirements, you can ensure that your federated search engine is effective, versatile, and user-friendly.Here is what we can use as an example of very simple search query:public class FederatedScratchSearchCommand : IRequest&lt;FederatedScratchSearchResponse&gt;{    public FederatedScratchCollectionRequest Request { get; init; } = default!;    public string Mapping { get; init; } = string.Empty;}public class FederatedScratchCollectionRequest{    public string PrimaryKey { get; set; }    public string ParentPath { get; set; }    public string CollectionName { get; set; }    public string Query { get; set; }    public string DataSource { get; set; }    public IList&lt;FederatedScratchCollectionRequest&gt; Descendants { get; set; }}The code provided gives us a glimpse of the query interface for a federated search engine. It‚Äôs a great example that shows how you might model the request data structure for an API-based search request.The EpamApiSearchCommand class appears to be a request model that signifies an API search command and is defined with two main properties:  Request of type FederatedScratchCollectionRequest: This object contains the detailed search parameters for the API. This is where the user can specify the search collection name, primary key, parent path, and query. It also allows for nested search requests by including a list of descendants, effectively creating a tree-like search structure.  Mapping of type string: This is an optional property that appears to store a JMESPath query. JMESPath is a query language for JSON, which means this property can be used to map or filter the JSON response returned by the API. This property allows the client to specify the data structure they want to see in the response.The FederatedScratchCollectionRequest class, on the other hand, represents the request details that are to be sent to the API for processing. It has several properties that provide information about the collection to be searched. Here are the main components:  PrimaryKey: Corresponds to the main identifier of the collection item being searched. This could be an ID, a username, or any other unique identifier.  ParentPath: Represents the path in the hierarchical data structure where the search should begin. Used to perform join on parent collection.  CollectionName: Corresponds to the name of the collection.  Query: Search query - the term or condition that the search engine is looking for within the collection.  DataSource: The name of the data source, adapter identifier.  Descendants: This is a list of additional FederatedScratchCollectionRequest objects. This structure allows the user to send complex, nested queries, where each descendant represents a sub-query within the main query.The ParentPath and PrimaryKey keys play a crucial role in connecting different entities together, just like pieces of a puzzle. Their role is to establish relationships between data, helping us link everything together in a meaningful way.How complex this linking process ‚Äì or ‚Äòjoining‚Äô, as it‚Äôs often called ‚Äì turns out to be will vary based on your specific requirements. In our current setup, we‚Äôve kept things straightforward: we extract keys based on field names and use them for exact match comparisons.However, one of the beautiful aspects of designing your search engine is that it can grow and evolve with your needs. Feel free to adapt and enhance your query model as needed, making it as simple or intricate as your project demands.‚ÄìIn essence, this interface provides a robust and flexible way for users to define complex search queries that can be executed against a database or data structure. As this is just the query interface, the actual processing, searching, and returning of results would be handled by other components of the federated search engine.Query TranslationThe query translation component plays a crucial role in a federated search system, particularly when the system encompasses multiple databases or search engines with varying query languages or data schemas.In a federated search system, a user‚Äôs query is received in a standard format. However, each underlying search engine or database may require queries in a different format or structure, specific to its own data model or language. This is where the query translation component comes in.Here are some key aspects to consider about this crucial building block:  Syntax Translation: Query translation primarily involves converting the syntax of the input query to match the syntax of each underlying search engine‚Äôs query language. This might include adapting the query structure, transforming operators, or even changing the order of conditions.  Semantic Translation: Some databases or search engines may interpret the same query differently, depending on the context or semantic rules of their query language.  Data Mapping: If the federated search system includes databases with different data schemas, the query translation may also involve data mapping. This means converting field or column names from the user query to match the names used in each database.  Error Handling: Another important role of query translation is to handle any syntax or semantic errors that might occur during the translation process and provide useful feedback to the user.  Performance Considerations: Query translation can impact the performance of the search system. Complex translations can introduce delays, so it‚Äôs important to optimize this process where possible. This might include caching frequently used translations or using efficient algorithms for the translation process.In summary, query translation is an integral and complex part of any federated search system. It enables users to make queries without needing to understand the specifics of each underlying search engine or database, thereby making the system more accessible and user-friendly.RSQL (RESTful Service Query Language) is a query language for RESTful APIs. It provides a set of conventions for expressing queries in URLs. If you build your federated search engine to rely on APIs that all support RSQL, you can eliminate the need for query translation entirely.With RSQL, you can use a standard format to make queries against your data. For example, you might use a URL like this to make a query:https://example.com/api/users?filter=firstName==John;age=gt=30In this example, firstName==John and age=gt=30 are RSQL expressions. They can be translated as ‚Äúfind users where the first name is John and the age is greater than 30‚Äù.Here‚Äôs how you could incorporate RSQL into a federated search engine:  RSQL as a Query Language: Design your query interface to accept queries in RSQL format. This provides a standard, uniform query syntax that can be used regardless of the underlying data source.  API Support for RSQL: Ensure that each of the underlying APIs support RSQL. This may limit your choice of APIs, as not all APIs support RSQL natively. However, there are libraries available in many programming languages that can add RSQL support to an existing API.  RSQL Query Engine: Use an RSQL query engine to parse and execute the RSQL queries against each API. This engine takes the RSQL query from the query interface, sends it to the appropriate APIs, collects the results, and returns them to the user.  Integration: Integrate the RSQL query engine with the other components of your federated search engine, such as the result presentation and error handling components.By using RSQL and a compatible query engine, you can create a federated search engine that uses a single, uniform query language across all data sources. This eliminates the need for query translation and can make the search engine easier to develop and maintain. However, it does require all underlying APIs to support RSQL, which may not be feasible in all cases.Result AggregationWhen a search query is executed, the search engine retrieves results from multiple databases or search engines. The function of the Result Aggregation component is to gather these results and consolidate them into a unified, structured, and coherent format, which can then be presented to the user. It essentially enables a comprehensive and useful presentation of search results from across different databases and systems.Designing an efficient and effective Result Aggregation component involves addressing several key requirements:  Unification: The component should be capable of merging results from various sources into a single set of results. The unification process should handle discrepancies in data formats and structures across different systems.  Duplication Handling: Since search results may come from multiple sources, it‚Äôs possible to have duplicate entries. The aggregation component needs to identify and handle such duplications appropriately.  Performance Optimization: To provide a seamless user experience, the Result Aggregation component must perform its tasks quickly, even when dealing with large volumes of data.  Error Handling: In case of any issues in retrieving or processing data from any source, the component should handle such situations gracefully without breaking the whole operation.  Data Transformation: Depending on the application, the aggregation component might need to perform data transformation tasks. For example, it might need to convert data types, translate data values, or apply a particular data schema.  Pagination Support: When dealing with a large number of search results, the aggregation component should support pagination to limit the number of results displayed at one time and provide a more manageable view.  Filtering: Post-query filtering should be supported, allowing users to refine the aggregated results based on specific criteria.  Security and Privacy: The component must ensure that it complies with all relevant data privacy and security regulations and standards. This includes handling sensitive data appropriately and respecting any access control rules specified by the underlying databases or search engines.These requirements, while not exhaustive, highlight some of the critical aspects to consider when designing a Result Aggregation component for a Federated Search Engine. By meeting these requirements, you can deliver a high-performing, robust, and user-friendly search solution that consolidates and presents search results effectively.Result PresentationAfter the search query has been processed, translated, and executed, and the results have been aggregated, these results need to be presented to the user in an understandable format.The Result Presentation component is responsible for this task. It formats the aggregated search results and displays them in a way that meets the users‚Äô needs and preferences. This might involve creating an HTML page, a PDF report, a data visualization, an interactive UI, or any other format that suits the application.Good result presentation can dramatically improve the user experience of a search engine. It ensures that users can easily understand the search results, navigate through them, and take any necessary actions based on the results. It can also highlight the most relevant results, provide additional context, and offer options for refining or expanding the search.In summary, the Result Presentation component is the face of the Federated Search Engine. It is the component that users interact with the most and therefore has a significant impact on users‚Äô perceptions of the search engine‚Äôs usefulness and usability. Thus, designing an effective Result Presentation component involves a deep understanding of user needs, a good sense of design and usability, and a strong grasp of the technical aspects of presenting data in various formats.JMESPath can extract and transform data from a JSON document, making it a useful tool for the Result Presentation component of a Federated Search Engine. It can help format and structure the aggregated search results into a more consumable format for users. You gain a lot of flexibility in how you can transform, filter, and aggregate your search results. You can effectively tailor your search results to the specific needs of your users, improving the usefulness and user-friendliness of your search engine.Let‚Äôs go through the process of how to use JMESPath for result presentation:  Receiving Aggregated Results: Once the Result Aggregation component has gathered and consolidated results, it passes them to the Result Presentation component. These results are typically in the form of a JSON document or a similar data structure.  Formulating JMESPath Expressions: Depending on how you want to present the results, formulate JMESPath expressions that will extract and structure the required data. For example, if you want to show only the names and locations of users from the search results, you might use an expression like [*].{name: name, location: location}.  Applying JMESPath Expressions: Use a JMESPath library or tool to apply your expressions to the JSON document. This will transform the JSON data according to your expressions.  Converting the Results: The output of the JMESPath expressions will be a new JSON document or data structure that contains only the data you‚Äôre interested in, structured in the way you specified. You can then convert this data into the desired presentation format. This could involve generating an HTML table, creating a data visualization, or formatting a text report.  Displaying the Results: Finally, display the results to the user. This could involve serving an HTML page, rendering a data visualization in a browser, or sending a text report by email.By using JMESPath in the Result Presentation component, you can provide users with clear, concise, and relevant search results. It allows you to tailor your result presentation to the specific needs of your users, improving the user experience of your search engine.However, it‚Äôs worth noting that JMESPath is only one tool you can use for result presentation. Depending on your needs and the complexity of your data, you may need to use additional tools or techniques to present your search results effectively.Challenges and ConsiderationsAs intriguing as federated search is, it‚Äôs not without its challenges. The issues range from dealing with heterogeneous databases, varying query languages, to handling potential time delays and ensuring the security and privacy of data.Wrapping UpDesigning and implementing a Federated Search Engine is a complex yet rewarding task. As we‚Äôve explored in this blog post, each component‚ÄîQuery Interface, Query Translation, Query Execution, Result Aggregation, and Result Presentation‚Äîplays a critical role and requires careful consideration.We‚Äôve also looked into the intricacies of handling query translation using RSQL, using JMESPath for result aggregation, and how it can also aid in the effective presentation of results. Moreover, we addressed some of the significant challenges you might face during implementation, such as handling diverse data, relevance ranking, scalability, language processing and security.However, it‚Äôs important to remember that these complexities are manageable. By understanding the fundamentals of each component, leveraging appropriate tools and techniques, and addressing potential challenges proactively, you can build a powerful and user-friendly federated search engine that meets your specific needs.I hope this blog post has provided you with a solid understanding of the building blocks of a Federated Search Engine and the considerations involved in designing and implementing each component. Whether you‚Äôre just getting started with your search engine project or looking to optimize an existing one, I trust this information will prove useful.Remember, every search engine is different, and there is no one-size-fits-all approach. Stay flexible, keep learning, and don‚Äôt be afraid to adapt as you discover more about your users‚Äô needs and your data.Happy searching!"
    },
  
    {
      "id": "53",
      "title": "Introduction to JMESPath - JSON processor you should definitely know",
      "url": "/dotnet/2023/01/08/jmespath-intro.html",
      "date": "January 08, 2023",
      "categories": ["dotnet"],
      "tags": ["dotnet","json"],
      "shortinfo": "JMESPath is a powerful query language that enables processing of JSON payloads.",
      "content": "  TL;DR  Introduction  Demo  Extensibility  Summary  ReferencesTL;DRJMESPath is a powerful query language that enables processing of JSON payloads. It can be used in .NET, see JmesPath.Net.Source code: https://github.com/NikiforovAll/jmespath-demoIntroductionJSON processing is a common task in the day-to-day work of developers. We are used to working with JSON, but, occasionally, we need something more dynamic and efficient than System.Text.Json and Newtonsoft.Json. JMESPath is a powerful query language that allows you to perform Map/Reduce tasks in a declarative and intuitive manner.JMESPath is simple to use, the query itself is just a plain string. The benefit of this approach is that you can follow the inversion of control principle and give your users the control of writing JMESPath queries.üí° For example, the Azure CLI uses the ‚Äìquery parameter to execute a JMESPath query on the results of commands.public sealed class JmesPath{    public string Transform(string json, string expression);}DemoRead a random example of JSON string from a file:var source = new StreamReader(\"./example.json\").ReadToEnd();The content of the file:{  \"_id\": \"63ba60670fe420f2fb346866\",  \"isActive\": true,  \"balance\": \"$2,285.51\",  \"age\": 20,  \"eyeColor\": \"blue\",  \"name\": \"Eva Sharpe\",  \"email\": \"evasharpe@zaggles.com\",  \"phone\": \"+1 (950) 479-2130\",  \"registered\": \"2023-01-08T08:07:44.1787922+00:00\",  \"latitude\": 46.325291,  \"longitude\": 5.211461,  \"friends\": [    {      \"id\": 0,      \"name\": \"Nielsen Casey\",      \"age\": 19    },    {      \"id\": 1,      \"name\": \"Carlene Long\",      \"age\": 38    }  ]}The code below shows the processing of the example payload above. It demonstrates different concepts such as projections, filtering, aggregation, type transformation, etc. I think the syntax is quite intuitive and doesn‚Äôt need an explanation.The processing is quite simple:var expressions = new (string, string)[]{    (\"scalar\", \"balance\"),    (\"projection\", \"{email: email, name: name}\"),    (\"functions\", \"to_string(latitude)\"),    (\"arrays\", \"friends[*].name\"),    (\"filtering\", \"friends[?age &gt; `20`].name\"),    (\"aggregation\", \"{sum: sum(friends[*].age), names: join(',', friends[*].name)}\"),    (\"now. ISO 8601\", \"now()\"),    (\"now. Universal sortable date/time pattern\", \"now('u')\"),    (\"now. Long date pattern\", \"now('D')\"),    (\"format\", \"date_format(registered, 'd')\"),};foreach (var (exampleName, expression) in expressions){    var result = parser.Transform(source, expression);} ExtensibilityThe cool thing about JMESPath is it provides a way to add custom functions. See more details: https://github.com/jdevillard/JmesPath.Net/issues/81For example, here is how we can write now() function that accepts .NET string format provider, see the Microsoft docs https://learn.microsoft.com/en-us/dotnet/standard/base-types/standard-date-and-time-format-strings.public class NowFunction : JmesPathFunction{    private const string DefaultDateFormat = \"o\";    public NowFunction() : base(\"now\", minCount: 0, variadic: true) { }    public override JToken Execute(params JmesPathFunctionArgument[] args)    {        var format = args is { Length: &gt; 0 } x            ? x[0].Token            : DefaultDateFormat;        return new JValue(DateTimeOffset.UtcNow.ToString(format.ToString()));    }}SummaryAs you can see, JMESPath solves the issues of dynamic JSON processing based on user input quite nicely. It has an extensibility model that opens tons of possibilities.References  https://jmespath.org/  https://github.com/jdevillard/JmesPath.Net  https://github.com/NikiforovAll/jmespath-demo  https://learn.microsoft.com/en-us/cli/azure/query-azure-cli"
    },
  
    {
      "id": "54",
      "title": "Keycloak as Authorization Server in .NET",
      "url": "/dotnet/keycloak/2022/12/28/keycloak-authorization-server.html",
      "date": "December 28, 2022",
      "categories": ["dotnet","keycloak"],
      "tags": ["aspnetcore","dotnet","auth","keycloak"],
      "shortinfo": "Authorization Server is a powerful abstraction that allows to control authorization concerns. Learn how to implement it for .NET applications.",
      "content": "  TL;DR  Introduction  Example Overview          Configure Keycloak      Authorization based on ASP.NET Core Identity roles      Authorization based on Keycloak realm and client roles      Authorization based on Authorization Server permissions        The power of Authorization Server - define policies and permissions          Evaluate permissions      Demo        Summary  ReferencesTL;DRKeycloak.AuthService.Authorization provides a toolkit to use Keycloak as Authorization Server. An authorization Server is a powerful abstraction that allows to control authorization concerns. An authorization Server is also advantageous in microservices scenario because it serves as a centralized place for IAM and access control.Keycloak.AuthServices.Authorization: https://github.com/NikiforovAll/keycloak-authorization-services-dotnet#keycloakauthservicesauthorizationExample source code: https://github.com/NikiforovAll/keycloak-authorization-services-dotnet/tree/main/samples/AuthZGettingStartedIntroductionAuthorization refers to the process that determines what a user is able to do.ASP.NET Core authorization provides a simple, declarative role and a rich policy-based model. Authorization is expressed in requirements, and handlers evaluate a user‚Äôs claims against requirements. Imperative checks can be based on simple policies or policies which evaluate both the user identity and properties of the resource that the user is attempting to access.Resource servers (applications or services serving protected resources) usually rely on some kind of information to decide if access should be granted to a protected resource. For RESTful-based resource servers, that information is usually obtained from a security token, usually sent as a bearer token on every request to the server. For web applications that rely on a session to authenticate users, that information is usually stored in a user‚Äôs session and retrieved from there for each request.Keycloak is based on a set of administrative UIs and a RESTful API, and provides the necessary means to create permissions for your protected resources and scopes, associate those permissions with authorization policies, and enforce authorization decisions in your applications and services.Considering that today we need to consider heterogeneous environments where users are distributed across different regions, with different local policies, using different devices, and with a high demand for information sharing, Keycloak Authorization Services can help you improve the authorization capabilities of your applications and services by providing:  Resource protection using fine-grained authorization policies and different access control mechanisms  Centralized Resource, Permission, and Policy Management  Centralized Policy Decision Point  REST security based on a set of REST-based authorization services  Authorization workflows and User-Managed Access  The infrastructure to help avoid code replication across projects (and redeploys) and quickly adapt to changes in your security requirements.Example OverviewIn this blog post I will demonstrate how to perform authorization in two ways:  Role-based access control (RBAC) check executed by Resource Server (API)          /endpoint - required ASP.NET Core identity role      /endpoint - required realm role      /endpoint - required client role        Remote authorization policy check executed by Authorization Server (Keycloak)          /endpoint - remotely executed policy selected for ‚Äúworkspace‚Äù - resource, ‚Äúworkspaces:read‚Äù - scope.      var app = builder.Build();app    .UseHttpsRedirection()    .UseApplicationSwagger(configuration)    .UseAuthentication()    .UseAuthorization();app.MapGet(\"/endpoint1\", (ClaimsPrincipal user) =&gt; user)    .RequireAuthorization(RequireAspNetCoreRole);app.MapGet(\"/endpoint2\", (ClaimsPrincipal user) =&gt; user)    .RequireAuthorization(RequireRealmRole);app.MapGet(\"/endpoint3\", (ClaimsPrincipal user) =&gt; user)    .RequireAuthorization(RequireClientRole);app.MapGet(\"/endpoint4\", (ClaimsPrincipal user) =&gt; user)    .RequireAuthorization(RequireToBeInKeycloakGroupAsReader);await app.RunAsync();Project structure:$ tree -L 2.‚îú‚îÄ‚îÄ AuthZGettingStarted.csproj‚îú‚îÄ‚îÄ Program.cs‚îú‚îÄ‚îÄ Properties‚îÇ   ‚îî‚îÄ‚îÄ launchSettings.json‚îú‚îÄ‚îÄ ServiceCollectionExtensions.Auth.cs‚îú‚îÄ‚îÄ ServiceCollectionExtensions.Logging.cs‚îú‚îÄ‚îÄ ServiceCollectionExtensions.OpenApi.cs‚îú‚îÄ‚îÄ appsettings.Development.json‚îú‚îÄ‚îÄ appsettings.json‚îú‚îÄ‚îÄ assets‚îÇ   ‚îú‚îÄ‚îÄ realm-export.json‚îÇ   ‚îî‚îÄ‚îÄ run.http‚îî‚îÄ‚îÄ docker-compose.ymlEntry point:var builder = WebApplication.CreateBuilder(args);var configuration = builder.Configuration;var services = builder.Services;builder.AddSerilog();services    .AddApplicationSwagger(configuration)    .AddAuth(configuration);Register AuthN and AuthZ services in Dependency Injection container:public static IServiceCollection AddAuth(    this IServiceCollection services, IConfiguration configuration){    services.AddKeycloakAuthentication(configuration);    services.AddAuthorization(options =&gt;    {        options.AddPolicy(            Policies.RequireAspNetCoreRole,            builder =&gt; builder.RequireRole(Roles.AspNetCoreRole));        options.AddPolicy(            Policies.RequireRealmRole,            builder =&gt; builder.RequireRealmRoles(Roles.RealmRole));        options.AddPolicy(            Policies.RequireClientRole,            builder =&gt; builder.RequireResourceRoles(Roles.ClientRole));        options.AddPolicy(            Policies.RequireToBeInKeycloakGroupAsReader,            builder =&gt; builder                .RequireAuthenticatedUser()                .RequireProtectedResource(\"workspace\", \"workspaces:read\"));    }).AddKeycloakAuthorization(configuration);    return services;}public static class AuthorizationConstants{    public static class Roles    {        public const string AspNetCoreRole = \"realm-role\";        public const string RealmRole = \"realm-role\";        public const string ClientRole = \"client-role\";    }    public static class Policies    {        public const string RequireAspNetCoreRole = nameof(RequireAspNetCoreRole);        public const string RequireRealmRole = nameof(RequireRealmRole);        public const string RequireClientRole = nameof(RequireClientRole);        public const string RequireToBeInKeycloakGroupAsReader =             nameof(RequireToBeInKeycloakGroupAsReader);    }}Configure KeycloakIn this post I‚Äôm going to skip basic Keycloak installation and configuration, please see my previous posts for more details https://nikiforovall.github.io/tags.html#keycloak-ref.Prerequisites:  Create a realm named: Test  Create a user with username/password: user/user  Create a realm role: realm-role  Create a client: test-client          Create an audience mapper: Audience ‚û° test-client      Enable Client Authentication and Authorization      Enable Implicit flow and add a valid redirect URL (used by Swagger to retrieve a token)        Create a client role: client-role  Create a group called workspace and add the ‚Äúuser‚Äù to itFull Keycloak configuration (including the steps below) can be found at realm-export.jsonAuthorization based on ASP.NET Core Identity rolesKeycloak.AuthService.Authentication ads the KeycloakRolesClaimsTransformation that maps roles provided by Keycloak. The source for role claim could be one of the following:  Realm - map realm roles  ResourceAccess - map client roles  None - don‚Äôt mapflowchart LR    AddKeycloakAuthentication --&gt; AddAuthentication    AddKeycloakAuthentication --&gt; KeycloakRolesClaimsTransformationDepending on your needs, you can use realm roles, client roles or skip automatic role mapping/transformation. The role claims transformation is based on the config. For example, here is how to use realms role for ASP.NET Core Identity roles. As result, you can use build-in role-based authorization.{  \"Keycloak\": {    \"realm\": \"Test\",    \"auth-server-url\": \"http://localhost:8080/\",    \"ssl-required\": \"none\",    \"resource\": \"test-client\",    \"verify-token-audience\": true,    \"credentials\": {      \"secret\": \"\"    },    \"confidential-port\": 0,    \"RolesSource\": \"Realm\"  }}So, for a user with the next access token generated by Keycloak the roles are effectively evaluated to ‚Äúrealm-role‚Äù, ‚Äúdefault-roles-test‚Äù, ‚Äúoffline_access‚Äù, ‚Äúuma_authorization‚Äù. And if you change ‚ÄúRolesSource‚Äù to ‚ÄúResourceAccess‚Äù it would be ‚Äúclient-role‚Äù.{  \"exp\": 1672275584,  \"iat\": 1672275284,  \"jti\": \"1ce527e6-b852-48e9-b27b-ed8cc01cf518\",  \"iss\": \"http://localhost:8080/realms/Test\",  \"aud\": [    \"test-client\",    \"account\"  ],  \"sub\": \"8fd9060e-9e3f-4107-94f6-6c3a242fb91a\",  \"typ\": \"Bearer\",  \"azp\": \"test-client\",  \"session_state\": \"c32e4165-f9bd-4d4c-93bd-3847f4ffc697\",  \"acr\": \"1\",  \"realm_access\": {    \"roles\": [      \"realm-role\",      \"default-roles-test\",      \"offline_access\",      \"uma_authorization\"    ]  },  \"resource_access\": {    \"test-client\": {      \"roles\": [        \"client-role\"      ]    },    \"account\": {      \"roles\": [        \"manage-account\",        \"manage-account-links\",        \"view-profile\"      ]    }  },  \"scope\": \"openid email profile\",  \"sid\": \"c32e4165-f9bd-4d4c-93bd-3847f4ffc697\",  \"email_verified\": false,  \"preferred_username\": \"user\",  \"given_name\": \"\",  \"family_name\": \"\"}üí° Note, you can change ‚ÄúRolesSource‚Äù to ‚ÄúNone‚Äù and instead of using KeycloakRolesClaimsTransformation, use Keycloak role claim mapper and populate role claim based on configuration. Luckily, it is easy to do from Keycloak admin panel.services.AddAuthorization(options =&gt;{    options.AddPolicy(        Policies.RequireAspNetCoreRole,        builder =&gt; builder.RequireRole(Roles.AspNetCoreRole))});Authorization based on Keycloak realm and client rolesAuthorizationPolicyBuilder allows to register policies and Keycloak.AuthServices.Authorization adds a handy method to register rules that make use of the specific structure of access tokens generated by Keycloak.services.AddAuthorization(options =&gt;{    options.AddPolicy(        Policies.RequireRealmRole,        builder =&gt; builder.RequireRealmRoles(Roles.RealmRole));    options.AddPolicy(        Policies.RequireClientRole,        builder =&gt; builder.RequireResourceRoles(Roles.ClientRole));});// PoliciesBuilderExtensions.cspublic static AuthorizationPolicyBuilder RequireResourceRoles(    this AuthorizationPolicyBuilder builder, params string[] roles) =&gt;    builder        .RequireClaim(KeycloakConstants.ResourceAccessClaimType)        .AddRequirements(new ResourceAccessRequirement(default, roles));public static AuthorizationPolicyBuilder RequireRealmRoles(    this AuthorizationPolicyBuilder builder, params string[] roles) =&gt;    builder        .RequireClaim(KeycloakConstants.RealmAccessClaimType)        .AddRequirements(new RealmAccessRequirement(roles));Authorization based on Authorization Server permissionsPolicy Enforcement Point (PEP) is responsible for enforcing access decisions from the Keycloak server where these decisions are taken by evaluating the policies associated with a protected resource. It acts as a filter or interceptor in your application in order to check whether or not a particular request to a protected resource can be fulfilled based on the permissions granted by these decisions.Keycloak supports fine-grained authorization policies and is able to combine different access control mechanisms such as:  Attribute-based access control (ABAC)  Role-based access control (RBAC)  User-based access control (UBAC)  Context-based access control (CBAC)  Rule-based access control  Using JavaScript  Time-based access control  Support for custom access control mechanisms (ACMs) through a Service Provider Interface (SPI)Here is what happens when authenticated user tries to access a protected resource:sequenceDiagram    participant User    participant API    participant AuthServer as Authorization Server - PEP    User -&gt;&gt;+ API: access a protected endpoint    API-&gt;&gt;+ AuthServer : verify if user has an access to resource + scope    AuthServer -&gt;&gt;- API: authorization result (yes/no)    API -&gt;&gt;- User: response (2xx/403)services.AddAuthorization(options =&gt;{    options.AddPolicy(        Policies.RequireToBeInKeycloakGroupAsReader,        builder =&gt; builder            .RequireAuthenticatedUser()            .RequireProtectedResource(\"workspace\", \"workspaces:read\"));});// PoliciesBuilderExtensions.cs/// &lt;summary&gt;/// Adds protected resource requirement to builder./// Makes outgoing HTTP requests to Authorization Server./// &lt;/summary&gt;public static AuthorizationPolicyBuilder RequireProtectedResource(    this AuthorizationPolicyBuilder builder, string resource, string scope) =&gt;    builder.AddRequirements(new DecisionRequirement(resource, scope));The power of Authorization Server - define policies and permissionsResource management is straightforward and generic. After creating a resource server, you can start creating the resources and scopes that you want to protect. Resources and scopes can be managed by navigating to the Resource and Authorization Scopes tabs, respectively.So, to define a protected resource we need to create it in the Keycloak and assigned scope to it. In our case, we need to create ‚Äúworkspace‚Äù resource with ‚Äúworkspaces:read‚Äù scope.For more details, please see https://www.keycloak.org/docs/latest/authorization_services/#_resource_overview.PermissionsPoliciesABACRBACUBACCBACRuleBasedJavaScriptTimeBasedResourcesScopesDecision   StrategyTo create a scope:  Navigate to ‚ÄúClients‚Äù tab on the sidebar  Select ‚Äútest-client‚Äù from the list  Go to ‚ÄúAuthorization‚Äù tab (make sure you enabled ‚ÄúAuthorization‚Äù checkbox on the ‚ÄúSettings‚Äù tab)  Select ‚ÄúScopes‚Äù sub-tab  Click ‚ÄúCreate authorization scope‚Äù  Specify workspaces:read as Name  Click ‚ÄúSave‚ÄùTo create a resource:  From the ‚ÄúAuthorization‚Äù tab  Select ‚ÄúResources‚Äù sub-tab  Click ‚ÄúCreate resource‚Äù  Specify workspace as Name  Specify urn:resource:workspace as Type  Specify ‚Äúworkspaces:read‚Äù as ‚ÄúAuthorization scopes‚Äù  Click ‚ÄúSave‚ÄùLet‚Äôs say we want to implement a rule that only users with realm-role role and membership in workspace group can read a ‚Äúworkspace‚Äù resource. To accomplish this, we need to create the next two policies:  From the ‚ÄúAuthorization‚Äù tab  Select ‚ÄúPolicies‚Äù sub-tab  Click ‚ÄúCreate policy‚Äù  Select ‚ÄúRole‚Äù option  Specify Is in realm-role as Name  Click ‚ÄúAdd roles‚Äù  Select realm-role role  Logic: Positive  Click ‚ÄúSave‚Äù  Click ‚ÄúCreate policy‚Äù  Select ‚ÄúGroup‚Äù option  Specify Is in workspace group as Name  Click ‚ÄúAdd group‚Äù  Select ‚Äúworkspace‚Äù group  Logic: Positive  Click ‚ÄúSave‚ÄùNow, we can create the permission:  From the ‚ÄúAuthorization‚Äù tab  Select ‚ÄúPermissions‚Äù sub-tab  Click ‚ÄúCreate permission‚Äù  Select ‚ÄúCreate resource-based permission‚Äù  Specify Workspace Access as Name  Specify workspace as resource  Add workspaces:read as authorization scope  Add two previously created policies to the ‚ÄúPolicies‚Äù  Specify Unanimous as Decision Strategy  Click ‚ÄúSave‚ÄùThe decision strategy dictates how the policies associated with a given permission are evaluated and how a final decision is obtained. ‚ÄòAffirmative‚Äô means that at least one policy must evaluate to a positive decision in order for the final decision to be also positive. ‚ÄòUnanimous‚Äô means that all policies must evaluate to a positive decision in order for the final decision to be also positive. ‚ÄòConsensus‚Äô means that the number of positive decisions must be greater than the number of negative decisions. If the number of positive and negative is the same, the final decision will be negative.Evaluate permissions  From the ‚ÄúAuthorization‚Äù tab  Select ‚ÄúEvaluate‚Äù sub-tabLet‚Äôs say the ‚Äúuser‚Äù has realm-role, but is not a member of workspace groupHere is how the permission evaluation is interpreted by Keycloak: And if we add the ‚Äúuser to workspace group: Demo  Navigate at https://localhost:7248/swagger/index.html  Click ‚ÄúAuthorize‚Äù. Note, the access token is retrieved based on ‚ÄúImplicit Flow‚Äù that we‚Äôve previously configured.  Enter credentials: ‚Äúuser/user‚Äù  Execute ‚Äú/endpoint4‚Äù As you can see, the response is 200 OK. I suggest you to try removing ‚Äúuser‚Äù from the ‚Äúworkspace‚Äù group and see how it works.As described above, the permission is evaluated by Keycloak therefore you can see outgoing HTTP requests in the logs:12:19:28 [INFO] Start processing HTTP request \"POST\" http://localhost:8080/realms/Test/protocol/openid-connect/token\"System.Net.Http.HttpClient.IKeycloakProtectionClient.ClientHandler\"12:19:28 [INFO] Sending HTTP request \"POST\" http://localhost:8080/realms/Test/protocol/openid-connect/token\"System.Net.Http.HttpClient.IKeycloakProtectionClient.ClientHandler\"12:19:28 [INFO] Received HTTP response headers after 8.5669ms - 200\"System.Net.Http.HttpClient.IKeycloakProtectionClient.LogicalHandler\"12:19:28 [INFO] End processing HTTP request after 25.3503ms - 200\"Keycloak.AuthServices.Authorization.Requirements.DecisionRequirementHandler\"12:19:28 [DBUG] [\"DecisionRequirement: workspace#workspaces:read\"] Access outcome True for user \"user\"\"Microsoft.AspNetCore.Authorization.DefaultAuthorizationService\"12:19:28 [DBUG] Authorization was successful. üí° Note, In this post, I‚Äôve showed you how to protect a one resource known to the system, but it is actually possible to create resource programmatically and compose ASP.NET Core policies during runtime. See Keycloak.AuthServices.Authorization.ProtectedResourcePolicyProvider for more details.SummaryAn authorization Server is a highly beneficial abstraction and it is quite easy to solve a wide range of well-known problems without ‚ÄúReinventing the wheel‚Äù. Keycloak.AuthServices.Authorization helps you to define a protected resource and does the interaction with Authorization Server for you. Let me know what you think üôÇReferences  https://learn.microsoft.com/en-us/aspnet/core/security/authorization/introduction  https://learn.microsoft.com/en-us/aspnet/core/security/authorization/roles  https://learn.microsoft.com/en-us/aspnet/core/security/authorization/policies  https://www.keycloak.org/docs/latest/authorization_services  https://www.keycloak.org/docs/latest/authorization_services/#_resource_overview  https://github.com/NikiforovAll/keycloak-authorization-services-dotnet"
    },
  
    {
      "id": "55",
      "title": "Use Keycloak as Identity Provider from Blazor WebAssembly (WASM) applications",
      "url": "/blazor/dotnet/2022/12/08/dotnet-keycloak-blazorwasm-auth.html",
      "date": "December 08, 2022",
      "categories": ["blazor","dotnet"],
      "tags": ["aspnetcore","dotnet","auth","keycloak","blazor"],
      "shortinfo": "Learn how to integrate with Keycloak from Blazor WASM. Create a public client and use built-in capabilities of Microsoft.AspNetCore.Components.WebAssembly.Authentication that integrates with OpenId Connect compliant providers.",
      "content": "  TL;DR  Example overview  Backend. Configure Keycloak. Add Authentication          Start Keycloak in a development mode      Configure Realm      Add Keycloak to backend      Get access token from Swagger UI        Frontend. Blazor WASM          Integrate with Keycloak from the frontend. Overview      Authentication component      App Component      Configure AuthenticationService      Demo        ReferenceTL;DRLearn how to integrate with Keycloak from Blazor WASM. Create a public client and use built-in capabilities of Microsoft.AspNetCore.Components.WebAssembly.Authentication that integrates with OpenId Connect compliant providers.Source code: https://github.com/NikiforovAll/keycloak-authorization-services-dotnet/blob/main/samples/BlazorExample overviewBasically, we have a trimmed and modified version of the default template for Blazor WASM. You can generate the code I started from by running dotnet new blazorwasm -n Blazor.üöÄGoal: The main point of interest is to protect ‚ÄúFetch Data‚Äù tab from unauthenticated access and make it possible to access WeatherForecast data from Web API that require authentication.Therefore, we need to implement some sort of mechanism that unauthorized redirects users to an Identity Provider (i.e.: Keycloak) and, once they logged in successfully, propagates the token from the client to the backend.Here is a project structure. Note, it is a finished version of the example.$ tree -L 3.‚îú‚îÄ‚îÄ Client‚îÇ   ‚îú‚îÄ‚îÄ App.razor‚îÇ   ‚îú‚îÄ‚îÄ Blazor.Client.csproj‚îÇ   ‚îú‚îÄ‚îÄ Pages‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Authentication.razor‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FetchData.razor‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Index.razor‚îÇ   ‚îú‚îÄ‚îÄ Program.cs‚îÇ   ‚îú‚îÄ‚îÄ Properties‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ launchSettings.json‚îÇ   ‚îú‚îÄ‚îÄ Shared‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LoginDisplay.razor‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MainLayout.razor‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MainLayout.razor.css‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ NavMenu.razor‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ NavMenu.razor.css‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ RedirectToLogin.razor‚îÇ   ‚îú‚îÄ‚îÄ _Imports.razor‚îÇ   ‚îî‚îÄ‚îÄ wwwroot‚îÇ       ‚îú‚îÄ‚îÄ css‚îÇ       ‚îî‚îÄ‚îÄ index.html‚îú‚îÄ‚îÄ Server‚îÇ   ‚îú‚îÄ‚îÄ Blazor.Server.csproj‚îÇ   ‚îú‚îÄ‚îÄ Controllers‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ WeatherForecastController.cs‚îÇ   ‚îú‚îÄ‚îÄ Pages‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Error.cshtml‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Error.cshtml.cs‚îÇ   ‚îú‚îÄ‚îÄ Program.cs‚îÇ   ‚îú‚îÄ‚îÄ Properties‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ launchSettings.json‚îÇ   ‚îú‚îÄ‚îÄ appsettings.Development.json‚îÇ   ‚îú‚îÄ‚îÄ appsettings.json‚îú‚îÄ‚îÄ Shared‚îÇ   ‚îú‚îÄ‚îÄ Blazor.Shared.csproj‚îÇ   ‚îú‚îÄ‚îÄ WeatherForecast.cs‚îú‚îÄ‚îÄ assets‚îÇ   ‚îî‚îÄ‚îÄ realm-export.json‚îî‚îÄ‚îÄ docker-compose.yml  Client - is a Microsoft.NET.Sdk.BlazorWebAssembly project responsible for UI, a client.  Server - is a Microsoft.NET.Sdk.Web project responsible for API, typical backend in your SPA application.  Shared - is a Microsoft.NET.Sdk, shared code between client and server. Usually, it is used to share data models / DTOs.A home page doesn‚Äôt require user to be authenticated and looks like this: Before we look at BlazorWASM (aka client-side), we need to add authentication to a backend.Backend. Configure Keycloak. Add AuthenticationKeycloak supports both OpenID Connect and SAML protocols. OpenID Connect (OIDC) is an authentication protocol that is an extension of OAuth 2.0. While OAuth 2.0 is only a framework for building authorization protocols and is mainly incomplete, OIDC is a full-fledged authentication and authorization protocol. OIDC also makes heavy use of the Json Web Token (JWT) set of standards. These standards define an identity token JSON format and ways to digitally sign and encrypt that data in a compact and web-friendly way.In my previous blog post - Use Keycloak as Identity Provider in ASP.NET Core 6, I showed you how to configure Keycloak as OAuth2 + OpenID Connect compliant provider to add authentication to Web API. Let‚Äôs go through the process one more time üíÉStart Keycloak in a development modeHere is docker-compose that you can use for development purposes. Create a file named docker-compose.yml and rundocker compose up command.version: \"3.9\"services:  keycloak:    image: quay.io/keycloak/keycloak:19.0.1    environment:      KEYCLOAK_ADMIN: admin      KEYCLOAK_ADMIN_PASSWORD: admin    command:      [        'start-dev'      ]    ports:      - 8080:8080Configure RealmIn order to logically group users of your system we need to create a Realm in the Keycloak. Note, Master Realm should only be used for Keycloak administration purposes.To create a Realm:  Navigate at http://localhost:8080/admin/master/console  On the left side bar click on Realm Dropdown (‚ÄúMaster‚Äù)  Click ‚ÄúCreate Realm‚Äù  Specify ‚ÄúRealm name‚Äù - TestYou should see something like that: Keycloak allows login and sign-up of users by a wide range of social logins (e.g.: microsoft, google, github). But for the simplicity, I will create a user manually.To Create a user in the ‚ÄúTest‚Äù Realm:  On the left side bar click on ‚ÄúUsers‚Äù item.  Click ‚ÄúCreate new user‚Äù  Specify user information          username: user      email: user@nikiforovall.com      first name: John      last name: Doe        Click ‚ÄúCreate‚Äù  Go to ‚ÄúCredentials‚Äù tab  Click ‚ÄúSet password‚Äù          password: user      password confirmation: user      temporary: off        Click ‚ÄúSave‚Äù; Click ‚ÄúSet Password‚ÄùYou should see something like that: Keycloak, by default, provides user account management functionality. It is available through the inherited role in ‚ÄúRole Mapping‚Äù tab in the user account. To see a user account:  Navigate at http://localhost:8080/realms/Test/account  Specify newly created user account credentials - user:user  Click ‚ÄúPersonal Info‚ÄùHere is what I see on my screen: Add Keycloak to backendInstall Keycloak.AuthServices.Authentication package for Blazor.Server project by running the next command from the project folder:dotnet add package Keycloak.AuthServices.Authentication --version 1.2.1Here is the simplest integration with Keycloak from .NET perspective:// Program.csvar builder = WebApplication.CreateBuilder(args);var services = builder.Services;var configuration = builder.Configuration;services.AddKeycloakAuthentication(configuration);var app = builder.Build();app.UseAuthentication();app.UseAuthorization();app.Run();To hookup, the backend with Keycloak we need to create a Client. Later, this client will be used to configure details of user authorization flow.  Clients are entities that interact with Keycloak to authenticate users and obtain tokens. Most often, clients are applications and services acting on behalf of users that provide a single sign-on experience to their users and access other services using the tokens issued by the server. Clients can also be entities only interested in obtaining tokens and acting on their own behalf for accessing other services.To create a client:  Open a ‚ÄúTest‚Äù realm  On the left side bar click on ‚ÄúClients‚Äù item.  Click ‚ÄúCreate client‚Äù          specify client-id: test-client        Click ‚ÄúNext‚Äù  Check ‚ÄúImplicit flow‚Äù (we will use it for swagger, useful for the development)  ‚ÄúSave‚ÄùNow, we can download something called adapter config. On the top-right click ‚ÄúAction‚Äù dropdown and select ‚ÄúDownload adapter config‚Äù option. Like this: Open appsettings.Development.json and paste as follows:{    \"Keycloak\": {        \"realm\": \"Test\",        \"auth-server-url\": \"http://localhost:8080/\",        \"ssl-required\": \"none\",        \"resource\": \"test-client\",        \"verify-token-audience\": true,        \"credentials\": {            \"secret\": \"\"        },        \"confidential-port\": 0    }}  üí°‚ö† Note, by default, Keycloak.AuthServices.Authentication assumes that the intended Audience is the ‚Äúresource‚Äù. And it is something that we must configure additionally for quay.io/keycloak/keycloak:19.0.1. Alternatively, for development purposes, you may want to change ‚Äúresource‚Äù to the audience that is provided by default - ‚Äúaccount‚Äù.Let‚Äôs see how to add an audience to a client by using client scopes. Client scope defines a set of mappers that shape the content of access and id tokens. For example, all newly created clients by default have a bunch of client scopes assigned such as email, profile, roles. As you might guess, these client scopes are responsible for adding well-known claims as part of the OpenId Connect protocol.  On the left side bar click on ‚ÄúClients‚Äù item.  Click ‚Äútest-client‚Äù  Open ‚ÄúClient scopes‚Äù tab  Click on ‚Äútest-client-dedicated‚Äù, should be on tope of the list of scopes  From the ‚ÄúMappers‚Äù tab, click ‚ÄúCreate a new mapper‚Äù  Pick ‚ÄúAudience‚Äù from the list          specify name: Audience      include client audience: ‚Äútest-client‚Äù        Click ‚ÄúSave‚ÄùHere is the result: Besides ‚ÄúSetup‚Äù sub-tab, ‚ÄúClient Scopes‚Äù tab has ‚ÄúEvaluate‚Äù sub-tab. It might come in handy when you need to figure out effective protocol mappers, effective role scope mappings, the content of access, and id tokens.From the ‚ÄúEvaluate‚Äù sub-tab specify ‚ÄúUser‚Äù = ‚ÄúJohn Doe‚Äù and client ‚ÄúGenerate ID token‚Äù down-right corner.{    \"exp\": 1670420420,    \"iat\": 1670420120,    \"jti\": \"a5ee2c73-f3e9-495c-8d7c-eb75909b9b16\",    \"iss\": \"http://localhost:8080/realms/Test\",    \"aud\": [        \"test-client\",        \"account\"    ],    \"sub\": \"b7fe3cec-7221-49e3-90db-3148c0467a5e\",    \"typ\": \"Bearer\",    \"azp\": \"test-client\",    \"session_state\": \"77243a94-ed91-4d5c-a03b-d8fb988cee81\",    \"acr\": \"1\",    \"realm_access\": {        \"roles\": [            \"default-roles-test\",            \"offline_access\",            \"uma_authorization\"        ]    },    \"resource_access\": {        \"account\": {            \"roles\": [                \"manage-account\",                \"manage-account-links\",                \"view-profile\"            ]        }    },    \"scope\": \"openid profile email\",    \"sid\": \"77243a94-ed91-4d5c-a03b-d8fb988cee81\",    \"email_verified\": false,    \"name\": \"John Doe\",    \"preferred_username\": \"user\",    \"given_name\": \"John\",    \"family_name\": \"Doe\",    \"email\": \"user@nikiforovall.com\"}Sure enough, we can see the ‚Äútest-client‚Äù in the intended audience claim ‚Äúaud‚Äù. Make sure it is specified in ‚Äúresource‚Äù from the Keycloak adapter config file and we are ready to access some APIs.Get access token from Swagger UISwashbuckle.AspNetCore can be configured to retrieve access tokens based on ‚ÄúOpenID Endpoint Configuration‚Äù. Keycloak serves at a well-known endpoint ‚Äúhttp://localhost:8080/realms/{realm}/.well-known/openid-configuration‚Äù. This way, Swagger UI will add all possible flows to retrieve an access token.// Program.csservices.AddEndpointsApiExplorer();var openIdConnectUrl = $\"{configuration[\"Keycloak:auth-server-url\"]}\" + \"realms/{configuration[\"Keycloak:realm\"]}/\" + \".well-known/openid-configuration\";services.AddSwaggerGen(c =&gt;{    var securityScheme = new OpenApiSecurityScheme    {        Name = \"Auth\",        In = ParameterLocation.Header,        Type = SecuritySchemeType.OpenIdConnect,        OpenIdConnectUrl = new Uri(openIdConnectUrl),        Scheme = \"bearer\",        BearerFormat = \"JWT\",        Reference = new OpenApiReference        {            Id = \"Bearer\",            Type = ReferenceType.SecurityScheme        }    };    c.AddSecurityDefinition(securityScheme.Reference.Id, securityScheme);    c.AddSecurityRequirement(new OpenApiSecurityRequirement    {        {securityScheme, Array.Empty&lt;string&gt;()}    });});For Swagger UI, I tend to use an ‚ÄúImplicit Flow‚Äù, therefore, we need to configure a valid redirect URI in order to prevent Keycloak from redirecting to malicious URLs.  On the left side bar click on ‚ÄúClients‚Äù item.  Click ‚Äútest-client‚Äù  Open ‚ÄúSettings‚Äù tab  Add Swagger UI URL http://localhost:5126/* to ‚ÄúValid redirect URIs‚Äù  In ‚ÄúWeb Origins‚Äù specify +  Click ‚ÄúSave‚Äù   Navigate http://localhost:5126/swagger/index.html and click ‚ÄúAuthorize‚Äù button.  Scroll down to ‚ÄúBearer (OAuth2, implicit)‚Äù  Specify client id: ‚Äútest-client‚Äù  Click ‚ÄúAuthorize‚Äù button  You will be redirected to Keycloak to enter the credentials  (user:user) Now, we can test the API from Swagger UI: Frontend. Blazor WASMLet‚Äôs start with basic principles. When a client (frontend) wants to gain access to remote services it asks Keycloak to get an access token it can use to invoke other remote services on behalf of the user. Keycloak authenticates the user and then asks the user for consent to grant access to the client requesting it. The client then receives the access token. This access token is digitally signed by the realm. The client can make HTTP invocations on remote services using this access token. The Web API extracts the access token, verifies the signature of the token, then decides based on access information within the token whether to process the request.To get an access token securely, we need to consider various characteristics of an application performing the action. There are several different flows in the OAuth2 protocol. But for public clients (clients that can‚Äôt store secrets securely, e.g.:  Native apps/SPAs) the current recommended flow is ‚ÄúAuthorization Code Flow with PKCE‚Äù. This flow is an extension of the ‚ÄúAuthorization Code Flow‚Äù. Proof Key for Code Exchange (abbreviated PKCE, pronounced ‚Äúpixie‚Äù) prevents CSRF and authorization code injection attacks. The technique involves the client first creating a secret on each authorization request, and then using that secret again when exchanging the authorization code for an access token. This way if the code is intercepted, it will not be useful since the token request relies on the initial secret. However PKCE is not a replacement for a client secret, and PKCE is recommended even if a client is using a client secret since apps with a client secret are still susceptible to authorization code injection attacks.See: https://auth0.com/docs/get-started/authentication-and-authorization-flow/authorization-code-flow-with-proof-key-for-code-exchange-pkce Integrate with Keycloak from the frontend. OverviewBlazor uses the existing ASP.NET Core authentication mechanisms to establish the user‚Äôs identity. The exact mechanism depends on how the Blazor app is hosted, Blazor WebAssembly or Blazor Server.Blazor WebAssembly supports authenticating and authorizing apps using OIDC via the Microsoft.AspNetCore.Components.WebAssembly.Authentication library. The library provides a set of primitives for seamlessly authenticating against ASP.NET Core backends. The authentication support in Blazor WebAssembly is built on top of the oidc-client.js library, which is used to handle the underlying authentication protocol details.  Other options for authenticating SPAs exist, such as the use of SameSite cookies. However, the engineering design of Blazor WebAssembly is settled on OAuth and OIDC as the best option for authentication in Blazor WebAssembly apps. Token-based authentication based on JSON Web Tokens (JWTs) was chosen over cookie-based authentication for functional and security reasonsIn broad terms, authentication works as follows:  When an anonymous user selects the login button or requests a page with the [Authorize] attribute applied, the user is redirected to the app‚Äôs login page (/authentication/login).  On the login page, the authentication library prepares for a redirect to the authorization endpoint. The authorization endpoint is outside of the Blazor WebAssembly app and can be hosted at a separate origin. The endpoint is responsible for determining whether the user is authenticated and for issuing one or more tokens in response. The authentication library provides a login callback to receive the authentication response.  When the Blazor WebAssembly app loads the login callback endpoint (/authentication/login-callback), the authentication response is processed.Authentication componentThe Authentication component handles remote authentication operations and permits the app to:  Configure app routes for authentication states.  Set UI content for authentication states.  Manage authentication state.@page \"/authentication/{action}\"@using Microsoft.AspNetCore.Components.WebAssembly.Authentication&lt;RemoteAuthenticatorView Action=\"@Action\" /&gt;@code{    [Parameter] public string? Action { get; set; }}The Index page (wwwroot/index.html) page includes a script that defines the AuthenticationService in JavaScript. AuthenticationService handles the low-level details of the OIDC protocol. The app internally calls methods defined in the script to perform the authentication operations.&lt;script src=\"_content/Microsoft.AspNetCore.Components.WebAssembly.Authentication/AuthenticationService.js\"&gt;&lt;/script&gt;For more details and full instructions, please follow https://learn.microsoft.com/en-us/aspnet/core/blazor/security/webassembly/standalone-with-authentication-library.App Component//App.razor&lt;CascadingAuthenticationState&gt;    &lt;Router AppAssembly=\"@typeof(App).Assembly\"&gt;        &lt;Found Context=\"routeData\"&gt;            &lt;AuthorizeRouteView RouteData=\"@routeData\" DefaultLayout=\"@typeof(MainLayout)\"&gt;                &lt;NotAuthorized&gt;                    @if (context.User.Identity?.IsAuthenticated != true)                    {                        &lt;RedirectToLogin /&gt;                    }                    else                    {                        &lt;p role=\"alert\"&gt;You are not authorized to access this resource.&lt;/p&gt;                    }                &lt;/NotAuthorized&gt;            &lt;/AuthorizeRouteView&gt;            &lt;FocusOnNavigate RouteData=\"@routeData\" Selector=\"h1\" /&gt;        &lt;/Found&gt;        &lt;NotFound&gt;            &lt;PageTitle&gt;Not found&lt;/PageTitle&gt;            &lt;LayoutView Layout=\"@typeof(MainLayout)\"&gt;                &lt;p role=\"alert\"&gt;Sorry, there's nothing at this address.&lt;/p&gt;            &lt;/LayoutView&gt;        &lt;/NotFound&gt;    &lt;/Router&gt;&lt;/CascadingAuthenticationState&gt;  The CascadingAuthenticationState component manages exposing the AuthenticationState to the rest of the app.  The AuthorizeRouteView component makes sure that the current user is authorized to access a given page or otherwise renders the RedirectToLogin component.  The RedirectToLogin component manages redirecting unauthorized users to the login page.// RedirectToLogin.razor@inject NavigationManager Navigation@code {    protected override void OnInitialized()    {        Navigation.NavigateTo($\"authentication/login?returnUrl={Uri.EscapeDataString(Navigation.Uri)}\");    }}Configure AuthenticationServiceSupport for authenticating users is registered in the service container with the AddOidcAuthentication extension method provided by the Microsoft.AspNetCore.Components.WebAssembly.Authentication package. This method sets up the services required for the app to interact with the Identity Provider (IP).The code below should be pretty self-explanatory:// Program.csvar builder = WebAssemblyHostBuilder.CreateDefault(args);builder.Services.AddOidcAuthentication(options =&gt;{    options.ProviderOptions.MetadataUrl = \"http://localhost:8080/realms/Test/.well-known/openid-configuration\";    options.ProviderOptions.Authority = \"http://localhost:8080/realms/Test\";    options.ProviderOptions.ClientId = \"test-client\";    options.ProviderOptions.ResponseType = \"id_token token\";    options.UserOptions.NameClaim = \"preferred_username\";    options.UserOptions.RoleClaim = \"roles\";    options.UserOptions.ScopeClaim = \"scope\";});Now, we want to be able to use ‚Äúclient roles‚Äù as roles in ASP.NET Core Identity in Blazor WASM. To do that we need to create an additional mapper.  On the left side bar click on ‚ÄúClients‚Äù item.  Click ‚Äútest-client‚Äù  Open ‚ÄúClient scopes‚Äù tab  Click on ‚Äútest-client-dedicated‚Äù, should be on top of the list of scopes  From the ‚ÄúMappers‚Äù tab, click ‚ÄúCreate a new mapper‚Äù          specify Name: test-client-roles      specify Token Claim Name: roles        Click ‚ÄúSave‚Äù  Open ‚ÄúClient scopes‚Äù tab  Click ‚ÄúCreate role‚Äù          specify Role name: User       To assign a client role to a user:  On the left side bar click on ‚ÄúUsers‚Äù item.  Select a ‚Äúuser‚Äù from the list  Open ‚ÄúRole mapping‚Äù tab  Click ‚ÄúAssign role‚Äù  Search for ‚ÄúUser‚Äù and click ‚ÄúAssign‚ÄùHere is how a trimmed version of an access token looks like:{  \"realm_access\": {    \"roles\": [      \"default-roles-test\",      \"offline_access\",      \"uma_authorization\"    ]  },  \"resource_access\": {    \"test-client\": {      \"roles\": [        \"User\"      ]    },    \"account\": {      \"roles\": [        \"manage-account\",        \"manage-account-links\",        \"view-profile\"      ]    }  },  \"scope\": \"openid profile email\",  \"sid\": \"5bec6b83-d4e1-4bb9-84df-4a23292f5247\",  \"email_verified\": false,  \"roles\": [    \"User\"  ],  \"name\": \"John Doe\",  \"preferred_username\": \"user\",  \"given_name\": \"John\",  \"family_name\": \"Doe\",  \"email\": \"user@nikiforovall.com\"}Note, the ‚Äútest-client‚Äù roles are duplicated into separate claim that can be used by ASP.NET Identity options.UserOptions.RoleClaim = \"roles\".DemoFrom ‚ÄúHome‚Äù page click ‚ÄúFetch Data‚Äù tab. You will be presented with the next error: As you might have already guessed, we need to specify Blazor WASM application URL as valid in order for Keycloak to trustfully redirect access tokens to it.  On the left side bar click on ‚ÄúClients‚Äù item.  Click ‚Äútest-client‚Äù  Open ‚ÄúSettings‚Äù tab  Add frontend URL https://localhost:7126/* to ‚ÄúValid redirect URIs‚Äù  Click ‚ÄúSave‚ÄùNow, we you can try again. This time you might see the next expected error: If you check the response from the backend, you will see the status 401 (Unauthorized). The problem is that you need to somehow propagate an access token from the frontend to the backend.Luckily, it is quite easy to do by using built-in HTTP Client middleware. The code below shows how to add BaseAddressAuthorizationMessageHandler to the default HttpClient used throughout the application.static void RegisterHttpClient(WebAssemblyHostBuilder builder, IServiceCollection services){    var httpClientName = \"Default\";    var baseAddress = new Uri(builder.HostEnvironment.BaseAddress);    services.AddHttpClient(httpClientName, client =&gt; client.BaseAddress = baseAddress)        .AddHttpMessageHandler&lt;BaseAddressAuthorizationMessageHandler&gt;();    services.AddScoped(sp =&gt; sp.GetRequiredService&lt;IHttpClientFactory&gt;().CreateClient(httpClientName));} üéâ Hooray. We have successfully integrated Keycloak with Blazor WebAssembly application.  ‚ùó Note, the instructions above are not intended to be a step-by-step guide, please consult the source code for more details.Reference  https://www.keycloak.org/docs/latest/securing_apps/index.html  https://www.keycloak.org/docs/latest/authorization_services/index.html  https://auth0.com/docs/get-started/authentication-and-authorization-flow  https://learn.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-auth-code-flow  https://learn.microsoft.com/en-us/azure/active-directory/develop/v2-protocols-oidc  https://www.oauth.com/oauth2-servers/pkce/  https://learn.microsoft.com/en-us/aspnet/core/blazor/security  https://learn.microsoft.com/en-us/aspnet/core/blazor/security/webassembly  https://learn.microsoft.com/en-us/aspnet/core/blazor/security/webassembly/standalone-with-authentication-library  https://github.com/NikiforovAll/keycloak-authorization-services-dotnet"
    },
  
    {
      "id": "56",
      "title": "Add persisted parameters to CLI applications in .NET",
      "url": "/dotnet/2022/08/26/persisted-parameters-in-dotnet-cli.html",
      "date": "August 26, 2022",
      "categories": ["dotnet"],
      "tags": ["dotnet","console","cli"],
      "shortinfo": "Learn how to use System.CommandLine to implement persisted parameters in CLI/console applications.",
      "content": "  TL;DR  Introduction  Implement configuration storage          Test configuration commands        Use configuration in business logic  Persisted parameters          Demo        Summary  ReferenceTL;DRSee how to implement persisted parameters feature in a CLI/console application. For example, Azure CLI offers persisted parameters that enable you to store parameter values for continued use. You will learn how to use System.CommandLine. It provides building blocks that make functionality composable and reusable.Source code: https://github.com/NikiforovAll/cli-persistent-parameters-exampleIntroductionPersisted parameters improve the overall developer experience. Since the previous values are stored and are ready to be re-used during the next command run. It is easy to understand the benefit of this feature by looking at how az does it:# Turn persisted parameters on.az config param-persist on# Create a resource group.az group create --name RGName --location westeurope# Create an Azure storage account in the resource group omitting \"--location\" and \"--resource-group\" parameters.az storage account create \\  --name sa3fortutorial \\  --sku Standard_LRSAs you might assume, there is some sort of state involved. So before we take a look at persisted parameters implementation let‚Äôs look at how to implement a configuration in general.Both Azure CLI and AWS CLI utilize a dedicated file created in a well-known directory. The format of the file is intended to be lightweight and human-readable. Ini markup language will do.For example:  ~/.azure/config is used by Azure CLI  ~/.aws/config and ~/.aws/credentials are used by AWS CLIHere is what ~/.azure/config can look like:‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ       ‚îÇ File: /home/oleksii_nikiforov/.azure/config‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   1   ‚îÇ [cloud]   2   ‚îÇ name = AzureCloud   3   ‚îÇ    4   ‚îÇ [core]   5   ‚îÇ first_run = yes   6   ‚îÇ output = jsonc   7   ‚îÇ only_show_errors = false   8   ‚îÇ error_recommendation = on   9   ‚îÇ no_color = True  10   ‚îÇ disable_progress_bar = false  11   ‚îÇ collect_telemetry = no‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄImplement configuration storageMy goal is to replicate the behavior of az CLI and make the implementation reusable, so you can just grab the code and add it to your application.Let‚Äôs look at commands provided by az CLI to work with the configuration.&gt; az config -hGroup    az config : Manage Azure CLI configuration.        Available since Azure CLI 2.10.0.        WARNING: This command group is experimental and under development. Reference and support        levels: https://aka.ms/CLI_refstatusSubgroups:    param-persist : Manage parameter persistence.Commands:    get           : Get a configuration.    set           : Set a configuration.    unset         : Unset a configuration.At the high level, the structure could be defined as follows:// Program.csvar root = new RootCommand();root.Name = \"clistore\";root.AddConfigCommands();// ConfigCommands.cspublic static RootCommand AddConfigCommands(this RootCommand root){    var command = new Command(\"config\", \"Manage CLI configuration\");    command.AddCommand(BuildGetCommand());    command.AddCommand(BuildSetCommand());    command.AddCommand(BuildUnsetCommand());    root.AddCommand(command);    return root;}private static Command BuildGetCommand(){    var get = new Command(\"get\", \"Get a configuration\");    var getpath = new Argument&lt;string?&gt;(        \"key\",        () =&gt; default,        @\"The configuration to get. If not provided, all sections and configurationswill be listed. If `section` is provided, all configurations under thespecified section will be listed. If `&lt;section&gt;.&lt;key&gt;` is provided, onlythe corresponding configuration is shown.\");    get.AddArgument(getpath);    return get;}private static Command BuildSetCommand(CliConfigurationProvider configurationProvider){    var set = new Command(\"set\", \"Set a configuration\");    var setpath = new Argument&lt;string[]&gt;(        \"key\",        \"Space-separated configurations in the form of &lt;section&gt;.&lt;key&gt;=&lt;value&gt;.\");    set.AddArgument(setpath);    return set;}private static Command BuildUnsetCommand(CliConfigurationProvider configurationProvider){    var unset = new Command(\"unset\", \"Unset a configuration\");    var unsetpath = new Argument&lt;string[]&gt;(        \"key\",        \"The configuration to unset, in the form of &lt;section&gt;.&lt;key&gt;.\");    unset.AddArgument(unsetpath);    return unset;}Now, we want the actual code that handles commands. But before that, we need to grab the configuration from someplace. In my opinion, we already have the required and de-facto standard way of working with configuration namely - IConfiguration. The only this is left is to use Dependency Injection capabilities of System.CommandLine and define BinderBase&lt;IConfiguration&gt;. See https://docs.microsoft.com/en-us/dotnet/standard/commandline/dependency-injection for more details.public class CliConfigurationProvider : BinderBase&lt;IConfiguration&gt;{    public static CliConfigurationProvider Create(string storeName = \"clistore\") =&gt;        new(storeName);    public CliConfigurationProvider(string storeName) =&gt; StoreName = storeName;    public string StoreName { get; }    public string ConfigLocationDir =&gt; Path.Combine(        Environment.GetFolderPath(Environment.SpecialFolder.UserProfile),        $\".{StoreName.TrimStart('.')}\");    public string ConfigLocation =&gt; Path.Combine(ConfigLocationDir, \"config\");    protected override IConfiguration GetBoundValue(BindingContext bindingContext) =&gt; GetConfiguration();    public IConfiguration GetConfiguration()    {        var configuration = new ConfigurationBuilder()            .AddIniFile(ConfigLocation, optional: true)            .AddEnvironmentVariables(StoreName.ToUpperInvariant())            .Build();        return configuration;    }}As you may notice, we are using ConfigurationBuilder to compose a configuration not only from a configuration file but also from prefix environment variables. As result, we can override parameters, e.g.: CLISTORE_MyConfigKey.The config get command handler. Note, the IConfiguration is injected as parameter:var get = new Command(\"get\", \"Get a configuration\");var getpath = new Argument&lt;string?&gt;(\"key\");get.AddArgument(getpath);get.SetHandler((string? path, IConfiguration configuration) =&gt;{    var output = new Dictionary&lt;string, object[]&gt;();    foreach (var config in configuration.GetChildren())    {        output[config.Key] = config.GetChildren()            .Select(x =&gt; new { Name = x.Key, x.Value })            .ToArray();    }    if (output.Any())    {        Console.WriteLine(JsonSerializer.Serialize(output, new JsonSerializerOptions()        {            WriteIndented = true,            PropertyNamingPolicy = JsonNamingPolicy.CamelCase,        }));    }    return Task.CompletedTask;}, getpath, CliConfigurationProvider.Create(root.Name));return get;On the write side, we need to be able to manipulate config file. I will skip parsing details. Basically, we load the file from known location and can call Save(path) when we are ready.public class CliConfigurationProvider : BinderBase&lt;IConfiguration&gt;{    // ... skipped    public IniFile LoadIniFile()    {        var ini = new IniFile();        Directory.CreateDirectory(ConfigLocationDir);        if (File.Exists(ConfigLocation))        {            ini.Load(ConfigLocation);        }        return ini;    }}The config set command handler.var configurationProvider = CliConfigurationProvider.Create(root.Name);var set = new Command(\"set\", \"Set a configuration\");var setpath = new Argument&lt;string[]&gt;(\"key\");set.AddArgument(setpath);set.SetHandler((string[] path) =&gt;{    var ini = configurationProvider.LoadIniFile();    foreach (var p in path)    {        var keyvalue = p.Split('=');        var (key, value) = (keyvalue[0], keyvalue[^1]);        var sectionKey = key[..key.IndexOf('.')];        var configKey = key[(key.IndexOf('.') + 1)..];        ini[sectionKey][configKey] = value;    }    ini.Save(configurationProvider.ConfigLocation);    return Task.CompletedTask;}, setpath);return set;Test configuration commandsWe can use Verify to perform snapshot testing and check for the correct output of the program. In order to make things easier and simplify working with process output capturing and invocation, I used CliWrap.Here are some tests. I included *.verfied.txt content as comments. For more details please refer to Tests.[UsesVerify]public class StoreCommands_Specs{    // kinda ugly, but I can live with it    private const string relativeSourcePath = \"../../../../../src\";    public StoreCommands_Specs()    {        // cleanup, runs every test, concurrent test execution is disabled        EnsureDeletedConfigFolder();    }    [Fact]    public async Task Help_text_is_displayed_for_config()    {        var stdOutBuffer = Execute(\"config\", \"--help\");        await Verify(stdOutBuffer.ToString());    }    // Description:    // Manage CLI configuration    // Usage:    // clistore config [command] [options]    // Options:    // -?, -h, --help  Show help and usage information    // Commands:    // get &lt;key&gt;    Get a configuration []    // set &lt;key&gt;    Set a configuration    // unset &lt;key&gt;  Unset a configuration    [Fact]    public async Task Get_config_is_performed_on_populated_config()    {        Execute(\"config\", \"set\", \"core.target=my_value\");        Execute(\"config\", \"set\", \"core.is_populated=true\");        Execute(\"config\", \"set\", \"extra.another_section=false\");        var stdOutBuffer = Execute(\"config\", \"get\");        await Verify(stdOutBuffer.ToString());    }    // {    // \"core\": [    //     {    //     \"name\": \"is_populated\",    //     \"value\": \"true\"    //     },    //     {    //     \"name\": \"target\",    //     \"value\": \"my_value\"    //     }    // ],    // \"extra\": [    //     {    //     \"name\": \"another_section\",    //     \"value\": \"false\"    //     }    // ]    // }    private static (CommandResult, StringBuilder, StringBuilder) Execute(params string[] command)    {        var stdOutBuffer = new StringBuilder();        var stdErrBuffer = new StringBuilder();        var result = Cli.Wrap(\"dotnet\")            .WithStandardOutputPipe(PipeTarget.ToStringBuilder(stdOutBuffer))            .WithStandardErrorPipe(PipeTarget.ToStringBuilder(stdErrBuffer))            .WithArguments(args =&gt; args                .Add(\"run\")                .Add(\"--project\")                .Add(relativeSourcePath)                .Add(\"--\")                .Add(command))            .WithValidation(CommandResultValidation.None)            .ExecuteAsync().ConfigureAwait(false).GetAwaiter().GetResult();        return stdOutBuffer;    }    private static void EnsureDeletedConfigFolder()    {        var path = Path.Combine(Environment.GetFolderPath(Environment.SpecialFolder.UserProfile),            \".clistore\",            \"config\");        if (File.Exists(path))        {            File.Delete(path);        }    }}Here is the content of ~/.clistore/config file after test run:       ‚îÇ File: /home/oleksii_nikiforov/.clistore/config‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   1   ‚îÇ [core]   2   ‚îÇ target=my_value   3   ‚îÇ is_populated=true   4   ‚îÇ    5   ‚îÇ [extra]   6   ‚îÇ another_section=false   7   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄUse configuration in business logicLet‚Äôs say we want to print out ‚ÄúHello, {target}‚Äù and take {target} as an option/argument. If an option is not provided we should check configuration storage for default values.The first approach is to directly use BinderBase&lt;IConfiguration&gt; and read from IConfiguration.// Program.csharproot.AddConfigCommands(out var configProvider);root.AddCommand(GreetFromConfigCommand(configProvider));// GreetCommandsFactory.csstatic void Greet(string target) =&gt; Console.WriteLine($\"Hello, {target}\");public static Command GreetFromConfigCommand(CliConfigurationProvider configProvider){    var command = new Command(\"greet-from-config\", \"Demonstrates how to use IConfiguration from DI container\");    var targetOption = new Option&lt;string?&gt;(\"--target\");    targetOption.IsRequired = false;    command.AddOption(targetOption);    command.SetHandler((string? target, IConfiguration configuration) =&gt;        Greet(target ?? configuration[\"core:target\"]), targetOption, configProvider);    return command;}The second one is to provide the default value factory getDefaultValue. Note, it might slow down built-in suggestions:// Program.csharproot.AddConfigCommands(out var configProvider);root.AddCommand(GreetFromDefaultValueCommand(configProvider));// GreetCommandsFactory.cspublic static Command GreetFromDefaultValueCommand(CliConfigurationProvider configProvider){    var command = new Command(\"greet-from-default-value\", \"Demonstrates how to provide default value to an option\");    var targetOptionWithDefault = new Option&lt;string&gt;(\"--target\", getDefaultValue: () =&gt;    {        // note, this is evaluate preemptively which may slow down autocompletion        var configuration = configProvider.GetConfiguration();        return configuration[\"core:target\"];    });    command.AddOption(targetOptionWithDefault);    command.SetHandler((string target) =&gt; Greet(target), targetOptionWithDefault);    return command;}The benefit of the approach above is that you don‚Äôt need to worry about configuration in the handler code.And handler could be simplified to command.SetHandler(Greet, targetOptionWithDefault) by using C# method groups.The tests:[Fact]public async Task GreetFromConfig_greets_with_populated_target_config(){    Execute(\"config\", \"set\", \"core.target=World\");    var stdOutBuffer = Execute(\"greet-from-config\");    await Verify(stdOutBuffer.ToString());}// Hello, World[Fact]public async Task GreetFromDefaultValue_greets_with_default_value_from_populated_target_config(){    Execute(\"config\", \"set\", \"core.target=World\");    var stdOutBuffer = Execute(\"greet-from-default-value\");    await Verify(stdOutBuffer.ToString());}// Hello, WorldPersisted parametersNow, we have a good grasp of how to implement stateful CLI applications. We can see how to add the persistent parameters feature. Let‚Äôs start with expected behavior and the corresponding test:[Fact]public async Task GreetFromPersisted_greets_with_persisted_params_value(){    Execute(\"greet-from-persisted\", \"--target\", \"World\");    var stdOutBuffer = Execute(\"greet-from-persisted\");    await Verify(stdOutBuffer.ToString());}// Hello, WorldThe --target parameter is stored as a result of a previous successful invocation. We can organize a dedicated section in configuration to store previously used values.To define persisted options we can use BinderBase&lt;T&gt; as a decorator to Option&lt;T&gt;. Like following:public class PersistedOptionProvider&lt;T&gt; : BinderBase&lt;T?&gt;{    // skipped     protected override T? GetBoundValue(BindingContext bindingContext)    {        if (!bindingContext.ParseResult.HasOption(_option))        {            var ini = _configProvider.LoadIniFile();            string text = ini[CliConfigurationProvider.PersistedParamsSection][_option.Name].ToString();            var value = (T)TypeDescriptor.GetConverter(typeof(T))                .ConvertFromString(text)!;            return value;        }        return bindingContext.ParseResult.GetValueForOption(_option);    }}So, if option is not provided, we check the config file for a potential fallback value.Here is the handler for the persisted parameters greeter variation:public static Command GreetFromPersistedCommand(CliConfigurationProvider configProvider){    var command = new Command(\"greet-from-persisted\", \"Demonstrates how to use persisted parameters\");    var targetOption = new Option&lt;string&gt;(\"--target\");    targetOption.IsRequired = false;    command.AddOption(targetOption);    command.SetHandler(Greet, new PersistedOptionProvider&lt;string&gt;(targetOption, configProvider));    return command;}That solves the read side of the task. What about the write side? We want to store options upon successful completion after all.Thankfully, System.CommandLine provides a handy abstraction - middleware. We can use it on CommandLineBuilder.AddMiddleware.var root = new RootCommand();root.Name = \"clistore\";var commandLineBuilder = new CommandLineBuilder(root);commandLineBuilder.AddMiddleware(async (context, next) =&gt; {/*implementation goes here*/});commandLineBuilder.UseDefaults();var parser = commandLineBuilder.Build();await parser.InvokeAsync(args);It fairly common practice to pack middleware registration in extension methods. Here is the how to write persisted parameters by using middleware approach.///&lt;summary&gt;/// Stores provided options registered with CliConfigurationProvider.RegisterPersistedOption///&lt;/summary&gt;public static CommandLineBuilder AddPersistedParametersMiddleware(    this CommandLineBuilder builder, CliConfigurationProvider configProvider){    return builder.AddMiddleware(async (context, next) =&gt;    {        Lazy&lt;IniFile&gt; config = new(() =&gt; configProvider.LoadIniFile());        var parseResult = context.ParseResult;        await next(context);        bool newValuesAdded = false;        foreach (var option in configProvider.PersistedOptions)        {            if (parseResult.HasOption(option))            {                config.Value[CliConfigurationProvider.PersistedParamsSection][option.Name] =                    parseResult.GetValueForOption(option)?.ToString();                newValuesAdded = true;            }        }        if (newValuesAdded)        {            config.Value.Save(configProvider.ConfigLocation);        }    });}Demo17:32 $ dotnet run -- -hDescription:  Shows proof of concept of how to store persistent configuration in a CLI appsUsage:  clistore [command] [options]Options:  --version       Show version information  -?, -h, --help  Show help and usage informationCommands:  config                    Manage CLI configuration  greet-from-config         Demonstrates how to use IConfiguration from DI container  greet-from-default-value  Demonstrates how to provide default value to an option  greet-from-persisted      Demonstrates how to use persisted parameters‚úò-1 ~/projects/configuration-builder/src [main|‚úî] 17:32 $ dotnet run -- greet-from-persisted --target fooHello, foo‚úò-1 ~/projects/configuration-builder/src [main|‚úî] 17:32 $ dotnet run -- greet-from-persisted Hello, foo‚úò-1 ~/projects/configuration-builder/src [main|‚úî] 17:32 $ dotnet run -- greet-from-persisted --target barHello, barSummaryI‚Äôve shown you how to use System.CommandLine and how to implement persisted parameters feature in your CLI/console application. The suggested code is not production-ready but is a decent starting point.Reference  https://docs.microsoft.com/en-us/dotnet/standard/commandline/  https://docs.microsoft.com/en-us/cli/azure/param-persist-tutorial?tabs=azure-cli  https://github.com/VerifyTests/Verify  https://github.com/Tyrrrz/CliWrap"
    },
  
    {
      "id": "57",
      "title": "Use Keycloak as Identity Provider in ASP.NET Core 6",
      "url": "/aspnetcore/dotnet/2022/08/24/dotnet-keycloak-auth.html",
      "date": "August 24, 2022",
      "categories": ["aspnetcore","dotnet"],
      "tags": ["aspnetcore","dotnet","auth","keycloak"],
      "shortinfo": "Learn how to add authentication to ASP.NET Core 6 via Keycloak.",
      "content": "  TL;DR  Introduction to Keycloak  Ideas behind Keycloak.AuthServices.Authentication          Configure KeycloakAuthenticationOptions        Demo  Bonus - Adding Authorization  SummaryTL;DRLearn how to use Keycloak in ASP.NET Core 6 by using Keycloak.AuthServices.Authentication.Source code: https://github.com/NikiforovAll/keycloak-authorization-services-dotnet/blob/main/samples/AuthGettingStarted/Program.csIntroduction to Keycloak  Keycloak - Open Source Identity and Access Management. Add authentication to applications and secure services with minimum effort. No need to deal with storing users or authenticating users. Keycloak provides user federation, strong authentication, user management, fine-grained authorization, and more.Personally, I‚Äôm a big fan of Keycloak because it is a mature and full-fledged identity and access management system. It contains everything you might need for most of the scenarios. Contrary to IdentityServer it is easy to use out-of-the-box with no code required to get things going. If Azure Active Directory is not an option for some reason I definitely consider Keycloak as a viable open-source option that could be installed on-prem.Features:  Single-Sign On  Identity Brokering and Social Login  User Federation  Admin Console  Account Management Console  Authorization ServicesSee: https://www.keycloak.org/Ideas behind Keycloak.AuthServices.AuthenticationKeycloak is OAuth2 + OpenID Connect compliant provider so it should be easy to use it. Although, it takes some time to may Keycloak concepts and configuration to ASP.NET Core Authentication configuration model. That is the way I‚Äôve prepared a convenience library to speed up the integration process and make using Keycloak in .NET world more enjoyable.Basically, we use AddKeycloakAuthentication to register and configure JwtBearerDefaults.AuthenticationScheme authentication scheme and provide KeycloakAuthenticationOptions./// &lt;summary&gt;/// Adds keycloak authentication services./// &lt;/summary&gt;public static AuthenticationBuilder AddKeycloakAuthentication(    this IServiceCollection services,    KeycloakAuthenticationOptions keycloakOptions,    Action&lt;JwtBearerOptions&gt;? configureOptions = default) {/*...*/}flowchart LR    AddKeycloakAuthentication[\"AddKeycloakAuthentication\"]    AddAuthentication[\"AddAuthentication\"]    AddKeycloakAuthentication--&gt;|JwtBearerDefaults.AuthenticationScheme|AddAuthentication    AddAuthentication--&gt;AddJwtBearer    AddKeycloakAuthentication --&gt; |IClaimsTransformation|KeycloakRolesClaimsTransformation    subgraph JwtBearer        direction LR        AddJwtBearer --&gt;        TokenValidationParameters --&gt; NameClaimType        TokenValidationParameters --&gt; RoleClaimType    endThe overall project structure looks like this:var builder = WebApplication.CreateBuilder(args);var services = builder.Services;services.AddKeycloakAuthentication(authenticationOptions);var app = builder.Build();app.UseAuthentication();app.UseAuthorization();app.MapGet(\"/\", (ClaimsPrincipal user) =&gt;{    app.Logger.LogInformation(user.Identity.Name);}).RequireAuthorization();app.Run();Configure KeycloakAuthenticationOptionsThe are various ways to configure Keycloak authentication. Later, I will show you how to use Keycloak OIDC client adapter seamlessly.1Ô∏è‚É£ Construct objectvar authenticationOptions = new KeycloakAuthenticationOptions{    AuthServerUrl = \"http://localhost:8088/\",    Realm = \"Test\",    Resource = \"test-client\",};services.AddKeycloakAuthentication(authenticationOptions);2Ô∏è‚É£ Read manually from configurationvar configuration = builder.Configuration;var authenticationOptions = configuration    .GetSection(KeycloakAuthenticationOptions.Section)    .Get&lt;KeycloakAuthenticationOptions&gt;();services.AddKeycloakAuthentication(authenticationOptions);3Ô∏è‚É£ From configurationvar configuration = builder.Configuration;services.AddKeycloakAuthentication(configuration, KeycloakAuthenticationOptions.Section);// section is optional KeycloakAuthenticationOptions.Section = \"Keycloak\"services.AddKeycloakAuthentication(configuration);4Ô∏è‚É£ From the configuration file  keycloak.json file used by the Keycloak OIDC client adapter to configure clients. You may also want to tweak this file after you download it.var configuration = builder.Configuration;var host = builder.Host;host.ConfigureKeycloakConfigurationSource(\"keycloak.json\"); // file name is optionalservices.AddKeycloakAuthentication(configuration);DemoBefore we start integrating we need to run an instance of Keycloak on a dev machine.docker run -p 8080:8080 \\    -e KEYCLOAK_ADMIN=admin -e KEYCLOAK_ADMIN_PASSWORD=admin \\    quay.io/keycloak/keycloak:19.0.1 start-devPlease refer to https://www.keycloak.org/getting-started/getting-started-docker for more details.Let‚Äôs create our first realm.  Open the Keycloak Admin Console  Hover the mouse over the dropdown in the top-left corner where it says Master, then click on Add realm  Fill in the form with the following values:          Name: Test        Click Create    Create a userInitially, there are no users in a new realm, so let‚Äôs create one:  Open the Keycloak Admin Console  Click Users (left-hand menu)  Click Add user (top-right corner of table)  Fill in the form with the following values:          Username: test@test.com      First Name: Your first name      Last Name: Your last name        Click Save    Let‚Äôs try to secure our first application. The first step is to register this application with your Keycloak instance:  Open the Keycloak Admin Console  Click Clients (left-hand menu)  Fill in the form with the following values:          Client Type: ‚ÄúOpenID Connect‚Äù      Client Id: test-client        Click Save    As mentioned above, Keycloak has a concept of adaptor config. It allows us to copy essential configurations. Keycloak.AuthServices.Authentication is deliberately designed to streamline the installation process, so the KeycloakAuthenticationOptions mimics the structure of the adapter config.Navigate the newly created client (top-right) and click ‚ÄúAction&gt;Download adapter config‚Äù    Here is what it looks like:{  \"realm\": \"Test\",  \"auth-server-url\": \"http://localhost:8080/\",  \"ssl-required\": \"external\",  \"resource\": \"test-client\",  \"public-client\": true,  \"verify-token-audience\": true,  \"use-resource-role-mappings\": true,  \"confidential-port\": 0}üí° Generally, you want to use protocol mapper to configure the required audience (‚Äúresource‚Äù in the config above). To simplify the demo, we can just turn off Audience validation. Change ‚Äúverify-token-audience‚Äù: false. See https://dbp-demo.tugraz.at/handbook/relay/keycloak/keycloak_audience/ and https://stackoverflow.com/a/53627747/8168625 for more details.üí° Tokens should be exchanged based on HTTPS, but for the developer environment, you can use HTTP. Change ‚Äússl-required‚Äù: ‚Äúnone‚ÄùNow, we can create keycloak.json or extend appsettings.json.{  \"Logging\": {    \"LogLevel\": {      \"Default\": \"Information\",      \"Microsoft.AspNetCore\": \"Warning\"    }  },  \"Keycloak\": {    \"realm\": \"Test\",    \"auth-server-url\": \"http://localhost:8080/\",    \"ssl-required\": \"none\",    \"resource\": \"test-client\",    \"verify-token-audience\": false    \"confidential-port\": 0  }}To use provided configuration, simply register AddKeycloakAuthenticationvar builder = WebApplication.CreateBuilder(args);var services = builder.Services;var configuration = builder.Configuration;var host = builder.Host;host.ConfigureLogger();services.AddEndpointsApiExplorer().AddSwagger();services.AddKeycloakAuthentication(configuration);var app = builder.Build();appv.UseSwagger().UseSwaggerUI();app.UseAuthentication();app.UseAuthorization();app.MapGet(\"/\", (ClaimsPrincipal user) =&gt;{    app.Logger.LogInformation(user.Identity.Name);}).RequireAuthorization();app.Run();Now we need to obtain an access token and make a call via swagger-ui. There are various ways you can do it. In my case, I opened ‚Äútest-client‚Äù and made it confidential, and enabled ‚ÄúDirect access grants‚Äù. This way we can use a username and password. Please make sure you understand how to configure OAuth 2.0 and OpenId Connect before moving to production. In my case, it is just easy to demonstrate.    curl --data \"grant_type=password&amp;client_id=test-client&amp;username=test&amp;password=test&amp;client_secret=Tgx4lvbyhho7oNFmiIupDRVA8ioQY7PW\" \\    localhost:8080/realms/Test/protocol/openid-connect/token    Now, we can navigate swagger https://localhost:5001/swagger and make an authentication request by providing an access token.    Bonus - Adding AuthorizationLet‚Äôs see how we can use Keycloak.AuthServices.Authorization to build basic Keycloak-aware policies. See: AuthorizationPolicyüí° Keycloak has a concept of roles. I added realm and resource roles behind the scenes. I suggest you figure it out on your own as an exercise üòõ.In the example below we require a user to have ‚Äúadmin‚Äù realm role and ‚Äúr-admin‚Äù resource role by using RequireRealmRoles and RequireResourceRoles respectively. Note, that resource roles are automatically mapped to Microsoft.AspNetCore.Authorization roles so b.RequireResourceRoles(\"r-admin\") is same as b.RequireRole(\"r-admin\").‚ö† The resource from KeycloakAuthenticationOptions is used as an Audience and as the key for role mapping.var builder = WebApplication.CreateBuilder(args);var services = builder.Services;var configuration = builder.Configuration;var host = builder.Host;host.ConfigureLogger();services.AddEndpointsApiExplorer().AddSwagger();services.AddKeycloakAuthentication(configuration);services.AddAuthorization(o =&gt; o.AddPolicy(\"IsAdmin\", b =&gt;{    b.RequireRealmRoles(\"admin\");    b.RequireResourceRoles(\"r-admin\"); // stands for \"resource admin\"    // resource roles are mapped to ASP.NET Core Identity roles    b.RequireRole(\"r-admin\"); }));services.AddKeycloakAuthorization(configuration);var app = builder.Build();app.UseSwagger().UseSwaggerUI();app.UseAuthentication();app.UseAuthorization();app.MapGet(\"/\", (ClaimsPrincipal user) =&gt;{    app.Logger.LogInformation(user.Identity.Name);}).RequireAuthorization(\"IsAdmin\");app.Run();Here is what the logs look like after a successful HTTP GET ‚Äú/‚Äù request.    SummaryYou can use Keycloak.AuthServices to integrate with Keycloak. Keycloak has tons of great features and thankfully we can benefit from the Java open-source world as .NET developers."
    },
  
    {
      "id": "58",
      "title": "Dev Environment as a Code (DEaaC) with DevContainers, Dotfiles, and GitHub Codespaces",
      "url": "/productivity/devcontainers/2022/08/13/deaac.html",
      "date": "August 13, 2022",
      "categories": ["productivity","devcontainers"],
      "tags": ["devcontainers","dotfiles","deaac"],
      "shortinfo": "Learn how to build a repeatable codespace configuration for all users of your project.",
      "content": "Table of Contents  Motivation  Anatomy of a dev environment          Dotfiles      DevContainers        A better way to do dotfiles - Dotbot  Combining everything together  Pick a host. GitHub Codespaces  Example  Summary  ReferenceMotivationCrafting a perfect dev environment takes time and effort. It is a tedious process that you have to do every time you change a working environment. For example, you buy a new laptop, switch to another operating system, or switch to a new project that requires extra tools and configuration. The latter case is interesting to me because I like to try out new things. Varying from different tools, and frameworks to new programming languages.There are lots of benefits to making a reproducible dev environment. For me the two main ones:  You open a project and it just works. Image some open-source project that interests you but you don‚Äôt know how to even run it. With DEaaC the installation problems go away.  You can share it with your peers. A codified developer setup is a form of knowledge that could be evaluated and evolved. It opens up tons of new opportunities.Anatomy of a dev environmentBasically, you can divide aspects of a dev environment into next categories:graph LR    dev[DevEnv]    dev--&gt;personal[Personal]    personal --&gt; pconfig[Configuration]    pconfig --&gt; shellconfig[Shell: aliases, prompts,...]    pconfig --&gt; editorconfig[Editor: extensions, customizations]    pconfig --&gt; gitconfig[Git]    personal--&gt;ptools[Tools]    ptools --&gt;pcli[command-line tools]    ptools --&gt;pgui[Graphical User Interface]    dev--&gt;project[Project]    project--&gt;pjconfig[Configuration]    project--&gt;projecttools[Tools]    pjconfig -.-&gt; pjshellconfig[Shell: aliases, prompts,...]    pjconfig --&gt; pjeditorconfig[Editor: extensions, customizations]    pjconfig -.-&gt; pjgitconfig[Git]    projecttools --&gt; pjcli[command-line tools]    projecttools--&gt;pjgui[Graphical User Interface]    style pjshellconfig stroke:grey,stroke-width:2px,stroke-dasharray: 5 5    style pjgitconfig stroke:grey,stroke-width:2px,stroke-dasharray: 5 5It might seem like a lot but luckily we have good tools to automate the scaffolding process. Namely dotfiles and devcontainers.DotfilesDotfiles are focused on personal configuration aspects and are usually maintained by a person. Dotfiles are tailored to make up a unique developer experience. Previously, I wrote about it here: Development environment based on Windows Terminal + WSL + ZSH + dotfilesUsually, dotfiles are comprised of common configurations and utilities that could be used on various different platforms and platform-specific configurations. For example, you use a specific package manager innate into the system to install dependencies.Here is an example of how I do it: https://github.com/NikiforovAll/dotfiles. As you can see, there are three different setups for windows, wsl, and dev-containers.As result, we can create a dev environment of choice from a scratch. All you need to do is to run one-liner:bash -c \"$(wget -qO - https://raw.github.com/nikiforovall/dotfiles/master/src/wsl/os/install.sh)\"An approach like this does work. But it is annoying and impractical. We can do better about it. I will show you in a moment.DevContainersFrom the official documentation:  The Visual Studio Code Remote - Containers extension lets you use a Docker container as a full-featured development environment. It allows you to open any folder or repository inside a container and take advantage of Visual Studio Code‚Äôs full feature set. A devcontainer.json file in your project tells VS Code how to access (or create) a development container with a well-defined tool and runtime stack. This container can be used to run an application or to separate tools, libraries, or runtimes needed for working with a codebase.  A devcontainer.json file in your project tells Visual Studio Code (and other services and tools that support the format) how to access (or create) a development container with a well-defined tool and runtime stack. It‚Äôs currently supported by the Remote - Containers extension and GitHub Codespaces.    DevContainers are meant to be reusable and extendable. From my experience, they have improved in this regard. Here is how you can add dependencies via .devconatiner.json{    \"name\": \"Azure Dev CLI\",    \"build\": {        \"dockerfile\": \"Dockerfile\", // base docker image        \"args\": {            \"VARIANT\": \"bullseye\" // parametrization        }    },    \"features\": {        \"github-cli\": \"2\",        \"azure-cli\": \"2.38\",        \"dotnet\": \"6.0\",        \"docker-from-docker\": \"20.10\",    },    \"extensions\": [        \"ms-azuretools.azure-dev\",        \"ms-azuretools.vscode-bicep\",        \"ms-azuretools.vscode-docker\",        \"ms-azuretools.vscode-azurefunctions\",        \"ms-dotnettools.csharp\",        \"ms-dotnettools.vscode-dotnet-runtime\",    ],}üí°DevContainers define project-specific configurations and dotfiles define user-specific configurations.A better way to do dotfiles - Dotbotüí≠ My initial version of dotfiles was intended for wsl &amp; early devcontainers version. Since then, I‚Äôve learned a couple of lessons. Mainly, it is a good idea to keep dotfiles simple and rely on essentials. You don‚Äôt need to bring tons of dependencies and convoluted configuration files. The simpler - the easier to maintain and synchronize.So, I‚Äôve created another dotfiles repo: https://github.com/NikiforovAll/dotbot. This one is based on https://github.com/anishathalye/dotbot. Dotbot is a tool that bootstraps your dotfiles. It is straightforward and easy to use. I suggest you to give a try on your own.As discussed, since I deliberately choose to not have heavy dependencies it takes a couple a seconds to install dotfiles. Exactly as I want it to be. My general suggestion is to move the heavy lifting of dependencies installation to DevContainers.One-liner: cd ~ &amp;&amp; git clone https://github.com/NikiforovAll/dotbot &amp;&amp; cd ./dotbot &amp;&amp; cd ./installHere is how the project structure looks like:# oleksii_nikiforov in ~/dotbot&gt; tree -L 2.‚îú‚îÄ‚îÄ bash‚îÇ¬†¬† ‚îú‚îÄ‚îÄ plugins.bash‚îÇ¬†¬† ‚îî‚îÄ‚îÄ prompt.bash‚îú‚îÄ‚îÄ bash_profile‚îú‚îÄ‚îÄ bashrc‚îú‚îÄ‚îÄ gitconfig‚îú‚îÄ‚îÄ gitignore_global‚îú‚îÄ‚îÄ install‚îú‚îÄ‚îÄ install.conf.yaml‚îú‚îÄ‚îÄ pre-install.sh‚îú‚îÄ‚îÄ shell‚îÇ¬†¬† ‚îú‚îÄ‚îÄ aliases.sh‚îÇ¬†¬† ‚îú‚îÄ‚îÄ bootstrap.sh‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dircolors.extra‚îÇ¬†¬† ‚îú‚îÄ‚îÄ external.sh‚îÇ¬†¬† ‚îú‚îÄ‚îÄ functions.sh‚îÇ¬†¬† ‚îî‚îÄ‚îÄ plugins‚îú‚îÄ‚îÄ zsh‚îÇ¬†¬† ‚îú‚îÄ‚îÄ plugins‚îÇ¬†¬† ‚îú‚îÄ‚îÄ plugins_after.zsh‚îÇ¬†¬† ‚îú‚îÄ‚îÄ plugins_before.zsh‚îÇ¬†¬† ‚îú‚îÄ‚îÄ prompt.zsh‚îÇ¬†¬† ‚îî‚îÄ‚îÄ settings.zsh‚îî‚îÄ‚îÄ zshrcCombining everything togetherThe resulting dev environment is combined from diverse sources. Roughly speaking a host (developer machine/codespace) is composed of the next pieces:flowchart    subgraph host[ ]        direction LR        subgraph dotfiles[Dotfiles]            direction LR            shellconfig[Shell]            shellconfig--&gt;aliases            shellconfig--&gt;prompts            shellconfig--&gt;scripts            shellconfig--&gt;tools            editorconfig[VSCode]            editorconfig--&gt;extensions            editorconfig--&gt;customizations[user settings]            gitconfig[Git]--&gt;preferences            gitconfig[Git]--&gt;galiases[aliases]        end        subgraph devconataners[DevContainers]            direction LR            projecttools[Tools]            projecttools--&gt;dockerimage[dockerfile]            projecttools--&gt;devcontainerfeatures[devcontainer.json features]            projecttools--&gt;postcreatecommand[postCreateCommand]            pjeditorconfig[VSCode]            pjeditorconfig--&gt;pjextensions[extensions]            pjeditorconfig--&gt;pjcustomization[workspaces settings]        end                tools-.-&gt;projecttools        extensions-.-&gt;pjextensions        customizations-.-&gt;pjcustomization        vscodesettingssync(VSCode settings Sync)--&gt;editorconfig        gitcredentials(Git Credentials)--&gt;gitconfig    end    style host fill:#e6eced,stroke:grey,stroke-width:2px,stroke-dasharray: 5 5    style dotfiles fill:#def4ea    style devconataners fill:#dee9f4Pick a host. GitHub CodespacesThe suggested approach abstracts away the host. It doesn‚Äôt necessarily has to be your own machine. This is what ‚ÄúGitHub Codespaces‚Äù product is about.  A codespace is a development environment that‚Äôs hosted in the cloud. You can customize your project for GitHub Codespaces by committing configuration files to your repository (often known as Configuration-as-Code), which creates a repeatable codespace configuration for all users of your project.  GitHub Codespaces run on a variety of VM-based compute options hosted by GitHub.com, which you can configure from 2 core machines up to 32 core machines. You can connect to your codespaces from the browser or locally using Visual Studio Code.    ExampleWe will examine Azure Developer CLI (azd) template - todo-csharp-cosmos-sql. Here is a repo that will be used as an example: https://github.com/NikiforovAll/todo-csharp-cosmos-sql.The end goal is to setup a full-featured dev environment and measure how much time it takes.This application utilizes the following Azure resources:  Azure App Services to host the Web frontend and API backend  Azure Cosmos DB SQL API for storage  Azure Monitor for monitoring and logging  Azure Key Vault for securing secretsThe demo combines techniquest from both Infrastructure as a Code (IaaC) and Dev Environment as a Code (DEaaC). It takes only 15 stagering minutes from zero to deployed azure resources on the first launch.1Ô∏è‚É£ Configure dotfiles in ‚ÄúSettings&gt;Codespaces‚Äù:2Ô∏è‚É£ Go to repository and launch a codespace:3Ô∏è‚É£ Open the codespace in the browser or locally (I will go with browser option for the demo)4Ô∏è‚É£ Login to Azure via CLI or VSCode extension:az login --use-device-code5Ô∏è‚É£ Run azd up6Ô∏è‚É£ Setup the breakpoint7Ô∏è‚É£ Inspect forwarded ports. Note, codespaces allocates some random URL8Ô∏è‚É£ Generate load9Ô∏è‚É£ Inspect logs by running azd monitor --logsüîü Tear down the dev environment in Azure azd downAs result we have:  Deployed whole developer setup in 15 minutes  Terminal/Shell is configured  Git is configured  All project tooling and dependencies are installed: dotnet, az, etc.  VSCode contains user extensions as well as suggested by devcontainer  A one happy developer üòâSummaryDEaaS has lots of advantages. Most of the time it just works. But it definitely required investing some time to learn about the moving parts to know how to maintain and troubleshoot the setup. I have not shown you the dark side of using DevContainers but I can assure you that it could be hard to understand when something goes wrong. Despite all challenges, it opens new horizons and it is a future of large-scale and open-source development.Reference  https://dotfiles.github.io  https://nikiforovall.github.io/productivity/2019/11/30/nikiforovall-setup.html  https://code.visualstudio.com/docs/remote/create-dev-container  https://docs.github.com/en/codespaces/overview  https://docs.github.com/en/codespaces/setting-up-your-project-for-codespaces/introduction-to-dev-containers  https://docs.github.com/en/codespaces/customizing-your-codespace/personalizing-github-codespaces-for-your-account"
    },
  
    {
      "id": "59",
      "title": "Creating and Using HTTP Client SDKs in .NET 6",
      "url": "/dotnet/aspnetcore/microservices/2022/04/04/creating-and-using-http-sdks.html",
      "date": "April 04, 2022",
      "categories": ["dotnet","aspnetcore","microservices"],
      "tags": ["dotnet","microservices","aspnetcore","api"],
      "shortinfo": "There are many ways of developing HTTP Client SDKs. This article helps you to choose the right one according to your scenario.",
      "content": "TL;DRLearn three ways you can develop HTTP Client SDKs in .NET. Source code: https://github.com/NikiforovAll/http-sdk-guide  TL;DR  Key Takeaways  Introduction  Writing HTTP Client SDK          Client Lifetime        Consuming API Clients          Consuming API Clients. HttpClientFactory        Extending HTTP Client SDKs. Adding cross-cutting concerns via DelegatingHandler          Third-Party Extensions        Testing HTTP Client SDKs  Writing HTTP Client SDK. Declarative approach  Consuming API Clients. Refit  Writing HTTP Client SDK. Automated approach  Choosing the right approach  Summary  ReferenceKey Takeaways  Writing and maintaining HTTP Client SDKs is a very important skill for modern .NET developers working with distributed systems.  In order to properly manage HTTP connections, you need to design your API Clients to be ready to be consumed from any Dependency Injection container.  A good client SDK is composable, providing straightforward ways to configure and extend it.  Testing HTTP Client SDKs can be very beneficial in certain scenarios and it gives you additional confidence in your code.  There are many ways of developing HTTP Client SDKs. This article helps you to choose the right one according to your scenario.IntroductionToday‚Äôs cloud-based, microservice-based or internet-of-things applications often depend on communicating with other systems across a network. Each service runs in its process and solves a bounded set of problems. Communication between services is based on a lightweight mechanism, often an HTTP resource API.From a .NET developer perspective, we want to provide a consistent and manageable way of integrating with a particular service in the form of a distributable package. Preferably, we also want to ship the service integration code we develop as a NuGet package and share it with other people, teams, or even organizations. In this article, I will share many aspects of creating and using HTTP Client SDKs using .NET 6.Client SDKs provide a meaningful abstraction layer over remote service. Essentially, it allows making Remote Procedure Calls (RPC). The responsibility of a Client SDK is to serialize some data, send it to remote, deserialize and process a response.The main benefits of API SDKs:  It speedups the API integration process  Provides a consistent and standard approach  Gives somewhat control to service owners over the way APIs are consumedWriting HTTP Client SDKThroughout this article, we will write full-functioning Dad Jokes API Client. It serves dad jokes, let‚Äôs have some fun. It is a good idea to start from the contract.public interface IDadJokesApiClient{    Task&lt;JokeSearchResponse&gt; SearchAsync(      string term, CancellationToken cancellationToken);    Task&lt;Joke&gt; GetJokeByIdAsync(        string id, CancellationToken cancellationToken);    Task&lt;Joke&gt; GetRandomJokeAsync(CancellationToken cancellationToken);}public class JokeSearchResponse{    public bool Success { get; init; }    public List&lt;Joke&gt; Body { get; init; } = new();}public class Joke{    public string Punchline { get; set; } = default!;    public string Setup { get; set; } = default!;    public string Type { get; set; } = default!;}The contract is created based on an API you are integrating with. My general recommendations are to develop common-purpose APIs and follow Robustness Principle and Principle of least astonishment. But it is totally fine if you want to modify and transform data contract based on your needs. Just think about it from a consumer perspective.The bread and butter of HTTP-based integrations is HttpClient. It contains everything you need to successfully work with HTTP abstractions.public class DadJokesApiClient : IDadJokesApiClient{    private readonly HttpClient httpClient;    public DadJokesApiClient(HttpClient httpClient) =&gt;        this.httpClient = httpClient;}Usually, we deal with JSON over HTTP APIs, so in .NET 5 System.Net.Http.Json namespace was added to BCL. It provides many extension methods for HttpClient and HttpContent that perform serialization and deserialization using System.Text.Json. If you don‚Äôt have something complex and exotic I would suggest using System.Net.Http.Json because it frees you from writing boilerplate code. Not only it is boring, but it also is not trivial to get it right in the most efficient and bug-free way from the get-go. I suggest you check Steves‚Äô Gordon blog post - sending and receiving JSON using HttpClientpublic async Task&lt;Joke&gt; GetRandomJokeAsync(CancellationToken cancellationToken){    var jokes = await this.httpClient.GetFromJsonAsync&lt;JokeSearchResponse&gt;(        ApiUrlConstants.GetRandomJoke, cancellationToken);    if (jokes is { Body.Count: 0 } or { Success: false })    {        // consider creating custom exceptions for situations like this        throw new InvalidOperationException(\"This API is no joke.\");    }    return jokes.Body.First();}üí° Tip: You may want to create some centralized place to manage the endpoints URLs, like this:public static class ApiUrlConstants{    public const string JokeSearch = \"/joke/search\";    public const string GetJokeById = \"/joke\";    public const string GetRandomJoke = \"/random/joke\";}üí° Tip: If you need to deal with complex URIs - use Flurl. It provides fluent URL building experience.public async Task&lt;Joke&gt; GetJokeByIdAsync(string id, CancellationToken cancellationToken){    // $\"{ApiUrlConstants.GetJokeById}/{id}\"    var path = ApiUrlConstants.GetJokeById.AppendPathSegment(id);    var joke = await this.httpClient.GetFromJsonAsync&lt;Joke&gt;(path, cancellationToken);    return joke ?? new();}Next, we need to specify required headers (or some other required configuration). We want to provide a flexible mechanism for configuring HttpClient used as part of SDK. In this case, we need to supply credentials in the custom header and specify a well-known ‚ÄúAccept‚Äù header.üí° Tip: Expose high-level building blocks as HttpClientExtensions. It makes it easy to discover API specific configuration. For example, if you have a custom authorization mechanism it should be supported by SDK (at least provide documentation for it).public static class HttpClientExtensions{    public static HttpClient AddDadJokesHeaders(        this HttpClient httpClient, string host, string apiKey)    {        var headers = httpClient.DefaultRequestHeaders;        headers.Add(ApiConstants.HostHeader, new Uri(host).Host);        headers.Add(ApiConstants.ApiKeyHeader, apiKey);        return httpClient;    }}Client LifetimeTo construct DadJokesApiClient we need to create HttpClient. As you know, HttpClient implements IDisposable because there is an underlying unmanageable resource - TCP connection. There is a limited amount of concurrent TCP connections that can be opened simultaneously on a single machine. So, it brings an important question - ‚ÄúShould I create HttpClient every time I need it or only once during an application startup?‚ÄùHttpClient is actually a shared object. This means that under the covers it is reentrant and thread-safe. Instead of creating a new instance of HttpClient for each execution, you should share a single instance of HttpClient. However, this comes with its own set of issues. For example, the client will keep connections open for the lifespan of the application, it won‚Äôt respect the DNS TTL settings and it will never get DNS updates. So this isn‚Äôt a perfect solution either.Basically, you need to manage a pool of TCP connections that are disposed from time to time to respect DNS updates. This is exactly what HttpClientFactory does. The official documentation describes HttpClientFactory as being ‚Äúan opinionated factory for creating HttpClient instances to be used in your applications‚Äù. We will see how to use it in a moment.Each time you get an HttpClient object from the IHttpClientFactory, a new instance is returned. But each HttpClient uses an HttpMessageHandler that‚Äôs pooled and reused by the IHttpClientFactory to reduce resource consumption. Pooling of handlers is desirable as each handler typically manages its own underlying HTTP connections. Some handlers also keep connections open indefinitely, which can prevent the handler from reacting to DNS changes. HttpMessageHandler has a limited lifetime.Down below you can see how HttpClientFactory comes into play when using HttpClient managed by DI.Consuming API ClientsThe very basic scenario is a console application without a dependency injection container. The goal here is to give consumers the fastest way possible to integrate.üí° Create static factory method that creates an API Client.public static class DadJokesApiClientFactory{    public static IDadJokesApiClient Create(string host, string apiKey)    {        var httpClient = new HttpClient()        {            BaseAddress = new Uri(host);        }        ConfigureHttpClient(httpClient, host, apiKey);        return new DadJokesApiClient(httpClient);    }    internal static void ConfigureHttpClient(        HttpClient httpClient, string host, string apiKey)    {        ConfigureHttpClientCore(httpClient);        httpClient.AddDadJokesHeaders(host, apiKey);    }    internal static void ConfigureHttpClientCore(HttpClient httpClient)    {        httpClient.DefaultRequestHeaders.Accept.Clear();        httpClient.DefaultRequestHeaders.Accept.Add(new(\"application/json\"));    }}Finally, we can use IDadJokesApiClient from the console application:var host = \"https://dad-jokes.p.rapidapi.com\";var apiKey = \"&lt;token&gt;\";var client = DadJokesApiClientFactory.Create(host, apiKey);var joke = await client.GetRandomJokeAsync();Console.WriteLine($\"{joke.Setup} {joke.Punchline}\");Consuming API Clients. HttpClientFactoryThe next step is to configure HttpClient as part of a Dependency Injection container. I will not go into details, there is a lot of good stuff on the internet. Once again, there is a really good article from Steve‚Äôs Gordon - HttpClientFactory in ASP.NET CoreTo add pooled HttpClient to DI you need to use IServiceCollection.AddHttpClient from Microsoft.Extensions.Http.üí° Provide a custom extension method to add typed HttpClient in DI.public static class ServiceCollectionExtensions{    public static IHttpClientBuilder AddDadJokesApiClient(        this IServiceCollection services,        Action&lt;HttpClient&gt; configureClient) =&gt;            services.AddHttpClient&lt;IDadJokesApiClient, DadJokesApiClient&gt;((httpClient) =&gt;            {                DadJokesApiClientFactory.ConfigureHttpClientCore(httpClient);                configureClient(httpClient);            });}Use extension method like the following:var host = \"https://da-jokes.p.rapidapi.com\";var apiKey = \"&lt;token&gt;\";var services = new ServiceCollection();services.AddDadJokesApiClient(httpClient =&gt;{    httpClient.BaseAddress = new(host);    httpClient.AddDadJokesHeaders(host, apiKey);});var provider = services.BuildServiceProvider();var client = provider.GetRequiredService&lt;IDadJokesApiClient&gt;();var joke = await client.GetRandomJokeAsync();logger.Information($\"{joke.Setup} {joke.Punchline}\");As you see, you can use IHttpClientFactory outside of ASP.NET Core. For example, console applications, workers, lambdas, etc.Let‚Äôs see it running:The interesting part here is that clients created by DI automatically logs outgoing requests. It makes development and troubleshooting so much easier.{SourceContext}[{EventId}] // patternSystem.Net.Http.HttpClient.IDadJokesApiClient.LogicalHandler [{ Id: 100, Name: \"RequestPipelineStart\" }]    System.Net.Http.HttpClient.IDadJokesApiClient.ClientHandler [{ Id: 100, Name: \"RequestStart\" }]    System.Net.Http.HttpClient.IDadJokesApiClient.ClientHandler [{ Id: 101, Name: \"RequestEnd\" }]System.Net.Http.HttpClient.IDadJokesApiClient.LogicalHandler [{ Id: 101, Name: \"RequestPipelineEnd\" }]The most common scenario is web applications. Here is .NET 6 MinimalAPI example:var builder = WebApplication.CreateBuilder(args);var services = builder.Services;var configuration = builder.Configuration;var host = configuration[\"DadJokesClient:host\"];services.AddDadJokesApiClient(httpClient =&gt;{    httpClient.BaseAddress = new(host);    httpClient.AddDadJokesHeaders(host, configuration[\"DADJOKES_TOKEN\"]);});var app = builder.Build();app.MapGet(\"/\", async (IDadJokesApiClient client) =&gt; await client.GetRandomJokeAsync());app.Run();{  \"punchline\": \"They are all paid actors anyway\",  \"setup\": \"We really shouldn't care what people at the Oscars say\",  \"type\": \"actor\"}Extending HTTP Client SDKs. Adding cross-cutting concerns via DelegatingHandlerHttpClient provide an extension point - message handler. A message handler is a class that receives an HTTP request and returns an HTTP response. A wide variety of problems could be expressed as cross-cutting concerns. For example, logging, authentication, caching, header forwarding, auditing, etc. Aspect-oriented programming aims to encapsulate cross-cutting concerns into aspects to retain modularity. Typically, a series of message handlers are chained together. The first handler receives an HTTP request, does some processing, and gives the request to the next handler. At some point, the response is created and goes back up the chain.// supports the most common requirements for most applicationspublic abstract class HttpMessageHandler : IDisposable{}// plug a handler into a handler chainpublic abstract class DelegatingHandler : HttpMessageHandler{}Task. Assume you need to copy a list of headers from ASP.NET Core HttpContext and pass them to all outgoing requests made by Dad Jokes API client.public class HeaderPropagationMessageHandler : DelegatingHandler{    private readonly HeaderPropagationOptions options;    private readonly IHttpContextAccessor contextAccessor;    public HeaderPropagationMessageHandler(        HeaderPropagationOptions options,        IHttpContextAccessor contextAccessor)    {        this.options = options;        this.contextAccessor = contextAccessor;    }    protected override Task&lt;HttpResponseMessage&gt; SendAsync(        HttpRequestMessage request, CancellationToken cancellationToken)    {        if (this.contextAccessor.HttpContext != null)        {            foreach (var headerName in this.options.HeaderNames)            {                var headerValue = this.contextAccessor                    .HttpContext.Request.Headers[headerName];                request.Headers.TryAddWithoutValidation(                    headerName, (string[])headerValue);            }        }        return base.SendAsync(request, cancellationToken);    }}public class HeaderPropagationOptions{    public IList&lt;string&gt; HeaderNames { get; set; } = new List&lt;string&gt;();}Now, we want to ‚Äúplug‚Äù DelegatingHandler into HttpClient request pipeline.For non-IHttpClientFactory scenarios, we want clients to specify a list of DelegatingHandler so we can build an underlying chain for HttpClient.//DadJokesApiClientFactory.cspublic static IDadJokesApiClient Create(    string host,    string apiKey,    params DelegatingHandler[] handlers){    var httpClient = new HttpClient();    if (handlers.Length &gt; 0)    {        _ = handlers.Aggregate((a, b) =&gt;        {            a.InnerHandler = b;            return b;        });        httpClient = new(handlers[0]);    }    httpClient.BaseAddress = new Uri(host);    ConfigureHttpClient(httpClient, host, apiKey);    return new DadJokesApiClient(httpClient);}So, without DI container, extended DadJokesApiClient could be constructed like this:var loggingHandler = new LoggingMessageHandler(); //outermostvar authHandler = new AuthMessageHandler();var propagationHandler = new HeaderPropagationMessageHandler();var primaryHandler = new HttpClientHandler();  // the default handler used by HttpClientDadJokesApiClientFactory.Create(    host, apiKey,    loggingHandler, authHandler, propagationHandler, primaryHandler);// LoggingMessageHandler ‚ûù AuthMessageHandler ‚ûù HeaderPropagationMessageHandler ‚ûù HttpClientHandlerFor DI container scenarios, on another hand, we want to provide auxiliary extension method to easily plug HeaderPropagationMessageHandler by using IHttpClientBuilder.AddHttpMessageHandler.public static class HeaderPropagationExtensions{    public static IHttpClientBuilder AddHeaderPropagation(        this IHttpClientBuilder builder,        Action&lt;HeaderPropagationOptions&gt; configure)    {        builder.Services.Configure(configure);        builder.AddHttpMessageHandler((sp) =&gt;        {            return new HeaderPropagationMessageHandler(                sp.GetRequiredService&lt;IOptions&lt;HeaderPropagationOptions&gt;&gt;().Value,                sp.GetRequiredService&lt;IHttpContextAccessor&gt;());        });        return builder;    }}Here is how extended MinimalAPI example looks like:var builder = WebApplication.CreateBuilder(args);var services = builder.Services;var configuration = builder.Configuration;var host = configuration[\"DadJokesClient:host\"];services.AddDadJokesApiClient(httpClient =&gt;{    httpClient.BaseAddress = new(host);    httpClient.AddDadJokesHeaders(host, configuration[\"DADJOKES_TOKEN\"]);}).AddHeaderPropagation(o =&gt; o.HeaderNames.Add(\"X-Correlation-ID\"));var app = builder.Build();app.MapGet(\"/\", async (IDadJokesApiClient client) =&gt; await client.GetRandomJokeAsync());app.Run();üí° Sometimes functionality like this is reused by other services. You might want to take it one step further and factor out all shared code into a common NuGet package and use it in HTTP Client SDKs.Third-Party ExtensionsNot only we can write our own message handlers. There is a lot of useful NuGet packages provided and supported by .NET OSS community. Here are my favorites:üí° Resiliency patterns - retry, cache, fallback, etc.: Very often, in distrusted systems world you need to ensure high availability by incorporating some resilience policies. Luckily, we have a built-in solution to build and define policies in .NET - Polly. There is out-of-the-box integration with IHttpClientFactory provided by Polly. This uses a convenience method, IHttpClientBuilder.AddTransientHttpErrorPolicy. It configures a policy to handle errors typical of HTTP calls: HttpRequestException, HTTP 5XX status codes (server errors), HTTP 408 status code (request timeout).services.AddDadJokesApiClient(httpClient =&gt;{    httpClient.BaseAddress = new(host);}).AddTransientHttpErrorPolicy(builder =&gt; builder.WaitAndRetryAsync(new[]{    TimeSpan.FromSeconds(1),    TimeSpan.FromSeconds(5),    TimeSpan.FromSeconds(10)}));For example, transient errors might be handled proactively by using Retry and Circuit Breaker patterns. Usually, we use a retry pattern when there is a hope that downstream service will self-correct eventually. Waiting between retries provides an opportunity for a downstream service to stabilize. It is common to use retries based on the Exponential Backoff algorithm. On paper, it sounds great, but in real-world scenarios, the retry pattern may be overused. Additional retries might be the source of additional load or spikes. In the worst case, resources in the caller may then become exhausted or excessively blocked, waiting for replies which will never come causing an upstream-cascading failure.This is when the Circuit Breaker pattern comes into play. It detects the level of faults and prevents calls to a downstream service when a fault threshold is exceeded. Use this pattern when there is no chance of succeeding - for example, where a subsystem is completely offline or struggling under load. The idea behind Circuit Breaker is pretty straightforward, although, you might build something more complex on top of it. When faults exceed the threshold, calls are placed through the circuit, so instead of processing a request, we practice the fail-fast approach, throwing an exception immediately.Polly is really powerful and it provides a way to combine resilience strategies. See PolicyWrap.Here is a classification of the strategies you might want to use:Designing reliable systems could be a challenging task, I suggest you investigate the subject on your own. Here is a good introduction - .NET microservices - Architecture e-book: Implement resilient applicationsüí° Authentication in OAuth2/OIDC: If you need to manage user and client access tokens I suggest using IdentityModel.AspNetCore. It acquires, caches, and rotates tokens for you, see the docs.// adds user and client access token managementservices.AddAccessTokenManagement(options =&gt;{    options.Client.Clients.Add(\"identity-provider\", new ClientCredentialsTokenRequest    {        Address = \"https://demo.identityserver.io/connect/token\",        ClientId = \"my-awesome-service\",        ClientSecret = \"secret\",        Scope = \"api\" // optional    });});// registers HTTP client that uses the managed client access token// adds the access token handler to HTTP client registrationservices.AddDadJokesApiClient(httpClient =&gt;{    httpClient.BaseAddress = new(host);}).AddClientAccessTokenHandler(); Testing HTTP Client SDKsBy this time, you should be pretty comfortable designing and writing your own HTTP Client SDKs. The only thing that is left is to write some tests to ensure expected behavior. Note, it might be a good idea to skip extensive unit testing and write more integration or e2e to ensure proper integration. For now, I will show you how to unit test DadJokesApiClient.As you have seen previously, HttpClient is extensible. Furthermore, we can replace the standard HttpMessageHandler with the test version. So, instead of sending actual requests over the wire, we will use the mock. This technique opens tons of opportunities because we can simulate all kinds of behaviors of HttpClient that otherwise could be hard to replicate in a normal situation.Let‚Äôs define reusable methods to create a mock of HttpClient that we will pass as a dependency to DadJokesApiClient.public static class TestHarness{    public static Mock&lt;HttpMessageHandler&gt; CreateMessageHandlerWithResult&lt;T&gt;(        T result, HttpStatusCode code = HttpStatusCode.OK)    {        var messageHandler = new Mock&lt;HttpMessageHandler&gt;();        messageHandler.Protected()            .Setup&lt;Task&lt;HttpResponseMessage&gt;&gt;(                \"SendAsync\",                ItExpr.IsAny&lt;HttpRequestMessage&gt;(),                ItExpr.IsAny&lt;CancellationToken&gt;())            .ReturnsAsync(new HttpResponseMessage()            {                StatusCode = code,                Content = new StringContent(JsonSerializer.Serialize(result)),            });        return messageHandler;    }    public static HttpClient CreateHttpClientWithResult&lt;T&gt;(        T result, HttpStatusCode code = HttpStatusCode.OK)    {        var httpClient = new HttpClient(CreateMessageHandlerWithResult(result, code).Object)        {            BaseAddress = new(\"https://api-client-under-test.com\"),        };        return httpClient;    }}From this point, unit testing is a pretty simple process:public class DadJokesApiClientTests{    [Theory, AutoData]    public async Task GetRandomJokeAsync_SingleJokeInResult_Returned(Joke joke)    {        // Arrange        var response = new JokeSearchResponse        {            Success = true,            Body = new() { joke }        };        var httpClient = CreateHttpClientWithResult(response);        var sut = new DadJokesApiClient(httpClient);        // Act        var result = await sut.GetRandomJokeAsync();        // Assert        result.Should().BeEquivalentTo(joke);    }    [Fact]    public async Task GetRandomJokeAsync_UnsuccessfulJokeResult_ExceptionThrown()    {        // Arrange        var response = new JokeSearchResponse();        var httpClient = CreateHttpClientWithResult(response);        var sut = new DadJokesApiClient(httpClient);        // Act        // Assert        await FluentActions.Invoking(() =&gt; sut.GetRandomJokeAsync())            .Should().ThrowAsync&lt;InvalidOperationException&gt;();    }}Using HttpClient is the most flexible approach. You have full control over integration with APIs. But, there is a downside, you need to write a lot of boilerplate code. In some situations, an API you are integrating with is trivial so you don‚Äôt really need all capabilities provided by HttpClient, HttpRequestMessage, HttpResponseMessage.Pros ‚ûï:  Full control over behavior and data contracts. You can even write ‚Äúsmart‚Äù API Client and move some logic inside SDK if it makes sense for a particular scenario. For example, you can throw custom exceptions, transform requests and responses, provide default values for headers, etc.  Full control over serialization and deserialization process  Easy to debug and troubleshoot. A stack trace is simple and you can always spin up the debugger to see what is happening under the hood.Cons ‚ûñ:  Need to write a lot of repetitive code  Someone should maintain a code base in case of API changes and bugs. This is a tedious and error-prone process.Writing HTTP Client SDK. Declarative approach  The less code, the fewer bugs.Refit is an automatic type-safe REST library for .NET. It turns your REST API into a live interface. Refit uses System.Text.Json as the default JSON serializer.Every method must have an HTTP attribute that provides the request method and relative URL.using Refit;public interface IDadJokesApiClient{    /// &lt;summary&gt;    /// Searches jokes by term.    /// &lt;/summary&gt;    [Get(\"/joke/search\")]    Task&lt;JokeSearchResponse&gt; SearchAsync(        string term,        CancellationToken cancellationToken = default);    /// &lt;summary&gt;    /// Gets a joke by id.    /// &lt;/summary&gt;    [Get(\"/joke/{id}\")]    Task&lt;Joke&gt; GetJokeByIdAsync(        string id,        CancellationToken cancellationToken = default);    /// &lt;summary&gt;    /// Gets a random joke.    /// &lt;/summary&gt;    [Get(\"/random/joke\")]    Task&lt;JokeSearchResponse&gt; GetRandomJokeAsync(        CancellationToken cancellationToken = default);}Refit generates type that implements IDadJokesApiClient based on information provided by Refit.HttpMethodAttributeConsuming API Clients. RefitThe approach is the same as for vanilla HttpClient integration, but instead of constructing a client manually, we use static method provided by Refit.public static class DadJokesApiClientFactory{    public static IDadJokesApiClient Create(        HttpClient httpClient,        string host,        string apiKey)    {        httpClient.BaseAddress = new Uri(host);        ConfigureHttpClient(httpClient, host, apiKey);        return RestService.For&lt;IDadJokesApiClient&gt;(httpClient);    }    // ...}For DI container scenarios, we can use Refit.HttpClientFactoryExtensions.AddRefitClient extension method.public static class ServiceCollectionExtensions{    public static IHttpClientBuilder AddDadJokesApiClient(        this IServiceCollection services,        Action&lt;HttpClient&gt; configureClient)    {        var settings = new RefitSettings()        {            ContentSerializer = new SystemTextJsonContentSerializer(new JsonSerializerOptions()            {                PropertyNameCaseInsensitive = true,                WriteIndented = true,            })        };        return services.AddRefitClient&lt;IDadJokesApiClient&gt;(settings)            .ConfigureHttpClient((httpClient) =&gt;            {                DadJokesApiClientFactory.ConfigureHttpClient(httpClient);                configureClient(httpClient);            });    }}Usage:var builder = WebApplication.CreateBuilder(args);var configuration = builder.Configuration;Log.Logger = new LoggerConfiguration().WriteTo.Console().CreateBootstrapLogger();builder.Host.UseSerilog((ctx, cfg) =&gt; cfg.WriteTo.Console());var services = builder.Services;services.AddDadJokesApiClient(httpClient =&gt;{    var host = configuration[\"DadJokesClient:host\"];    httpClient.BaseAddress = new(host);    httpClient.AddDadJokesHeaders(host, configuration[\"DADJOKES_TOKEN\"]);});var app = builder.Build();app.MapGet(\"/\", async Task&lt;Joke&gt; (IDadJokesApiClient client) =&gt;{    var jokeResponse = await client.GetRandomJokeAsync();    return jokeResponse.Body.First(); // unwraps JokeSearchResponse});app.Run();Note, since the contract of the generated client should match the underlying data contract, we no longer have control of contract transformation and this responsibility is delegated to consumers.Let‚Äôs see how the code above works in practice. The output of MinimalAPI example is different because I‚Äôve added Serilog logging.{  \"punchline\": \"Forgery.\",  \"setup\": \"Why was the blacksmith charged with?\",  \"type\": \"forgery\"}As usual, there are some pros and some cons:Pros ‚ûï:  Easy to use and develop API clients.  Highly configurable. Flexible enough to get things done.  No need for additional unit testingCons ‚ûñ:  Hard to troubleshoot. Sometimes it can be hard to understand how the generated code works. For example, there is a mismatch in configuration.  Requires other team members to understand how to read and write code developed with Refit.  Still consumes some time for medium/large APIs.Honorable mentions: RestEase, RESTFulSenseWriting HTTP Client SDK. Automated approachThere is a way to fully automate HTTP Client SDKs. The OpenAPI/Swagger specification uses JSON and JSON Schema to describe a RESTful web API. The NSwag project provides tools to generate client code from these OpenAPI specifications. Everything can be automated via CLI (distributed via NuGet tool or build target; or NPM).Actually, Dad Jokes API doesn‚Äôt provide OpenAPI, so I had to write it manually. Fortunately, it was quite easy to do:openapi: '3.0.2'info:  title: Dad Jokes API  version: '1.0'servers:  - url: https://dad-jokes.p.rapidapi.compaths:  /joke/{id}:    get:      description: ''      operationId: 'GetJokeById'      parameters:      - name: \"id\"        in: \"path\"        description: \"\"        required: true        schema:          type: \"string\"      responses:        '200':          description: successful operation          content:            application/json:              schema:                \"$ref\": \"#/components/schemas/Joke\"  /random/joke:    get:      description: ''      operationId: 'GetRandomJoke'      parameters: []      responses:        '200':          description: successful operation          content:            application/json:              schema:                \"$ref\": \"#/components/schemas/JokeResponse\"  /joke/search:    get:      description: ''      operationId: 'SearchJoke'      parameters: []      responses:        '200':          description: successful operation          content:            application/json:              schema:                \"$ref\": \"#/components/schemas/JokeResponse\"components:  schemas:    Joke:      type: object      required:      - _id      - punchline      - setup      - type      properties:        _id:          type: string        type:          type: string        setup:          type: string        punchline:          type: string    JokeResponse:      type: object      properties:        sucess:          type: boolean        body:          type: array          items:            $ref: '#/components/schemas/Joke'Now, we want to generate HTTP Client SDK automatically. Let‚Äôs use NSwagStudio.Here is how the generated IDadJokesApiClient looks like (XML comments are deleted for brevity):    [System.CodeDom.Compiler.GeneratedCode(\"NSwag\", \"13.10.9.0 (NJsonSchema v10.4.1.0 (Newtonsoft.Json v12.0.0.0))\")]    public partial interface IDadJokesApiClient    {        System.Threading.Tasks.Task&lt;Joke&gt; GetJokeByIdAsync(string id);            System.Threading.Tasks.Task&lt;Joke&gt; GetJokeByIdAsync(string id, System.Threading.CancellationToken cancellationToken);            System.Threading.Tasks.Task&lt;JokeResponse&gt; GetRandomJokeAsync();            System.Threading.Tasks.Task&lt;JokeResponse&gt; GetRandomJokeAsync(System.Threading.CancellationToken cancellationToken);            System.Threading.Tasks.Task&lt;JokeResponse&gt; SearchJokeAsync();            System.Threading.Tasks.Task&lt;JokeResponse&gt; SearchJokeAsync(System.Threading.CancellationToken cancellationToken);    }As usual, we want to provide the registration of typed client as an extension method:public static class ServiceCollectionExtensions{    public static IHttpClientBuilder AddDadJokesApiClient(        this IServiceCollection services, Action&lt;HttpClient&gt; configureClient) =&gt;             services.AddHttpClient&lt;IDadJokesApiClient, DadJokesApiClient&gt;(                httpClient =&gt; configureClient(httpClient));}Usage:var builder = WebApplication.CreateBuilder(args);var configuration = builder.Configuration;var services = builder.Services;services.AddDadJokesApiClient(httpClient =&gt;{    var host = configuration[\"DadJokesClient:host\"];    httpClient.BaseAddress = new(host);    httpClient.AddDadJokesHeaders(host, configuration[\"DADJOKES_TOKEN\"]);});var app = builder.Build();app.MapGet(\"/\", async Task&lt;Joke&gt; (IDadJokesApiClient client) =&gt;{    var jokeResponse = await client.GetRandomJokeAsync();    return jokeResponse.Body.First();});app.Run();Let‚Äôs run it and enjoy the last joke of this article:{  \"punchline\": \"And it's really taken off\",  \"setup\": \"So I invested in a hot air balloon company...\",  \"type\": \"air\"}Pros ‚ûï:  Based on the well-known specification  Supported by rich set of tools and vibrant community  Fully automated, new SDK can be generated as part of CI/CD process every time OpenAPI specification is changed  Generate SDKs for multiple programming languages  Relatively easy to troubleshoot since we can see the code generated by the toolchain.Cons ‚ûñ:  Can‚Äôt be applied without proper OpenAPI specification  Hard to customize and control the contract of generated API ClientHonorable mentions: AutoRest, Visual Studio Connected ServicesChoosing the right approachWe have three different ways of producing SDK clients. The selection process can be simplified to the next categories  I‚Äôm a simple man/woman/non-binary, I want to have full control over my HTTP Client integration.Use manual approach.  I‚Äôm a busy man/woman/non-binary, but I still want to have somewhat control.Use a declarative approach.  I‚Äôm a lazy man/woman/non-binary. Do the thing for me.Use an automated approach.Decision chart:SummaryWe‚Äôve reviewed different ways of developing HTTP Client SDKs. Choosing the right approach depends on use-case and requirements, but I hope this article gives you nice foundations for making the best design decisions. Thank you.Reference  https://github.com/NikiforovAll/http-sdk-guide  https://www.stevejgordon.co.uk/sending-and-receiving-json-using-httpclient-with-system-net-http-json  https://devblogs.microsoft.com/dotnet/net-5-new-networking-improvements/  https://www.stevejgordon.co.uk/httpclientfactory-aspnetcore-outgoing-request-middleware-pipeline-delegatinghandlers  https://www.aspnetmonsters.com/2016/08/2016-08-27-httpclientwrong/  https://www.stevejgordon.co.uk/httpclient-creation-and-disposal-internals-should-i-dispose-of-httpclient  https://www.assemblyai.com/blog/getting-started-with-httpclientfactory-in-c-sharp-and-net-5/  https://www.stevejgordon.co.uk/httpclientfactory-named-typed-clients-aspnetcore  https://andrewlock.net/understanding-scopes-with-ihttpclientfactory-message-handlers/  https://andrewlock.net/exporing-the-code-behind-ihttpclientfactory/  https://docs.microsoft.com/en-us/dotnet/architecture/microservices/implement-resilient-applications/  https://app.pluralsight.com/library/courses/using-httpclient-consume-apis-dot-net"
    },
  
    {
      "id": "60",
      "title": "Creating and Using HTTP Client SDKs in .NET 6. (Announcement)",
      "url": "/dotnet/aspnetcore/microservices/2022/02/06/http-sdks.html",
      "date": "February 06, 2022",
      "categories": ["dotnet","aspnetcore","microservices"],
      "tags": ["dotnet","microservices","aspnetcore","api"],
      "shortinfo": "There are many ways of developing HTTP Client SDKs. This article helps you to choose the right one according to your scenario.",
      "content": "This article was prepared in collaboration with InfoQ. Hope you enjoy it!.Key Takeaways  Writing and maintaining HTTP Client SDKs is a very important skill for modern .NET developers working with distributed systems.  In order to properly manage HTTP connections, you need to design your API Clients to be ready to be consumed from any Dependency Injection container.  A good client SDK is composable, providing straightforward ways to configure and extend it.  Testing HTTP Client SDKs can be very beneficial in certain scenarios and it gives you additional confidence in your code.  There are many ways of developing HTTP Client SDKs. This article helps you to choose the right one according to your scenario.See full article at: https://www.infoq.com/articles/creating-http-sdks-dotnet-6/ üëÄ  Originally published at https://nikiforovall.github.io/dotnet/aspnetcore/microservices/2022/04/04/creating-and-using-http-sdks.html"
    },
  
    {
      "id": "61",
      "title": "Console applications with Spectre.Console",
      "url": "/dotnet/2022/01/22/building-console-application-with-spectre-console.html",
      "date": "January 22, 2022",
      "categories": ["dotnet"],
      "tags": ["dotnet","cli","console"],
      "shortinfo": "Learn an easy way to develop CLI applications",
      "content": "TL;DRSpectre.Console not only makes it easier to create beautiful console applications, but also it provides an application model to bind args[] to git-style commands.var app = new CommandApp();app.Configure(c =&gt;{    c.AddCommand&lt;ExportBots&gt;(\"scrape\");    c.AddCommand&lt;ListBots&gt;(\"list\");    c.AddCommand&lt;DownloadBot&gt;(\"download\")        .WithExample(new[] {\"download\", \"--random\"});});await app.RunAsync(args);Source code: https://github.com/NikiforovAll/cli-with-spectre-console.Spectre.Console.Extensions - https://github.com/NikiforovAll/Spectre.Console.Extensions.IntroductionPreviously, I shared with you how to use System.CommandLine to develop CLI applications in my ‚ÄúDevelop Clean Command Line Applications with System.CommandLine. Clean CLI‚Äù post. If you liked the approach and want to try an alternative solution, check out Spectre.Console.UI KitYou can use Spectre.Console.AnsiConsole to:  Output text with different colors and styles  Interact with the user - prompt, selection  Render complex UI elements/widgets: tables, trees, ASCII images, status control, progress bars, etc.Credits: https://spectreconsole.net/ExtensionsSpectre.Console is quite extensible and you can build custom IRenderable components on top of it. This is what I did. I developed additional widgets for Spectre.Console and put them inside NuGet packages Spectre.Console.Extensions:            Package      Version      Description                  Spectre.Console.Extensions.Progress            IProgress adapter and HttpClient reporting.              Spectre.Console.Extensions.Table            DataTable and DataSet support.      Spectre.Console.Extensions.Progress provides an adapter of BCL‚Äôs System.IProgress to Spectre.Progress. Also, there is an option to plug HttpClient into Spectre.Progress so you can automatically report progress of downloading something.Spectre.Console.Extensions.Table allows to draw well-known System.Data.DataSet and System.Data.DataTable by using Spectre.Table.Example - .NET Bots scrapperI will show you how to develop CLI application based on Spectre.Console.The goal is to fetch bots from .NET bot gallery https://mod-dotnet-bot.net/gallery/ and show a bot inside a console.Here is --help output looks like:$ dotnet run -- -hUSAGE:    dotnet-bots.dll [OPTIONS] &lt;COMMAND&gt;EXAMPLES:    dotnet-bots.dll download dotnet-bot-1.png    dotnet-bots.dll download --randomOPTIONS:    -h, --help       Prints help information    -v, --version    Prints version informationCOMMANDS:    scrape    list    downloadEntry Point - Program.csvar services = new ServiceCollection();services.AddDbContext&lt;RobotContext&gt;(opt =&gt; opt.UseSqlite(\"Data Source=robots.db\"));services.AddHttpClient();var app = new CommandApp(new TypeRegistrar(services));app.Configure(c =&gt;{    c.AddCommand&lt;ExportBots&gt;(\"scrape\");    c.AddCommand&lt;ListBots&gt;(\"list\");    c.AddCommand&lt;DownloadBot&gt;(\"download\")        .WithExample(new[] {\"download\", \"dotnet-bot-1.png\"})        .WithExample(new[] {\"download\", \"--random\"});});await app.RunAsync(args);Spectre.Console.Cli.CommandApp defines the structure of the CLI application.It is very easy to define git-style (verb) commands. As you can see, we adding three commands to the configuration.If you have a more complex scenario, you might want to use AddBranch method you can organize your commands in a tree-like structure, e.g.:var app2 = new CommandApp();app2.Configure(c =&gt;{    c.AddBranch(\"bots\", bots =&gt;    {        // bot.exe bots list        bots.AddCommand&lt;ListBots&gt;(\"list\");        bots.AddBranch(\"download\", create =&gt;        {            // bot.exe bots download table            create.AddCommand&lt;ExportBots&gt;(\"table\");            // bot.exe bots download bot &lt;bot-name.png&gt;            create.AddCommand&lt;DownloadBot&gt;(\"bot\");        });    });});Export bots - ExportBots.csSpectre.Console.Cli.Command&lt;T&gt; defines Execute method, it is a handler and called by CommandApp. The command is created by Dependency Injection container (DI), so you can expect dependencies to be resolved via constructor injection. In the command below, we retrieve bots from external source and store them.üí° You can wire up the DI container if you need by providing an implementation of ITypeRegistrar as the constructor parameter. Note, there are no built-in adapters in Spectre.Console.Cli, but you can easily find a way to implement one. Check official documentation or source code for this post.public class ExportBotsSettings : CommandSettings {}public class ExportBots : Command&lt;ExportBotsSettings&gt;{    private readonly RobotContext db;    public ExportBots(RobotContext db) =&gt; this.db = db;    public override int Execute(CommandContext context, ExportBotsSettings settings)    {        // details are omitted for brevity        // the actual implementation parses HTML file and collects bots        var robots = FindRobots(doc).ToList();        this.db.Database.EnsureCreated();        this.db.AddRange(robots);        try        {            this.db.SaveChanges();        }        catch (Exception e) when (e.InnerException is not null)        {            AnsiConsole.WriteException(e.InnerException);            return -1;        }        AnsiConsole.WriteLine();        AnsiConsole.Markup(\"[default on grey] exported robots [/]\");        AnsiConsole.Markup($\"[white on green] {robots.Count} [/]\");        AnsiConsole.WriteLine();        return 0;    }}  ‚ûï AnsiConsole.WriteException prints colored exceptions  ‚ûïAnsiConsole.Markup outputs rich text to console.List bots - ListBots.csDisplaying tabular data is a very common task, here is how you can use Spectre.Console.Extensions to draw a table using Entity Framework. Basically, we want to transform the query to System.Data.DataTable object. You can always switch to Spectre.Table, it gives you full control over how you display data.The method public static IRenderable FromDataSet(this DataSet dataSet, Action&lt;Panel&gt;? configurePanel) builds table that Spectre.Console can display.public class ListBots : Command&lt;ListBotsSettings&gt;{    private readonly RobotContext db;    public ListBots(RobotContext db) =&gt; this.db = db;    public override int Execute(CommandContext context, ListBotsSettings settings)    {        AnsiConsole.Write(new FigletText(\".NET Bots\").Centered().Color(Color.Purple));        var dataset = new DataSet {DataSetName = \"Bot Gallery\",};        var connection = this.db.Database.GetDbConnection();        dataset.Tables.Add(RetrieveDataTable(connection, this.db.Robots));        var dataSetToDisplay = dataset.FromDataSet(opt =&gt; opt.BorderColor(Color.Aqua));        AnsiConsole.Write(dataSetToDisplay);        return 0;    }    private static DataTable RetrieveDataTable(DbConnection connection, IQueryable query)    {        connection.Open();        using var cmd = connection.CreateCommand();        cmd.Connection = connection;        cmd.CommandType = CommandType.Text;        cmd.CommandText = query.ToQueryString();        using var reader = cmd.ExecuteReader();        var table = new DataTable();        table.Load(reader);        return table;    }}  ‚ûï Spectre.Console.FigletText renders FIGlet text.  ‚ûï Spectre.Console.Extensions.Table.FromDataSet and Spectre.Console.Extensions.Table.FromDataTable displays tabular data as Spectre.Console tables.Download and display a bot - DownloadBot.csLet‚Äôs see how to retrieve a bot by name from the database and download and display ASCII image of the bot.Before we delve into details let‚Äôs see the overall structure of the command.public class DownloadBotSettings : CommandSettings{    [CommandArgument(0, \"[name]\")]    [Description(\"Download bot by name\")]    public string? Name { get; set; }    [CommandOption(\"-r|--random\")]    [Description(\"Specifies if random bot should be stored in the system\")]    public bool IsRandom { get; set; }}public class DownloadBot : AsyncCommand&lt;DownloadBotSettings&gt;{    private readonly HttpClient httpClient;    private readonly RobotContext db;    public DownloadBot(HttpClient httpClient, RobotContext db)    {        this.httpClient = httpClient;        this.db = db;    }    public override async Task&lt;int&gt; ExecuteAsync(CommandContext context, DownloadBotSettings settings)    {        // omitted for brevity, will be explained in a moment    }}As you may notice, instead of Command we use AsyncCommand. It allows us to use async/await inside handler code, so you don‚Äôt need to do sync-over-async, which is not critical for this scenario, but still is a bad design choice if you have an alternative.DownloadBotSettings contains two properties. The first one is a positional argument and the second an option/flag. Spectre.Console binds the CommandSettings for you, don‚Äôt forget to pass args in your Program.cs await app.RunAsync(args);.Let‚Äôs see the insides of the handler. Everything starts with user input processing and we want to make sure we have a bot name to work with.If a user doesn‚Äôt specify bot name we can ask a user by using AnsiConsole.Promptprivate void EnsureBotSettings(DownloadBotSettings settings){    if (settings.IsRandom)    {        var toSkip = Random            .Shared.Next(0, this.db.Robots.Count());        settings.Name = this.db            .Robots.Skip(toSkip).Take(1).First().Name;    }    else if (string.IsNullOrWhiteSpace(settings.Name))    {        settings.Name = AnsiConsole.Prompt(            new SelectionPrompt&lt;string&gt;()                .Title(\"What's your [green]favorite fruit[/]?\")                .PageSize(5)                .AddChoices(this.db.Robots.Select(r =&gt; r.Name)));    }}Exported bots are stored in the database, so we want to find one and try to retrieve the corresponding image and store it locally somewhere. Next, we will use Spectre.Console.ImageSharp to display saved image.public override async Task&lt;int&gt; ExecuteAsync(CommandContext context, DownloadBotSettings settings){    this.EnsureBotSettings(settings);    var robot = this.db.Robots.First(r =&gt; r.Name == settings.Name);    var fileName = string.Empty;    var httpRequestMessage = new HttpRequestMessage(HttpMethod.Get, robot.Uri);    await AnsiConsole.Progress()        .Columns(new ProgressColumn[]        {            new TaskDescriptionColumn(), new ProgressBarColumn(), new PercentageColumn(),            new RemainingTimeColumn(), new SpinnerColumn(),        }).StartAsync(this.httpClient, httpRequestMessage, \"Downloading a bot\", SaveImageToRandomFile);    var rule = new Rule($\"[red]{robot.Name}[/]\") {Style = Style.Parse(\"red dim\")};    AnsiConsole.Write(rule);    var image = new CanvasImage(fileName).MaxWidth(16);    AnsiConsole.WriteLine();    AnsiConsole.Write(new Panel(image).BorderColor(Color.Maroon));    return 0;    async Task SaveImageToRandomFile(Stream stream)    {        fileName = Path.GetTempPath() + Guid.NewGuid().ToString() + \".png\";        await using var fileStream = File.Create(fileName);        stream.Seek(0, SeekOrigin.Begin);        await stream.CopyToAsync(fileStream);    }  ‚ûï Spectre.Console.SelectionPrompt prompts user to select an option from provided list.  ‚ûï Spectre.Console.Extensions.Progress.StartAsync adds HttpClient reporting capabilities by using Spectre.Console.Progress. ‚ö† You might want to check the implementation before using it because it performs additional buffering to do the reporting trick.  ‚ûï Spectre.Console.ImageSharp.CanvasImage displays ASCII images to console.üí° For full-featured terminal UI applications (TUIs) you might want to try something like Terminal.GuiSummarySpectre.Console gives you everything you need to start developing good-looking and functional CLI applications. It gives you enough structure and building blocks to naturally evolve your applications over time."
    },
  
    {
      "id": "62",
      "title": "Use System.CommandLine to write .NET global tools. Copy-paste-driven development with copy-paster.",
      "url": "/dotnet/2021/12/20/dotnet-global-tool-copy-paster.html",
      "date": "December 20, 2021",
      "categories": ["dotnet"],
      "tags": ["dotnet","productivity","cli","console"],
      "shortinfo": "Learn how to create a .NET tool.",
      "content": "TL;DRLearn how to write a well-structured CLI application and pack it as a .NET tool.IntroductionPreviously, I wrote a blog post that explains to you how to create something called ‚ÄúClean CLI‚Äù - a combination of CLI application and Clean Architecture. This time I want to show you how to build .NET tool using the same techniques.The .NET CLI lets you create a console application as a tool, which others can install and run. .NET tools are NuGet packages that could be installed from the .NET CLI.A tool can be installed in the following ways:  As a global tool - The tool binaries are installed in a default directory that is added to the PATH environment variable.  As a local tool.  As a global tool in a custom location. The tool binaries are installed in a default directory. You invoke the tool from the installation directory or any of its subdirectories.Example application - Copy-paste-driven development with copy-pasterI like to optimize little things in my developer inner-loop. Lately, I found myself copying some code from GitHub over and over again. You may call me a copy-paste-driven development practitioner. I decided to write a tool to simplify efforts of downloading files from github - copy-paster (copa for short). At first, it was a joke (a meme if you like), I just wanted to show you how to write a .NET tool by example, but now, I really use copa üòÖ.Usage:copa https://github.com/NikiforovAll/copy-paster/blob/main/LICENSE.NET tools can be found by running dotnet tool search or simply use https://nuget.org (there is a filter for global tools).~\\dev‚ûú  dotnet tool search copy-pasterPackage ID                   Latest Version      Authors           Downloads      Verified------------------------------------------------------------------------------------------nikiforovall.copypaster      1.0.0               nikiforovall      0Once discovered, a tool can be installed like this:dotnet tool install --global NikiforovAll.CopyPasterCreate global tool üî®Let‚Äôs start off by creating simple console application.dotnet new console -n copy-pasterInstall main dependencies to build a .NET tool.dotnet add package System.CommandLine # command line parsing and invocationdotnet add package System.CommandLine.Hosting # Plug dependency injection containerdotnet add package Microsoft.Extensions.Hosting # Default DI containerdotnet add package Microsoft.Extensions.DependencyInjection.Abstractionsdotnet add package Spectre.Console # Makes it easier to create beautiful console appsAs mentioned, .NET tool is nothing but a console application packaged and distributed as a NuGet package. The only thing to do is to add special properties used by MSBuild to package .NET tools.&lt;Project Sdk=\"Microsoft.NET.Sdk\"&gt;  &lt;PropertyGroup&gt;    &lt;ToolCommandName&gt;copa&lt;/ToolCommandName&gt;    &lt;PackAsTool&gt;True&lt;/PackAsTool&gt;    &lt;OutputType&gt;Exe&lt;/OutputType&gt;    &lt;TargetFramework&gt;net6&lt;/TargetFramework&gt;  &lt;/PropertyGroup&gt;&lt;/Project&gt;Application‚Äôs entry point looks like this:using System.CommandLine;using System.CommandLine.Builder;using System.CommandLine.Hosting;using System.CommandLine.Parsing;using Microsoft.Extensions.DependencyInjection;using Microsoft.Extensions.Hosting;using NikiforovAll.CopyPaster.Commands;using NikiforovAll.CopyPaster.Services;var runner = new CommandLineBuilder(new DownloadFromGithub())    .UseHost(_ =&gt; new HostBuilder(), (builder) =&gt; builder        .ConfigureServices((_, services) =&gt;        {            services.AddHttpClient&lt;IGithubCodeDownloader, GitHubCodeDownloader&gt;();            services.AddSingleton(new FileSaver());        })        .UseCommandHandler&lt;DownloadFromGithub, DownloadFromGithub.Handler&gt;())        .UseDefaults().Build();await runner.InvokeAsync(args);  System.CommandLine.Builder.CommandLineBuilder is used to define CLI applications. It is responsible for the way your CLI looks and feels.  System.CommandLine.Hosting.HostingExtensions.UseHost accepts Microsoft.Extensions.Hosting.IHostBuilder as input parameter and enables Dependency Injection configuration. For example, you can register an implementation of System.CommandLine.Invocation.ICommandHandler in DI container, so it will be automatically resolved from the DI container during command execution.  System.CommandLine.Hosting.UseCommandHandler connects System.CommandLine.Command to System.CommandLine.Invocation.ICommandHandler.  CommandLineBuilder.Build() returns System.CommandLine.Parsing.Parser that is used to parse arguments and to run the CLI application.The anatomy of command is pretty simple:public class DownloadFromGithub : RootCommand{    public DownloadFromGithub()        : base(\"Copies file by url\")    {        this.AddArgument(new Argument&lt;string&gt;(\"url\", \"url\"));        this.AddOption(new Option&lt;string&gt;(            new string[] { \"--output\", \"-o\" }, \"Output file path\"));    }    public new class Handler : ICommandHandler    {        public string? Url { get; set; }        public string? Output { get; set; }        public Handler(IGithubCodeDownloader codeDownloader, FileSaver fileSaver)        {            //...        }        public async Task&lt;int&gt; InvokeAsync(InvocationContext context)        {            //...        }    }}  Arguments and options are registered in a declarative manner inside the constructor.  Every System.CommandLine.Command command contains associated handler with it. public ICommandHandler? Handler {get; set;} but in the DI scenario we want to let DI to do it for us. ICommandHandler can be declared anywhere in your project, but I prefer to locate it inside the Command declaration  ICommandHandler.InvokeAsync is used to describe how the command should be processed. Note, in order to accept input parameters (arguments, options, etc.) you need to define public properties inside ICommandHandler, names of the properties should match the full names specified in a command declaration. E.g. ‚Äù‚Äìoutput‚Äù option corresponds to public string? Output { get; set; }.Publish üì¢Since every .NET tool is a NuGet package. You can specify additional build parameters to improve discoverability and overall user experience at https://nuget.org&lt;Project&gt;  &lt;PropertyGroup Label=\"Authoring\"&gt;    &lt;Company&gt;nikiforovall&lt;/Company&gt;    &lt;Authors&gt;nikiforovall&lt;/Authors&gt;  &lt;/PropertyGroup&gt;  &lt;PropertyGroup Label=\"Package\"&gt;    &lt;Title&gt;CopyPaster (copa)&lt;/Title&gt;    &lt;Description&gt;Enables copy-paste driven development. CopyPaster (aka copa)&lt;/Description&gt;    &lt;PackageTags&gt;global-tool;copy-paste;productivity&lt;/PackageTags&gt;    &lt;PackageLicenseExpression&gt;MIT&lt;/PackageLicenseExpression&gt;    &lt;PackageProjectUrl&gt;https://github.com/nikiforovall/copy-paster&lt;/PackageProjectUrl&gt;    &lt;PackageIcon&gt;Icon.jpg&lt;/PackageIcon&gt;    &lt;PackageReleaseNotes&gt;https://github.com/nikiforovall/copy-paster/releases&lt;/PackageReleaseNotes&gt;  &lt;/PropertyGroup&gt;  &lt;PropertyGroup Label=\"Repository\"&gt;    &lt;RepositoryUrl&gt;https://nuget.org/packages/NikiforovAll.CopyPaster&lt;/RepositoryUrl&gt;    &lt;RepositoryType&gt;git&lt;/RepositoryType&gt;    &lt;NeutralLanguage&gt;en-US&lt;/NeutralLanguage&gt;  &lt;/PropertyGroup&gt;&lt;/Project&gt;Once ready, all you need to do is to pack the NuGet package and upload it. I use Cake but it is totally fine to pack it with dotnet pack.Task(\"Pack\")    .Description(\"Creates NuGet packages and outputs them to the artefacts directory.\")    .Does(() =&gt;    {        DotNetPack(            \".\",            new DotNetPackSettings()            {                Configuration = configuration,                IncludeSymbols = true,                MSBuildSettings = new DotNetMSBuildSettings(),                NoBuild = false,                NoRestore = false,                OutputDirectory = artefactsDirectory,            });    });Before publishing, you may want to install a tool from a folder:dotnet tool install --global --add-source ./ArtefactsSummaryI hope this blog post motivates you to create your own .NET tools. Please feel free to contribute to copy-paster. Thanks üëã.Reference  https://docs.microsoft.com/en-us/dotnet/core/tools/global-tools  https://docs.microsoft.com/en-us/dotnet/core/tools/global-tools-how-to-create  https://nikiforovall.github.io/dotnet/cli/2021/06/06/clean-cli.html  https://github.com/NikiforovAll/copy-paster"
    },
  
    {
      "id": "63",
      "title": "A developer guide to automated testing with Postman. Run postman collections inside Docker via newman CLI.",
      "url": "/dotnet/docker/testing/2021/11/27/testing-with-newman.html",
      "date": "November 27, 2021",
      "categories": ["dotnet","docker","testing"],
      "tags": ["dotnet","docker","testing","postman","cli"],
      "shortinfo": "See a practical example of how to organize automated postman collection execution.",
      "content": "TL;DRSee a practical example of how to write and execute postman collections inside a Docker container. Source code https://github.com/NikiforovAll/testing-with-newman-demo.IntroductionWhat is Automated Testing?  Tests are automated by creating test suites that can run again and again. Postman can be used to automate many types of tests including unit tests, functional tests, integration tests, end-to-end tests, regression tests, mock tests, etc. Automated testing prevents human error and streamlines testing.Postman offers a comprehensive API testing tool that makes it easy to set up automated tests. Organizing your requests into Postman Collections enables you to run and automate a series of requests.An ExampleLet‚Äôs say we want to start refactoring of some microservice called ‚ÄúOrder Service‚Äù. Before we start our refactoring we want to make sure that we don‚Äôt break anything in the meantime. A very common migration strategy is to invest time into writing tests to lower the risk of regression bugs and migration errors. We will use postman and cover ‚ÄúOrder Service‚Äù with automated API tests to ensure a successful migration.In this example, ‚ÄúOrder Service‚Äù is quite small and has one responsibility of order processing.Use cases:  As a user I want to be able to create orders so it is possible to buy goods for money  As a user I want to close order  As a user I expect order status to be completed when someone successfully pays for it.public static class OrderApiRoutes{    public static WebApplication MapOrderApiRoutes(this WebApplication app)    {        app.MapPost(\"/orders/\", CreateOrder);        app.MapGet(\"/orders\", GetOrders);        app.MapGet(\"/orders/{id}\", GetOrderById).WithName(nameof(GetOrderById));        app.MapPut(\"/orders/{id}/cancel\", CancelOrderById);        return app;    }    private static async Task&lt;IResult&gt; CancelOrderById(ulong id, IMongoClient mongoClient)    {        var db = mongoClient.GetOrderCollection();        var order = await db.Find(x =&gt; x.Id == id).FirstOrDefaultAsync();        if(order is null)        {            return Results.NotFound();        }        order.Cancel();        await db.ReplaceOneAsync(x =&gt; x.Id == id, order);        return Results.NoContent();    }    private static async Task&lt;Order&gt; GetOrderById(ulong id, IMongoClient mongoClient)    {        var db = mongoClient.GetOrderCollection();        return await db.Find(x =&gt; x.Id == id).FirstOrDefaultAsync();    }    private static async Task&lt;IResult&gt; CreateOrder(        Order order, IMongoClient mongoClient, Generator idGenerator)    {        order = order with        {            Id = idGenerator.NextLong(),            CreatedAt = DateTime.Now        };        var db = mongoClient.GetOrderCollection();        await db.InsertOneAsync(order);        return Results.CreatedAtRoute(nameof(GetOrderById), new { id = order.Id });    }    private static async Task&lt;IEnumerable&lt;Order&gt;&gt; GetOrders(IMongoClient mongoClient)    {        var db = mongoClient.GetOrderCollection();        return await db.Find(x =&gt; true).ToListAsync();    }}Please see source code for more details: https://github.com/NikiforovAll/testing-with-newman-demoWriting requestsPersonally, I‚Äôm a big fan of Postman because it has an intuitive UI that drastically improves your performance. I recommend you invest some time into learning it.I will keep it straightforward and will show you how to create order in the system and then close it. Before we start sending HTTP requests we want to create ‚ÄúPostman Collection‚Äù and ‚ÄúPostman Environment‚Äù.  See: ‚ÄúCreating your first collection‚Äù          Create ‚Äútesting-with-newman‚Äù collection      Add folder ‚Äúmain-flow‚Äù        Create environment ‚Äútesting-with-newman‚Äù and add base-url and rabbitmq-host variables. Select newly created environment on the top-right.  Add ‚ÄúCreate Order‚Äù request to the main-flow folder. As you can see, we want to send HTTP POST to {base-url}/orders with the JSON body. Now, we are ready to send the request and see how it works.  Start the server cd ./src/OrderService &amp;&amp; dotnet run and send HTTP request through Postman UI. It looks like the order has been accepted and ‚Äú201 CREATED‚Äù was returned. You can check the ‚ÄúLocation‚Äù header to see the address of the created resource.  Once we are comfortable with Postman and ‚ÄúOrder Service‚Äù we can start writing ‚ÄúPostman Tests‚Äù to verify expected behavior. Tests are written based on JavaScript. Postman UI helps you to write tests by providing tons of examples, see the right sidebar. Down below, we make sure that the order request return status is 201 and the customer field contains ‚ÄúJohn Doe‚Äù. As you can see, we also writing ‚ÄúorderId‚Äù into the environment variable to use it in the next requests.pm.test(\"Status code is 201\", function () {    pm.response.to.have.status(201);});pm.sendRequest(pm.response.headers.get(\"Location\"), function (err, res) {    pm.test(\"Order created\", function () {        pm.expect(res.code).to.eql(200);        pm.environment.set(\"orderId\", res.json().id);        pm.test(\"Order has customer\", function () {            pm.expect(res.json().customer).to.eql('John Doe');        });    });});Writing requests. Publish to RabbitMQAlso, you can publish messages to RabbitMQ from Postman. It is possible because RabbitMQ exposes HTTP client that enables management. To publish a message, you need to specify an exchange in the URL  {rabbitmq-host}/api/exchanges/%2F/amq.default/publish and provide the message as part of JSON body. Note %2F is the url-encoded default Virtual Host ‚Äú/‚Äù.{    \"properties\": {        \"content_type\": \"application/json\"    },    \"routing_key\": \"order-paid\",    \"payload\": \"{\\\"orderId\\\": \\\"244165178826752\\\"}\",    \"payload_encoding\": \"string\"}Because publishing a message it is by nature async operation. All we can do at this point is to verify that it was routed (stored in a queue).pm.test(\"Is Routed\", function () {    var jsonData = pm.response.json();    pm.expect(jsonData.routed).to.eql(true);});Finally, we want to see that the order has been completed as the result of the previous operation, we do it by sending GET {base-url}/orders/{orderId} and running the tests:pm.test(\"Status code is 200\", function () {    pm.response.to.have.status(200);});pm.test(\"Order is Completed\", function () {    pm.expect(pm.response.json().status).to.eql(\"Completed\");});That was ‚Äúmain-flow‚Äù. I suggest you check ‚Äúcancel-flow‚Äù on your own. Let‚Äôs see how to run Postman collections from CLI.Running Postman Collections from CLI via newmanYou can export collections and environments from Postman UI and use exported files locally. We are going to set up collection run inside docker-composeRunning postman collections inside Docker has many benefits:  Easy to write integration tests. Testing allows you to investigate how the system works. Huge productivity boost. Try it and you will like it.  Short feedback cycle. You can use it during your inner developer-loop.  Integration testing could be outsourced to a different team/people.Here is docker-compose.postman.yml:version: \"3.4\"services:  main-flow:    image: postman/newman_alpine33    command:      run testing-with-newman.postman_collection.json      --environment testing-with-newman.postman_environment.json      --folder main-flow      -r cli      --delay-request 500      --iteration-count 1      --color on    volumes:      - ./tests/postman:/etc/newman    networks:      - newman-tests  cancel-flow:    image: postman/newman_alpine33    command:      run testing-with-newman.postman_collection.json      --environment testing-with-newman.postman_environment.json      --folder cancel-flow      -r cli      --iteration-count 1      --color on    volumes:      - ./tests/postman:/etc/newman    networks:      - newman-testsnetworks:  newman-tests:    external: true    name: newman-testsTo run an e2e scenarios - executedocker compose -f docker-compose.postman.yml up main-flowdocker compose -f docker-compose.postman.yml up cancel-flowRunning newman as part of the continuous integration process. GitHub Actions exampleOne of the greatest benefits of newman is that you can use it as part of your CI.name: testson:  push:jobs:  build:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v2      - name: Setup network        run: docker network create newman-tests      - name: Build the docker-compose stack        run: docker-compose -f docker-compose.override.yml -f docker-compose.yml up -d      - name: Check running containers        run: docker ps -a      - name: Check logs        run: docker-compose logs order-service      - name: Run test suite main-flow        run: docker-compose -f docker-compose.postman.yml up main-flow      - name: Run test suite cancel-flow        run: docker-compose -f docker-compose.postman.yml up cancel-flowüí° Hint: to run github actions locally use https://github.com/nektos/actSummaryWriting tests is what makes a good developer an excellent developer. In this blog post, we‚Äôve covered how to set up integration tests as runnable postman collections inside Docker.Reference  https://www.postman.com/automated-testing/  https://www.sm-cloud.com/testing-api-with-postman/  https://github.com/nektos/act"
    },
  
    {
      "id": "64",
      "title": "Rapid Microservices Development in .NET. An introduction.",
      "url": "/dotnet/microservices/csharp/2021/11/06/rapid-microservices-development-part1-build-project.html",
      "date": "November 06, 2021",
      "categories": ["dotnet","microservices","csharp"],
      "tags": ["dotnet","microservices"],
      "shortinfo": "Learn how to speed up microservices development process by using set of predefined templates and practices.",
      "content": "TL;DRLearn how to speed up microservices development process by using set of predefined templates and practices. See https://www.nuget.org/packages/NikiforovAll.CleanArchitecture.Templates/Problem StatementThe way we develop software is built around the idea of high velocity and readiness to change and adapt to the market‚Äôs requirements. This approach brings up the next questions:  How to successfully contribute to the existing code base?  How to not break something in meantime?  How do we reduce the learning curve of an existing solution?(1) Actually, all you need to do is to write clean, understandable, and open for modification code. The actual craft of writing code is honed through years of learning and making mistakes. Component-level design is important and you can benefit from good predefined solution templates, something like Clean Architecture gives you a good starting point. Also, if you feel like your project has some complex domain you might want to incorporate DDD in the mix. Personally, I find it really useful because it facilitates communication between developers and domain experts. It is easier to map code snippets to domain and project requirements.(2) Frequent changes increase the possibility of bugs, as software engineers, we deal with it by extensively testing a codebase. At first glance, you might think writing tests is an additional effort that prevents you from shipping some valuable and urgent features. But, in practice, it is actually another way around, you want to invest in a testing toolkit to save future self from some nasty bugs. Also, tests serve as live documentation and enable refactoring. My suggestion is to be practical about it, don‚Äôt try to get 100% coverage by writing tons of useless unit tests, determine what is the best in a given context. Simply, write tests, my dude.(3) Clean code reduces the cognitive load and overall complexity of a codebase. In my opinion, for rapid development, you need somewhat consistency for technical decisions. It is a good idea to use common frameworks, libraries, tools, and even cross-cutting concerns code, just make sure it doesn‚Äôt introduce additional coupling and unnecessary complexity.ProposalLuckily, there is the remedy! Rapid Microservices Development RMD === \"remedy\", got it? üòèAs an organization responsible for developing microservices solutions you want to build reusable components so it will be easy to create a new microservice from the scratch. Creating project templates is a well-known approach and it is quite simple. Rapid microservice development is a goal and templates might be a viable solution, just be mindful about the goal.Templates FamilyI‚Äôve created a set of project templates that provides you information of how you might organize microservices solutions yourself. Note, each template might be used individually outside of microservices development context.Feedback is highly appreciated. üôè  https://www.nuget.org/packages/NikiforovAll.CleanArchitecture.Templates/All you need to do is to install it via running the command:dotnet new --install NikiforovAll.CleanArchitecture.Templates::1.1.1Once installed, you can see a list of templates by running:$ dotnet new -l na-# These templates matched your input: 'na-'# Template Name                Short Name  Language  Tags# ---------------------------  ----------  --------  --------------------------------------------# Build Project Template       na-bu       bash      build-project/Template# Clean Architecture Template  na-ca       [C#]      CleanArchitecture/DDD/Template# Event Sourcing Template      na-es       [C#]      EventSourcing/CleanArchitecture/DDD/Template# Gateway Template             na-ga       [C#]      gateway/Template            Name      Alias      Repository      Status                  Build Project      na-bu      https://github.com/NikiforovAll/na-bu      N/A              Clean Architecture Template      na-ca      https://github.com/NikiforovAll/na-ca                                                      Event Sourcing Template      na-bu      https://github.com/NikiforovAll/na-es                                                      Gateway      na-ga      https://github.com/NikiforovAll/na-ga                                              Build projectIn this blog post I will show you how to create a one of the components yourself.The responsibility of build project is a starting project of any developer. The main goal is to have zero-configuration required to get the project up and running, this is really important and people will say thank you for that.Build project consists of something like:  Scripts to pull code base and latest changes from the remotes.  Scripts to manage infrastructure used during development. All you need to do is to run docker compose service1, service2, ... to run the system locally. This is really useful.  Projects assets, something like architecture documentation, postman collection to speed up manual developer testing, guidelines, etc.I will explain the anatomy of the build project from the template in a moment. But first, we need to generate one:$ dotnet new na-bu -n MyFirstBuildProject --dry-runFile actions would have been taken:  Create: ./.env  Create: ./.gitignore  Create: ./.vscode/settings.json  Create: ./assets/http/gateway/projects.http  Create: ./assets/http/naca/projects.http  Create: ./assets/http/naes/projects.http  Create: ./build/docker_postgres_init.sql  Create: ./build/execute-tests.sh  Create: ./build/generate-report.sh  Create: ./build/run-services.sh  Create: ./build/setup-infrastructure.sh  Create: ./docker-compose-local-infrastructure.yml  Create: ./docker-compose-tests.override.yml  Create: ./docker-compose-tests.yml  Create: ./docker-compose.override.yml  Create: ./docker-compose.yml  Create: ./docker-images.txt  Create: ./README.md  Create: ./routes.conf.json  Create: ./scripts/generate-report.sh  Create: ./scripts/git-clone-all.sh  Create: ./scripts/git-pull-all.sh  Create: ./scripts/git-summary/.gitignore  Create: ./scripts/git-summary/git-summary.sh  Create: ./scripts/git-summary/README.md  Create: ./scripts/git-summary/screenshot.png  Create: ./scripts/open-in-browser.sh$ dotnet new na-bu -n MyFirstBuildProjectThe template \"Build Project Template\" was created successfully.As you can see we have:  üìÇ ./scripts - some tools and scripts to manage build project.  üìÇ ./build - commands to build, start and test the solution.  üìÇ ./assets - project artifacts  üìÇ docker-compose.yml, docker-compose-local-infrastructure.yml, docker-compose.override.yml - run system locally  üìÇ docker-compose-tests.yml, docker-compose-tests.override.yml - run tests locallyThe template pulls three other predefined templates na-ca, na-es, na-ga. You can change that.Let‚Äôs clone microservices from the na-bu template:$ ./scripts/git-clone-all.sh========================================================Cloning repository: na-ca========================================================Cloning into 'na-ca'...remote: Enumerating objects: 1146, done.remote: Counting objects: 100% (1146/1146), done.remote: Compressing objects: 100% (568/568), done.remote: Total 1146 (delta 673), reused 968 (delta 506), pack-reused 0Receiving objects:  96% (1101/1146)Receiving objects: 100% (1146/1146), 172.02 KiB | 978.00 KiB/s, done.Resolving deltas: 100% (673/673), done.========================================================Cloning repository: na-es========================================================Cloning into 'na-es'...remote: Enumerating objects: 1019, done.remote: Counting objects: 100% (1019/1019), done.remote: Compressing objects: 100% (523/523), done.remote: Total 1019 (delta 571), reused 870 (delta 430), pack-reused 0RReceiving objects: 100% (1019/1019), 173.87 KiB | 1.26 MiB/s, done.Resolving deltas: 100% (571/571), done.Updating files: 100% (183/183), done.========================================================Cloning repository: na-ga========================================================Cloning into 'na-ga'...remote: Enumerating objects: 53, done.remote: Counting objects: 100% (53/53), done.remote: Compressing objects: 100% (35/35), done.remote: Total 53 (delta 15), reused 46 (delta 8), pack-reused 0Receiving objects: 100% (53/53), 18.59 KiB | 2.66 MiB/s, done.Resolving deltas: 100% (15/15), done.Check the current state of the solution:$ ./scripts/git-summary/git-summary.shRepository                       Branch Remote State================================ ==== ====== =====/d/dev/MyFirstBuildProject       main ?  --/d/dev/MyFirstBuildProject/na-ca main origin/d/dev/MyFirstBuildProject/na-es main origin/d/dev/MyFirstBuildProject/na-ga main originNow we can build and run the solution:$ ./build/run-services.sh startCreating network \"myfirstbuildproject_default\" with the default driverCreating volume \"myfirstbuildproject_rabbitmqdata-nikiforovall\" with local driverCreating volume \"myfirstbuildproject_postgresdata-nikiforovall\" with local driverCreating volume \"myfirstbuildproject_seq-nikiforovall\" with local driver# ...Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix themCreating seq                                ... doneCreating myfirstbuildproject_postgres_1     ... doneCreating myfirstbuildproject_rabbitmq_1     ... doneCreating myfirstbuildproject_naga.gateway_1 ... doneCreating myfirstbuildproject_naes.api_1     ... doneCreating myfirstbuildproject_naes.worker_1  ... doneCreating myfirstbuildproject_naca.worker_1  ... doneCreating myfirstbuildproject_naca.api_1     ... doneContainers starting in backgroundFor log info: run-services.sh infoThe system is up and running (screenshot from awesome https://github.com/jesseduffield/lazydocker)Now we can use code from assets folder to test things out:# Create a project in \"Clean Architecture\" service created from na-ca templatePOST http://localhost:3000/ca/projectsContent-Type: application/json{    \"name\": \"Clean Architecture Tasks\",    \"colourCode\": \"#FFFFFF\"}# Get projectsGET http://localhost:3000/ca/projects/# Create a project in \"Event Sourcing\" service created from na-es templatePOST http://localhost:3000/es/projectsContent-Type: application/json{    \"name\": \"Event Sourcing Tasks\",    \"colourCode\": \"#FF5733\"}# Get projectsGET http://localhost:3000/es/projects/SummaryI encourage you to continue the investigation of the codebase on your own. You may find some inspiration or good practices, there are hidden gems üôÇ. Let me know if you want to see a review of one of the components in the comments.Reference  https://www.nuget.org/packages/NikiforovAll.CleanArchitecture.Templates/  https://martinfowler.com/articles/practical-test-pyramid.html"
    },
  
    {
      "id": "65",
      "title": "What's new in .NET 6 and C# 10. Everything you wanted to know.",
      "url": "/dotnet/csharp/coding-stories/2021/10/29/whats-new-in-dotnet6.html",
      "date": "October 29, 2021",
      "categories": ["dotnet","csharp","coding-stories"],
      "tags": ["dotnet","coding-stories"],
      "shortinfo": "See how you can set up your reproducible development environment based on devcontainers without extra hustle.",
      "content": "TL;DRThis blog post is a compilation of the latest and greatest additions from the .NET 6 release. Also, I‚Äôve created a coding story that will help you to learn new improvements.Please check the coding story üëá:  https://codingstories.io/stories/6139f4a2f2cd3d0031cd8ef1/617b06b5d2ec28001acfe618Source code üëá:  https://github.com/NikiforovAll/whats-new-in-dotnet6Also, see my blog post. It explains how to run .NET 6 inside devcontainer: https://nikiforovall.github.io/productivity/devcontainers/2021/10/14/devcontainer-for-dotnet6.html  Part 1. C# 10          Global usings                  Reference                    File-scoped namespaces                  Reference                    Record structs                  Reference                      Part 2. .NET API          Minimal API                  Reference                    LINQ Improvements                  Reference                    Parallel.ForEachAsync                  Reference                    PeriodicTimer      Priority Queue                  Reference                    DateOnly and TimeOnly      System.Text.Json        Summary‚ö†üëÄ Please note that the content below consists of excerpts from the actual coding story. Please check the coding story if you haven‚Äôt already.Part 1. C# 10Full list of changes:  https://docs.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-10  https://devblogs.microsoft.com/dotnet/announcing-net-6-release-candidate-2/Also, see:  Every feature added in C# 10 with examples by NickChapsas https://www.youtube.com/watch?v=Vft4QDUpyWY&amp;ab_channel=NickChapsas  https://devblogs.microsoft.com/dotnet/new-dotnet-6-apis-driven-by-the-developer-community/  https://www.infoworld.com/article/3608611/whats-new-in-microsoft-net-6.htmlGlobal usingsglobal using global::System;global using global::System.Collections.Generic;global using global::System.IO;global using global::System.Linq;global using global::System.Net.Http;global using global::System.Threading;global using global::System.Threading.Tasks;global using static System.Console;global using static System.Math;WriteLine(Sqrt(3 * 3 + 4 * 4));$ dotnet run 5Reference  https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/using-directive#global-modifier  https://docs.microsoft.com/en-us/dotnet/core/compatibility/sdk/6.0/implicit-namespaces#new-behavior  https://www.hanselman.com/blog/implicit-usings-in-net-6  https://dotnetcoretutorials.com/2021/08/31/implicit-using-statements-in-net-6/File-scoped namespaces// IProductRepository.csnamespace MyNamespace;public interface IProductRepository{    Task&lt;Product&gt; GetProductAsync(int id);}public record class Product(int Id, string Name, ProductWarranty Warranty);// Program.csusing MyNamespace;var repository = new ProductRepository();var product = await repository.GetProductAsync(Parse(args[0]));WriteLine(product);$ dotnet run 12Product { Id = 1, Name = Name, Warranty = OneYear }Reference  https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/namespace  https://ardalis.com/dotnet-format-and-file-scoped-namespaces/  https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/proposals/csharp-10.0/file-scoped-namespacesRecord structspublic record class Point(double X, double Y, double Z);// public record class Point// {//     public double X {  get; init; }//     public double Y {  get; init; }//     public double Z {  get; init; }// }public readonly record struct Point2(double X, double Y, double Z);// public record struct Point2// {//     public double X {  get; init; }//     public double Y {  get; init; }//     public double Z {  get; init; }// }public record struct Point3(double X, double Y, double Z);// public record struct Point3// {//     public double X { get; set; }//     public double Y { get; set; }//     public double Z { get; set; }// }Reference  https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/builtin-types/recordPart 2. .NET API‚ûï System.Numerics.BitOperationsMinimal API// ================================================================// Main Components// ================================================================// WebApplicationpublic static Microsoft.AspNetCore.Builder.WebApplicationBuilder CreateBuilder ();// WebApplicationBuilderpublic Microsoft.AspNetCore.Builder.WebApplication Build ();// WebApplicationpublic void Run (string? url = default);var builder = WebApplication.CreateBuilder(args);var services = builder.Services;services    .AddEndpointsApiExplorer()    .AddSwaggerGen();var app = builder.Build();app    .UseSwagger()    .UseSwaggerUI();app.MapGet(\"ka/{pow}\", (int pow) =&gt; IsPow2(pow));app.Run();$ dotnet runBuilding...info: Microsoft.Hosting.Lifetime[14]      Now listening on: https://localhost:7225info: Microsoft.Hosting.Lifetime[14]      Now listening on: http://localhost:5069info: Microsoft.Hosting.Lifetime[0]      Application started. Press Ctrl+C to shut down.info: Microsoft.Hosting.Lifetime[0]      Hosting environment: Developmentinfo: Microsoft.Hosting.Lifetime[0]      Content root path: D:\\home\\dev\\codingstories\\whats-new-in-dotnet6\\MinimalAPIReference  Minimal APIs at a glance by davidfowl https://gist.github.com/davidfowl/ff1addd02d239d2d26f4648a06158727  Migration to ASP.NET Core in .NET 6 by davidfowl https://gist.github.com/davidfowl/0e0372c3c1d895c3ce195ba983b1e03d  https://minimal-apis.github.io/  https://andrewlock.net/series/exploring-dotnet-6/  https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.builder.webapplicationbuilder?view=aspnetcore-6.0  https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.builder.webapplication?view=aspnetcore-6.0  https://github.com/martincostello/dotnet-minimal-api-integration-testing  https://nikiforovall.github.io/dotnet/aspnetcore/2021/09/10/opinionated-minimal-api.htmlLINQ Improvementsusing Bogus;var faker = new OrderFaker();var orders = faker.Generate(100);// ================================================================Header(\"Chunking\");// ================================================================foreach (var chunk in orders.Chunk(25)){    var sum = chunk.Sum(o =&gt; o.Quantity);    WriteLine($\"chunk[{chunk.Length}].Quantity={sum}\");}Chunkingchunk[25].Quantity=154chunk[25].Quantity=138chunk[25].Quantity=119chunk[25].Quantity=138// ================================================================Header(\"Index Support for ElementAt\");// ================================================================var order1 = orders.ElementAt(^5);WriteLine(order1);Index Support for ElementAtOrder { OrderId = 15, Address = 604 Leffler Glens, Romaport, Cape Verde, Quantity = 2, Index = 96 }// ================================================================Header(\"Range Support for Take\");// ================================================================var orders2 = orders.Take(^5..);WriteLine(string.Join(Environment.NewLine, orders2));Range Support for TakeOrder { OrderId = 15, Address = 604 Leffler Glens, Romaport, Cape Verde, Quantity = 2, Index = 96 }Order { OrderId = 8, Address = 35694 Cummings Ville, Turnerville, Republic of Korea, Quantity = 5, Index = 97 }Order { OrderId = 13, Address = 8711 Vita Burgs, East Jamaalberg, Zambia, Quantity = 4, Index = 98 }Order { OrderId = 83, Address = 11506 Skiles Curve, Lake Abbeychester, Suriname, Quantity = 8, Index = 99 }Order { OrderId = 89, Address = 9444 Schamberger Burgs, Wisokyland, Costa Rica, Quantity = 2, Index = 100 }// ================================================================Header(\"Three way zipping\");// ================================================================string[] a1 = { \"1\", \"2\", \"3\" };string[] a2 = { \"One\", \"Two\", \"Three\" };float[] a3 = { 1f, 2f, 3f };foreach ((string i, string i2, float i3) in a1.Zip(a2, a3)){    WriteLine($\"{i}-{i2}-{i3}\");}Three-way zipping1-One-12-Two-23-Three-3// ================================================================Header(\"Default Parameters for Common Methods\");// ================================================================var order2 = orders.FirstOrDefault(    o =&gt; o.Address.Contains(\"Odesa\"), defaultValue: orders.ElementAt(^10));WriteLine(order2);Default Parameters for Common MethodsOrder { OrderId = 14, Address = 49026 Huels Groves, Armandville, Bhutan, Quantity = 3, Index = 91 }// ================================================================Header(\"Avoiding Enumeration with TryGetNonEnumeratedCount\");// ================================================================if (orders.TryGetNonEnumeratedCount(out int count))    WriteLine($\"The count is {count}\");Avoiding Enumeration with TryGetNonEnumeratedCountThe count is 100// ================================================================Header(\"MaxBy and MinBy\");// ================================================================WriteLine(orders.MaxBy(o =&gt; o.Quantity));MaxBy and MinByOrder { OrderId = 16, Address = 30901 Madilyn Meadow, Lake Marlenfort, Malawi, Quantity = 10, Index = 2 }Reference  7 awesome improvements for LINQ in .NET 6 by NickChapsas https://www.youtube.com/watch?v=sIXKpyhxHR8&amp;t=1s&amp;ab_channel=NickChapsasParallel.ForEachAsync‚ûï Task.WaitAsync‚ûï Random.Sharedpublic static Task ForEachAsync&lt;TSource&gt;(    IAsyncEnumerable&lt;TSource&gt; source,    Func&lt;TSource, CancellationToken, ValueTask&gt; body);public static Task ForEachAsync&lt;TSource&gt;(    IEnumerable&lt;TSource&gt; source,    Func&lt;TSource, CancellationToken, ValueTask&gt; body);await Parallel.ForEachAsync(Generate(), Handle)    .WaitAsync(TimeSpan.FromMilliseconds(200))    .ContinueWith(Report);static async IAsyncEnumerable&lt;int&gt; Generate(){    while (true)    {        var delay = Random.Shared.Next(100);        WriteLine($\"Issued {delay}\");        await Task.Delay(delay);        yield return delay;    }}static async ValueTask Handle(int i, CancellationToken ct){    await Task.Delay(i, ct);    WriteLine($\"Handled {i}\");}static void Report(Task t) =&gt; WriteLine(    $\"Finished at: {DateTime.Now:T}, task.Status {t.Status}\");$ dotnet runIssued 56Issued 23Issued 81Handled 23Handled 56Issued 75Finished at: 10:59:08 PM, task.Status FaultedReference  https://www.hanselman.com/blog/parallelforeachasync-in-net-6PeriodicTimer‚ûï ArgumentNullException.ThrowIfNullusing var timer = new PeriodicTimer(TimeSpan.FromSeconds(1));while (await timer.WaitForNextTickAsync()){    LogEntry? l = Random.Shared.Next(128) is int r &amp;&amp; IsPow2(r)        ? default        : new(r);    ProcessLogEntry(l);}static void ProcessLogEntry(LogEntry? entry){    ArgumentNullException.ThrowIfNull(entry, nameof(entry));    WriteLine(entry);}public record class LogEntry(int Value);$ dotnet runLogEntry { Value = 104 }LogEntry { Value = 115 }LogEntry { Value = 21 }LogEntry { Value = 110 }LogEntry { Value = 106 }LogEntry { Value = 63 }LogEntry { Value = 81 }Unhandled exception. System.ArgumentNullException: Value cannot be null. (Parameter 'entry')   at System.ArgumentNullException.Throw(String paramName)   at System.ArgumentNullException.ThrowIfNull(Object argument, String paramName)   at Program.&lt;&lt;Main&gt;$&gt;g__ProcessLogEntry|0_0(LogEntry entry) in PeriodicTimer\\Program.cs:line 18   at Program.&lt;Main&gt;$(String[] args) in PeriodicTimer\\Program.cs:line 12   at Program.&lt;Main&gt;(String[] args)Priority Queuevar queue = new PriorityQueue&lt;Job, int&gt;(ReverseComparer&lt;int&gt;.Default);foreach (var i in 10){    var p = Random.Shared.Next(100);    queue.Enqueue(new($\"Job{i}\", p), p);}using var timer = new PeriodicTimer(TimeSpan.FromMilliseconds(500));while (await timer.WaitForNextTickAsync()){    if (!queue.TryDequeue(out var job, out _))        break;    WriteLine(job);}public record struct Job(string Name, int Priority);public sealed class ReverseComparer&lt;T&gt; : IComparer&lt;T&gt;{    public static readonly ReverseComparer&lt;T&gt; Default = new(Comparer&lt;T&gt;.Default);    public static ReverseComparer&lt;T&gt; Reverse(IComparer&lt;T&gt; comparer) =&gt;        new ReverseComparer&lt;T&gt;(comparer);    private readonly IComparer&lt;T&gt; comparer = Default;    private ReverseComparer(IComparer&lt;T&gt; comparer) =&gt;        this.comparer = comparer;    public int Compare(T? x, T? y) =&gt; comparer.Compare(y, x);}public static class ProduceNumericEnumeratorExtensions{    public static IEnumerator&lt;int&gt; GetEnumerator(this int number)    {        for (var i = 0; i &lt; number; i++)        {            yield return i;        }    }}$ dotnet run Job { Name = Job1, Priority = 75 }Job { Name = Job5, Priority = 66 }Job { Name = Job2, Priority = 61 }Job { Name = Job0, Priority = 58 }Job { Name = Job4, Priority = 53 }Job { Name = Job9, Priority = 34 }Job { Name = Job7, Priority = 34 }Job { Name = Job3, Priority = 29 }Job { Name = Job8, Priority = 17 }Job { Name = Job6, Priority = 8 }Reference  https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.priorityqueue-2?view=net-6.0  https://dotnetcoretutorials.com/2021/03/17/priorityqueue-in-net/DateOnly and TimeOnlyDateOnly date = DateOnly.MinValue;Log(date, nameof(date)); //Outputs 01/01/0001 (With no Time)TimeOnly time = TimeOnly.MinValue;Log(time, nameof(time)); //Outputs 12:00 AMTimeOnly startTime = TimeOnly.Parse(\"11:00 PM\");var hoursWorked = 2;var endTime = startTime.AddHours(hoursWorked);Log(endTime, nameof(endTime)); //Outputs 1:00 AMvar isBetween = TimeOnly.Parse(\"12:00 AM\").IsBetween(startTime, endTime); Log(isBetween, nameof(isBetween)); //Outputs true.static void Log&lt;T&gt;(T value, string name = \"\") =&gt; WriteLine($\"{name} {value}\");$ dotnet run date 1/1/0001time 12:00 AMendTime 1:00 AMisBetween TrueSystem.Text.Json// ================================================================Header(\"Serialization Notification\");// ================================================================string invalidOrderJson = \"{}\";var success = IgnoreErrors(() =&gt; JsonSerializer.Deserialize&lt;Order2&gt;(invalidOrderJson));WriteLine($\"Exception thrown: {!success}\");// IJsonOnDeserialized, IJsonOnDeserializing, IJsonOnSerialized, IJsonOnSerializingpublic record class Order2 : Order, IJsonOnDeserialized{    public void OnDeserialized() =&gt; this.Validate();    private void Validate()    {        if (this.OrderId &lt;= 0)            throw new ArgumentException();    }}public record class Order{    public int OrderId { get; init; }    public string Address { get; init; }    public int Quantity { get; init; }}static bool IgnoreErrors(Action operation){    if (operation == null)        return false;    try    {        operation.Invoke();    }    catch    {        return false;    }    return true;}$ dotnet run Serialization NotificationException thrown: True// ================================================================Header(\"Property Ordering\");// ================================================================Order3 order = new(){    OrderId = 1,    Address = \"Address\",    Quantity = 1,    Comments = new() { \"Cool\", \"Awesome\" }};var serializedOrder = JsonSerializer.Serialize(order, options);WriteLine(serializedOrder);public record class Order3{    [JsonPropertyOrder(-1)]    public int OrderId { get; init; }    [JsonPropertyOrder(1)]    public string Address { get; init; }    [JsonPropertyOrder(2)]    public int Quantity { get; init; }    [JsonPropertyOrder(99)]    public List&lt;string&gt; Comments { get; init; }}$ dotnet run Property Ordering{  \"OrderId\": 1,  \"Address\": \"Address\",  \"Quantity\": 1,  \"Comments\": [    \"Cool\",    \"Awesome\"  ]}// ================================================================Header(\"IAsyncEnumerable support &amp;&amp; Working with Streams\");// ================================================================var data = new { Data = RangeAsync(1, 5) };// SerializeAsyncusing var stream = new MemoryStream();await JsonSerializer.SerializeAsync(stream, RangeAsync(1, 5), options);stream.Position = 0;// DeserializeAsyncEnumerableawait foreach (var i in JsonSerializer.DeserializeAsyncEnumerable&lt;int&gt;(stream, options)){    WriteLine(i);}static async IAsyncEnumerable&lt;int&gt; RangeAsync(int start, int count){    for (int i = 0; i &lt; count; i++)    {        await Task.Delay(i);        yield return start + i;    }}$ dotnet run 12345// ================================================================Header(\"Working with JSON DOM\");// ================================================================// Parsevar node = JsonNode.Parse(serializedOrder);WriteLine($\"OrderId: {node[\"OrderId\"].GetValue&lt;int&gt;()}\");WriteLine($\"Order.Comments[0]: {node[\"Comments\"][0].GetValue&lt;string&gt;()}\");// Create DOM Object via APIvar jObjectOrder = new JsonObject{    [\"OrderId\"] = 1,    [\"Discounts\"] = new JsonArray(        new JsonObject        {            [\"DiscountId\"] = 1,            [\"Value\"] = .05,        },        new JsonObject        {            [\"DiscountId\"] = 2,            [\"Value\"] = .1,        }    ),};WriteLine(jObjectOrder.ToJsonString(options));OrderId: 1Order.Comments[0]: Cool{  \"OrderId\": 1,  \"Discounts\": [    {      \"DiscountId\": 1,      \"Value\": 0.05    },    {      \"DiscountId\": 2,      \"Value\": 0.1    }  ]}SummaryThank you very much. I hope you find this blog post useful üëç."
    },
  
    {
      "id": "66",
      "title": "Try out .NET 6 inside your own development environment built with devcontainers, docker, and vscode",
      "url": "/productivity/devcontainers/2021/10/14/devcontainer-for-dotnet6.html",
      "date": "October 14, 2021",
      "categories": ["productivity","devcontainers"],
      "tags": ["dotnet","docker","dotfiles","vscode","deaac","devcontainers"],
      "shortinfo": "See how you can set up your reproducible development environment based on devcontainers without extra hustle.",
      "content": "TL;DRIn this blog post, I provide an example of how to organize/dockerize your personal development setup. Also, you may want to use one of my devcontainers to try .NET 6. Everything is baked inside the devcontainer, so you don‚Äôt have to install SDK and other tooling on your host machine.IntroductionAs you may know, .NET 6 Release Candidate 2 has been announced and it is very close to the final build of .NET 6 that will be shipped in November this year in time for .NET Conf 2021. It is the perfect time to start playing with .NET 6 and C# 10 and all of its goodness.This is why I‚Äôve created sandbox https://github.com/NikiforovAll/devcontainer-for-dotnet6-demo. This is dotnet new webapi -minimal packed inside devcontainer. So, all you need to do is to get the source code and compile the devcontainer. You can do it by downloading via VSCode URL launcher or clone via git. I suggest you go straight to documentation for more details. My goal is to share my experience using devcontainers.Here is how I do it:git clone https://github.com/NikiforovAll/devcontainer-for-dotnet6-demodevcontainer open ./devcontainer-for-dotnet6-demoüí° Tip: You can open repository in VSCode code ./devcontainer-for-dotnet6-demo and after that, you will be prompted with a suggestion to reopen workspace in devcontainer (https://code.visualstudio.com/docs/remote/devcontainer-cli#_opening-a-folder-directly-within-a-dev-container).üöÄ Run dotnet run --project ./src/App/ and enjoy your coding.Anatomy of devcontainersWhen you generate configuration from VSCode, by default, it generates something like this:$ tree -a .‚îî‚îÄ‚îÄ .devcontainer    ‚îú‚îÄ‚îÄ devcontainer.json    ‚îú‚îÄ‚îÄ Dockerfile    ‚îî‚îÄ‚îÄ library-scripts        ‚îî‚îÄ‚îÄ azcli-debian.sh# DockerfileARG VARIANT=\"5.0\"FROM mcr.microsoft.com/vscode/devcontainers/dotnet:0-${VARIANT}// .devcontainer/devcontainer.json// For format details, see https://aka.ms/devcontainer.json. For config options, see the README at:// https://github.com/microsoft/vscode-dev-containers/tree/v0.202.3/containers/dotnet{    \"name\": \"\",    \"build\": {        \"dockerfile\": \"Dockerfile\",        \"args\": {             // Update 'VARIANT' to pick a .NET Core version: 2.1, 3.1, 5.0            \"VARIANT\": \"5.0\"        }    },    // Set *default* container specific settings.json values on container create.    \"settings\": {},    // Add the IDs of extensions you want installed when the container is created.    \"extensions\": [    ],    \"remoteUser\": \"vscode\"}A devcontainer.json file in your project tells VS Code how to access (or create) a development container with a well-defined tool and runtime stack. This container can be used to run an application or to separate tools, libraries, or runtimes needed for working with a codebase.Everything you need is already added in the base docker image, but it is quite easy to extend it. You may want to pre-install some dependencies and tools in a custom Dockerfile.üí°Tip: You are not limited to using devcontainers for .NET 6 project, actually, you can use them pretty much for everything. For example, I‚Äôm writing this blog post from devcontainer üôÇ.Distribute devcontainersPersonally, I think it should be possible to easily explore the content of the devcontainer in order to change it on demand. This is why my go-to option is to create custom-tailored containers for each project based on some lightweight base images with shared tooling installed in them. For example, you might want to install something like: https://github.com/rothgar/awesome-tuis, https://github.com/unixorn/git-extra-commands, https://github.com/junegunn/fzf, https://github.com/sharkdp/fd, https://github.com/ogham/exa, etc. ü§ìHere you can find my devcontainer for .NET:  Source: https://github.com/NikiforovAll/dev-containers/tree/master/containers/dotnet  GitHub Container Registry https://github.com/NikiforovAll/dev-containers/pkgs/container/devcontainers%2Fdotnet  Docker Hub https://hub.docker.com/repository/docker/nikiforovall/devcontainers-dotnetAfter that, you can install it directly from docker container registry by specifying ‚Äúimage‚Äù field inside devcontainer.json.Minimum devcontainer looks like this:{    \"name\": \".NET 6 devcontainer\",    \"image\": \"nikiforovall/devcontainers-dotnet:latest\",    \"settings\": {},    \"extensions\": []}DotfilesYou can also include your dotfiles repository to replicate your terminal experience (configurations, aliases, customizations, tools, etc.). See user settings for the ‚ÄúRemote - Containers‚Äù extension.Basically, you want to configure the remote git repository as the source of dotfiles and tell vscode what to do upon installation. Here is my dotfile for devcontainers: https://github.com/NikiforovAll/dotfiles/blob/master/src/dev-container/boot/install.sh.\"settings\": {    \"dotfiles.installCommand\": \"\",    \"remote.containers.dotfiles.repository\": \"https://github.com/NikiforovAll/dotfiles.git\",    \"remote.containers.dotfiles.installCommand\": \"~/dotfiles/src/dev-container/boot/install.sh\",    \"remote.containers.dotfiles.targetPath\": \"~/dotfiles\",},Anatomy - Summary            Name      Description      Responsibility                  .devconatiner/devcontainer.json      Workspace definition.       Specify how to assemble devcontainer. Customize IDE (vscode) behavior.              .devconatiner/Dockerfile      Image for docker container.      Install dependencies and tools. Configure defaults.              dotfiles      External versioned source of configuration for developer environment.      Customize the way your terminal looks and feels and developer experience in general.      SummaryI‚Äôve explained to you the main building blocks of devcontainers and shared some ideas regarding how you may want to organize your development setup. Hope you find it useful.Reference  https://github.com/NikiforovAll/devcontainer-for-dotnet6-demo  https://nikiforovall.github.io/docker/2020/09/19/publish-package-to-ghcr.html  https://nikiforovall.github.io/productivity/2019/11/30/nikiforovall-setup.html  https://devblogs.microsoft.com/aspnet/asp-net-core-updates-in-net-6-rc-2/  https://code.visualstudio.com/docs/remote/create-dev-container  https://code.visualstudio.com/docs/remote/devcontainer-cli"
    },
  
    {
      "id": "67",
      "title": "An opinionated look at Minimal API in .NET 6",
      "url": "/dotnet/aspnetcore/2021/09/10/opinionated-minimal-api.html",
      "date": "September 10, 2021",
      "categories": ["dotnet","aspnetcore"],
      "tags": ["dotnet","aspnetcore","minimal-api","coding-stories"],
      "shortinfo": "In this blog post I share my thoughts on how to organize Minimal API project to keep code structure under control and still get benefits from low-ceremony approach.",
      "content": "TL;DRIn this blog post, I share my thoughts on how to organize Minimal API projects to keep code structure under control and still get benefits from the low-ceremony approach.IntroductionMinimal API is a refreshing and promising application model for building lightweight Web APIs. Now you can create a microservice and start prototyping without the necessity to create lots of boilerplate code and worrying about too much about code structure.var app = WebApplication.Create();app.MapGet(\"/\", () =&gt; \"Hello World!\");app.Run();Presumably, this kind of style gives you a productivity boost and flattens the learning curve for newcomers. So it is considered as a more lightweight version, but using Minimal API doesn‚Äôt mean you have to write small applications. It is rather a different application model that one day will be as much powerful as MVC counterpart.Problem StatementOne of the problems with Minimal API is that Program.cs can get to big. So initial simplicity may lead you to the big ball of mud type of solution. At this point, you want to use refactoring techniques and my goal is to share some ideas on how to tackle emerging challenges.Example: Building Minimal APII‚Äôve prepared a demo application. I strongly recommend checking it before you move further.Source code can be found at GitHub: NikiforovAll/minimal-api-exampleRecommendationsMy general recommendation is to write something that may be called Modular Minimal API or Vertical Slice Minimal API.Keep Program.cs aka Composition Root smallA Composition Root is a unique location in an application where modules are composed together. You should have a good understanding of what this application is about just by looking at it.You want to keep Program.cs clean and focus on high-level modules.var builder = WebApplication.CreateBuilder(args);builder.AddSerilog();builder.AddSwagger();builder.AddAuthentication();builder.AddAuthorization();builder.Services.AddCors();builder.AddStorage();builder.Services.AddCarter();var app = builder.Build();var environment = app.Environment;app    .UseExceptionHandling(environment)    .UseSwaggerEndpoints(routePrefix: string.Empty)    .UseAppCors()    .UseAuthentication()    .UseAuthorization();app.MapCarter();app.Run();üí°Tip: One of the techniques you can apply here is to create extension methods for IServiceCollection, IApplicationBuilder. For Minimal API I would suggest using ‚Äúfile-per-concern‚Äù organization. See ApplicationBuilderExtensions and ServiceCollectionExtensions folders.$ tree.‚îú‚îÄ‚îÄ ApplicationBuilderExtensions‚îÇ   ‚îú‚îÄ‚îÄ ApplicationBuilderExtensions.cs‚îÇ   ‚îî‚îÄ‚îÄ ApplicationBuilderExtensions.OpenAPI.cs‚îú‚îÄ‚îÄ assets‚îÇ   ‚îî‚îÄ‚îÄ run.http‚îú‚îÄ‚îÄ Features‚îÇ   ‚îú‚îÄ‚îÄ HomeModule.cs‚îÇ   ‚îî‚îÄ‚îÄ TodosModule.cs‚îú‚îÄ‚îÄ GlobalUsing.cs‚îú‚îÄ‚îÄ MinimalAPI.csproj‚îú‚îÄ‚îÄ Program.cs‚îú‚îÄ‚îÄ Properties‚îÇ   ‚îî‚îÄ‚îÄ launchSettings.json‚îú‚îÄ‚îÄ ServiceCollectionExtensions‚îÇ   ‚îú‚îÄ‚îÄ ServiceCollectionExtensions.Auth.cs‚îÇ   ‚îú‚îÄ‚îÄ ServiceCollectionExtensions.Logging.cs‚îÇ   ‚îú‚îÄ‚îÄ ServiceCollectionExtensions.OpenAPI.cs‚îÇ   ‚îî‚îÄ‚îÄ ServiceCollectionExtensions.Persistence.cs‚îî‚îÄ‚îÄ todos.dbAnd here is an example of how to add OpenAPI/Swagger concern:namespace Microsoft.Extensions.DependencyInjection;using Microsoft.OpenApi.Models;public static partial class ServiceCollectionExtensions{    public static WebApplicationBuilder AddSwagger(this WebApplicationBuilder builder)    {        builder.Services.AddSwagger();        return builder;    }    public static IServiceCollection AddSwagger(this IServiceCollection services)    {        services.AddEndpointsApiExplorer();        services.AddSwaggerGen(c =&gt;        {            c.SwaggerDoc(\"v1\", new OpenApiInfo()            {                Description = \"Minimal API Demo\",                Title = \"Minimal API Demo\",                Version = \"v1\",                Contact = new OpenApiContact()                {                    Name = \"Oleksii Nikiforov\",                    Url = new Uri(\"https://github.com/nikiforovall\")                }            });        });        return services;    }}Organize endpoints around featuresA MinimalApiPlayground from Damian Edwards is a really good place to start learning more about Minimal API, but things start to get hairy (https://github.com/DamianEdwards/MinimalApiPlayground/blob/main/src/Todo.Dapper/Program.cs). Functionality by functionality you turn into a scrolling machine more and more - no good üòõ. It means we need to organize code into manageable components/modules.Modular approach allows us to focus on cohesive units of functionality. Luckily, there is an awesome open source project - Carter. It supports some essential missing features (Minimal API .NET 6) and one of them is module registration ICarterModule.namespace MinimalAPI;using Dapper;using Microsoft.Data.Sqlite;public class TodosModule : ICarterModule{    public void AddRoutes(IEndpointRouteBuilder app)    {        app.MapGet(\"/api/todos\", GetTodos);        app.MapGet(\"/api/todos/{id}\", GetTodo);        app.MapPost(\"/api/todos\", CreateTodo);        app.MapPut(\"/api/todos/{id}/mark-complete\", MarkComplete);        app.MapDelete(\"/api/todos/{id}\", DeleteTodo);    }    private static async Task&lt;IResult&gt; GetTodo(int id, SqliteConnection db) =&gt;        await db.QuerySingleOrDefaultAsync&lt;Todo&gt;(            \"SELECT * FROM Todos WHERE Id = @id\", new { id })            is Todo todo                ? Results.Ok(todo)                : Results.NotFound();    private async Task&lt;IEnumerable&lt;Todo&gt;&gt; GetTodos(SqliteConnection db) =&gt;        await db.QueryAsync&lt;Todo&gt;(\"SELECT * FROM Todos\");    private static async Task&lt;IResult&gt; CreateTodo(Todo todo, SqliteConnection db)    {        var newTodo = await db.QuerySingleAsync&lt;Todo&gt;(            \"INSERT INTO Todos(Title, IsComplete) Values(@Title, @IsComplete) RETURNING * \", todo);        return Results.Created($\"/todos/{newTodo.Id}\", newTodo);    }    private static async Task&lt;IResult&gt; DeleteTodo(int id, SqliteConnection db) =&gt;        await db.ExecuteAsync(            \"DELETE FROM Todos WHERE Id = @id\", new { id }) == 1            ? Results.NoContent()            : Results.NotFound();    private static async Task&lt;IResult&gt; MarkComplete(int id, SqliteConnection db) =&gt;        await db.ExecuteAsync(            \"UPDATE Todos SET IsComplete = true WHERE Id = @Id\", new { Id = id }) == 1            ? Results.NoContent()            : Results.NotFound();}public class Todo{    public int Id { get; set; }    public string? Title { get; set; }    public bool IsComplete { get; set; }}üí°Tip: You can use Method Group (C#) instead of lambda expression to avoid formatting issues and keep code clean. Also, it provides automatic endpoint metadata aspnetcore#34540, that‚Äôs cool.// FROMapp.MapGet(\"/todos\", async (SqliteConnection db) =&gt;    await db.QueryAsync&lt;Todo&gt;(\"SELECT * FROM Todos\"));// TOapp.MapGet(\"/api/todos\", GetTodos);async Task&lt;IEnumerable&lt;Todo&gt;&gt; GetTodos(SqliteConnection db) =&gt;        await db.QueryAsync&lt;Todo&gt;(\"SELECT * FROM Todos\");To register modules you simply need to add two lines of code in Program.cs. Modules are registered based on assemblies scanning and added to DI automatically, see. You can go even further and split Carter modules into separate assemblies.builder.Services.AddCarter();// ...app.MapCarter();I recommend you to enhance your Minimal APIs with Carter because it tries to close the gap between Minimal API and full-fledged ASP.NET MVC version. Go check out Carter on GitHub, give them a Star, try it out!High cohesionModules go well together with Vertical Slice Architecture. Simply start with ./Features folder and keep related models, services, factories, etc. together.ConclusionMinimal API doesn‚Äôt mean your application has to be small. In this blog post, I‚Äôve shared some ideas on how to handle project complexity. Personally, I like this style and believe that one day Minimal API will be as much powerful as ASP.NET MVC.Reference  https://www.hanselman.com/blog/carter-community-for-aspnet-core-means-enjoyable-web-apis-on-the-cutting-edge  https://github.com/CarterCommunity/Carter  https://github.com/AnthonyGiretti/aspnetcore-minimal-api  https://www.youtube.com/watch?v=4ORO-KOufeU&amp;ab_channel=NickChapsas  https://www.youtube.com/watch?v=bSJ5n7alhTs&amp;ab_channel=dotNET  https://benfoster.io/blog/mvc-to-minimal-apis-aspnet-6/  https://blog.ploeh.dk/2011/07/28/CompositionRoot/"
    },
  
    {
      "id": "68",
      "title": "How to add Health Checks to ASP.NET Core project. A coding story.",
      "url": "/dotnet/aspnetcore/coding-stories/2021/07/25/add-health-checks-to-aspnetcore.html",
      "date": "July 25, 2021",
      "categories": ["dotnet","aspnetcore","coding-stories"],
      "tags": ["dotnet","aspnetcore","coding-stories"],
      "shortinfo": "A coding story (https://codingstories.io/) that explains how to add health checks to ASP.NET Core project.",
      "content": "TL;DRHealth checks are valuable and it is pretty straightforward to use them in ASP.NET Core. I‚Äôve created a coding story to show you how to add and use them in different scenarios. (database, rabbitmq, downstream services).Observability of the system is crucial for successful maintenance and monitoring. Health checks help you with that!Below, you can find a coding story that describes the process of adding health checks support to ASP.NET Core project.  https://codingstories.io/stories/6139f4a2f2cd3d0031cd8ef1/6103bf8972273c2133d37ce6https://gitlab.com/codingstories/how-to-add-health-checks-to-aspnetcoreSneak peek the resultBy the end of the coding story, you will see something like the following:public class Startup{   public IConfiguration Configuration { get; set; }   public Startup(IConfiguration configuration)   {      this.Configuration = configuration;   }   public void ConfigureServices(IServiceCollection services)   {      var connectionString = this.Configuration.GetConnectionString(\"DefaultConnection\");      var rabbitMqConnectionString = this.Configuration.GetConnectionString(\"RabbitMQ\");      var downstreamServiceUrl = this.Configuration[\"DownstreamService:BaseUrl\"];      services.AddHealthChecks()            .AddSqlServer(               connectionString,               name: \"Database\",               failureStatus: HealthStatus.Degraded,               timeout: TimeSpan.FromSeconds(1),               tags: new string[] { \"services\" })            .AddRabbitMQ(               rabbitMqConnectionString,               name: \"RabbitMQ\",               failureStatus: HealthStatus.Degraded,               timeout: TimeSpan.FromSeconds(1),               tags: new string[] { \"services\" })            .AddUrlGroup(               new Uri($\"{downstreamServiceUrl}/health\"),               name: \"Downstream API Health Check\",               failureStatus: HealthStatus.Unhealthy,               timeout: TimeSpan.FromSeconds(3),               tags: new string[] { \"services\" });   }   public void Configure(IApplicationBuilder app)   {      app.UseRouting();      app.UseEndpoints(endpoints =&gt;      {            endpoints.MapCustomHealthCheck();            endpoints.MapGet(\"/{**path}\", async context =&gt;            {               await context.Response.WriteAsync(                  \"Navigate to /health to see the health status.\");            });      });   }}public static class EndpointRouteBuilderExtensions{   /// &lt;summary&gt;   /// Adds a Health Check endpoint to the &lt;see cref=\"IEndpointRouteBuilder\"/&gt; with the specified template.   /// &lt;/summary&gt;   /// &lt;param name=\"endpoints\"&gt;The &lt;see cref=\"IEndpointRouteBuilder\"/&gt; to add endpoint to.&lt;/param&gt;   /// &lt;param name=\"pattern\"&gt;The URL pattern of the liveness endpoint.&lt;/param&gt;   /// &lt;param name=\"servicesPattern\"&gt;The URL pattern of the readiness endpoint.&lt;/param&gt;   /// &lt;returns&gt;A route for the endpoint.&lt;/returns&gt;   public static IEndpointRouteBuilder MapCustomHealthCheck(      this IEndpointRouteBuilder endpoints,      string pattern = \"/health\",      string servicesPattern = \"/health/ready\")   {      if (endpoints == null)      {            throw new ArgumentNullException(nameof(endpoints));      }      endpoints.MapHealthChecks(pattern, new HealthCheckOptions()      {            Predicate = (check) =&gt; !check.Tags.Contains(\"services\"),            AllowCachingResponses = false,            ResponseWriter = WriteResponse,      });      endpoints.MapHealthChecks(servicesPattern, new HealthCheckOptions()      {            Predicate = (check) =&gt; true,            AllowCachingResponses = false,            ResponseWriter = WriteResponse,      });      return endpoints;   }   private static Task WriteResponse(HttpContext context, HealthReport result)   {      context.Response.ContentType = \"application/json; charset=utf-8\";      var options = new JsonWriterOptions      {            Indented = true      };      using var stream = new MemoryStream();      using (var writer = new Utf8JsonWriter(stream, options))      {            writer.WriteStartObject();            writer.WriteString(\"status\", result.Status.ToString());            writer.WriteStartObject(\"results\");            foreach (var entry in result.Entries)            {               writer.WriteStartObject(entry.Key);               writer.WriteString(\"status\", entry.Value.Status.ToString());               writer.WriteEndObject();            }            writer.WriteEndObject();            writer.WriteEndObject();      }      var json = Encoding.UTF8.GetString(stream.ToArray());      return context.Response.WriteAsync(json);   }}Please let me know what you think about this coding story. Feedback is very much appreciated üëç.Reference  https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/health-checks  https://gitlab.com/NikiforovAll/how-to-add-health-checks-to-aspnetcore"
    },
  
    {
      "id": "69",
      "title": "How to add OpenAPI to ASP.NET Core project. A coding story.",
      "url": "/dotnet/aspnetcore/coding-stories/2021/06/13/add-openapi-to-aspnetcore.html",
      "date": "June 13, 2021",
      "categories": ["dotnet","aspnetcore","coding-stories"],
      "tags": ["dotnet","aspnetcore","openapi","coding-stories"],
      "shortinfo": "A coding story (https://codingstories.io/) that explains how to enhance your project with OpenAPI support.",
      "content": "TL;DROpenAPI is de facto standard that you should use to increase explorability of your ASP.NET Core projects.Previously, I blogged about the tool https://codingstories.io/ to share step-by-step guides and real programming experience in general. In this blog post, you will find a coding story that describes the process of adding OpenAPI support to a ASP.NET Core project.OpenAPI is great and you should definitely consider it. From this point, please navigate to the actual coding story how-to-add-openapi-to-aspnetcore, it is supposed to be self-contained and easy to understand. I‚Äôll see you there!  https://gitlab.com/codingstories/how-to-add-openapi-to-aspnetcoreSneak peek the resultBy the end of the coding story, you will see something like following:public class Startup{   public void ConfigureServices(IServiceCollection services)   {      services.AddControllers();      services.AddApiVersioning(options =&gt;      {            options.ReportApiVersions = true;            options.DefaultApiVersion = new ApiVersion(2, 0);            options.AssumeDefaultVersionWhenUnspecified = true;      });      services.AddSwaggerGen(options =&gt;      {            options.SwaggerDoc(\"v1\", new OpenApiInfo()            {               Title = \"Sample API\",               Version = \"v1\",               Description = \"A sample application with Swagger, Swashbuckle, and API versioning.\",               Contact = new OpenApiContact() { Name = \"Sample User\", Email = \"sample@user.com\" },               License = new OpenApiLicense() { Name = \"MIT\", Url = new Uri(\"https://opensource.org/licenses/MIT\") }            });            options.SwaggerDoc(\"v2\", new OpenApiInfo()            {               Title = \"Sample API\",               Version = \"v2\",               Description = \"A sample application with Swagger, Swashbuckle, and API versioning.\",               Contact = new OpenApiContact() { Name = \"Sample User\", Email = \"sample@user.com\" },               License = new OpenApiLicense() { Name = \"MIT\", Url = new Uri(\"https://opensource.org/licenses/MIT\") }            });            options.OperationFilter&lt;SwaggerDefaultValues&gt;();      });      services.AddVersionedApiExplorer(options =&gt;      {            // Add the versioned api explorer, which also adds IApiVersionDescriptionProvider service            // Note: the specified format code will format the version as \"'v'major[.minor][-status]\"            options.GroupNameFormat = \"'v'VVV\";      });   }   public void Configure(      IApplicationBuilder app,      IWebHostEnvironment env,      IApiVersionDescriptionProvider apiVersionDescriptionProvider)   {      if (env.IsDevelopment())      {            app.UseDeveloperExceptionPage();      }      app.UseRouting();      app.UseEndpoints(endpoints =&gt; endpoints.MapControllers());      app.UseSwagger();      app.UseSwaggerUI(c =&gt;      {            foreach (var description in apiVersionDescriptionProvider.ApiVersionDescriptions.Reverse())            {               c.SwaggerEndpoint($\"/swagger/{description.GroupName}/swagger.json\",                  description.GroupName.ToUpperInvariant());            }            c.RoutePrefix = string.Empty;      });   }   static string XmlCommentsFilePath   {      get      {            var basePath = PlatformServices.Default.Application.ApplicationBasePath;            var fileName = typeof(Startup).GetTypeInfo().Assembly.GetName().Name + \".xml\";            return Path.Combine(basePath, fileName);      }   }}Please let me know what you think about this coding story. A feedback is very much appreciated üëç.Reference  https://docs.microsoft.com/en-us/aspnet/core/tutorials/web-api-help-pages-using-swagger  https://docs.microsoft.com/en-us/aspnet/core/tutorials/getting-started-with-swashbuckle  https://github.com/microsoft/aspnet-api-versioning/tree/master/samples/aspnetcore"
    },
  
    {
      "id": "70",
      "title": "Develop Clean Command Line Applications with System.CommandLine. Clean CLI.",
      "url": "/dotnet/cli/2021/06/06/clean-cli.html",
      "date": "June 06, 2021",
      "categories": ["dotnet","CLI"],
      "tags": ["dotnet","cli","console"],
      "shortinfo": "In this blog post I will show you how to develop powerful console applications that could benefit of your existing Clean Architecture projects in a great way.",
      "content": "TL;DRYou can use System.CommandLine to build console applications. The blog post explains how to build one on top of Clean Architecture solution. You can check out the sample, it contains more information and source code: https://github.com/NikiforovAll/clean-cli-todo-example.As a developer, I quite often want to create a console application to try things out. Usually, it works well, but I always find myself in an inconvenient position. The Program.cs bloats in messy monster with actual useful nuggets of codes in between of code that works with args and some plumbing code. For some tasks, it works, but for others, I would like to suggest something more manageable and clean.Introduction to System.CommandLineSystem.CommandLine gives you a great experience by providing essential functionality, such as parsing, invocation, and rendering. Let‚Äôs see how we can write a simple ‚ÄúTodo List‚Äù application with it.Here is the simple console application that prints ‚Äúhelp‚Äù provided automatically by the System.CommandLine.// Program.csusing System.CommandLine;using System.CommandLine.Builder;using System.CommandLine.Invocation;using System.CommandLine.Parsing;var root = new RootCommand(\"Root command description\");root.Handler = CommandHandler.Create(() =&gt; root.Invoke(\"-h\"));var builder = new CommandLineBuilder(root);var parser = builder.UseDefaults().Build();await parser.InvokeAsync(args);And the output:$ dotnet runapp-name  Root command descriptionUsage:  app-name [options]Options:  --version       Show version information  -?, -h, --help  Show help and usage informationThe Command composition defines the structure and the way your app looks and feels. You can assign ICommandHandler to the command. It is used during the invocation phase and provides you model-binding functionality. I suggest you to go through https://github.com/dotnet/command-line-api/blob/main/docs/How-To.md and https://github.com/dotnet/command-line-api/blob/main/docs/Features-overview.md to get yourself more comfortable with this awesome library.Let‚Äôs move forward and see a slightly different version built on top of .NET Core Generic Host.The code below performs the following:  Initializes and configures CommandLineBuilder.  Plugs IHostBuilder into CommandLineBuilder so it can be used later to build an invocation pipeline.  Builds Parser based on the configured instance of CommandLineBuilder.  Invokes Parser with arguments provided by Program.Main method.var parser = BuildCommandLine()    .UseHost(_ =&gt; Host.CreateDefaultBuilder(args), (builder) =&gt;    {        builder.ConfigureServices((hostContext, services) =&gt;        {            var configuration = hostContext.Configuration;            // register other dependencies here        })        .UseCommandHandler&lt;ExampleCommand, ExampleCommand.Handler&gt;()            }).UseDefaults().Build();return await parser.InvokeAsync(args);static CommandLineBuilder BuildCommandLine(){   var root = new RootCommand();   root.AddCommand(new ExampleCommand());    return new CommandLineBuilder(root);}The benefit of this style is that you can easily understand how CLI application is composed and what are the dependencies.As you may notice, we hooked up the command and corresponding handler via UseCommandHandler. It allows us to resolve dependencies for command handlers.public class ExampleCommand : Command{   public ExampleCommand() : base(name: \"example\", \"Example description\") {}   public new class Handler : ICommandHandler   {      private readonly IMediator meditor;      public Handler(IMediator meditor) =&gt;            this.meditor = meditor ?? throw new ArgumentNullException(nameof(meditor));      public async Task&lt;int&gt; InvokeAsync(InvocationContext context)      {         await this.meditor.Send(new ExampleQuery{});         return 0;      }   }}With this knowledge in mind, let‚Äôs move further with something more interesting functionality to implement. I would like to show you how to use ‚ÄúClean Architecture‚Äù approach together with CLI applications, therefore Clean CLI. I will use https://github.com/jasontaylordev/CleanArchitecture as an example of Clean Architecture project, please investigate sample code based before continue reading the blog post https://github.com/NikiforovAll/clean-cli-todo-example.System.CommandLine ‚ûï Clean Architecture = Clean CLIOur goal is to build ‚ÄúTodo List‚Äù application. üìÉüìùDesign goals:  To use a CLI as UI for a Clean Architecture based solution.  To design an application that clearly communicates (through code) implemented functionality (commands) and structural composition in general.  To provide a first-class CLI interface user experience. Luckily, System.CommandLine helps with things such as ‚Äúhelp text‚Äù generation and autocompletion.Before going deeper into source code let‚Äôs examine how to consume the todo-cli.$ dotnet run -- -hCleanCli.Todo.ConsoleUsage:  CleanCli.Todo.Console [options] [command]Options:  --silent        Disables diagnostics output  --version       Show version information  -?, -h, --help  Show help and usage informationCommands:  todolist  Todo lists management  todoitem  Todo items management  migrate   Migrates database$ dotnet run -- todolist -htodolist  Todo lists managementUsage:  CleanCli.Todo.Console [options] todolist [command]Options:  --silent        Disables diagnostics output  -?, -h, --help  Show help and usage informationCommands:  create       Creates todo list  delete &lt;id&gt;  Deletes todo list  get &lt;id&gt;     Gets a todo list  list         Lists all todo lists in the system.$ dotnet run -- todolist create -hcreate  Creates todo listUsage:  CleanCli.Todo.Console [options] todolist createOptions:  -t, --title &lt;title&gt;  Title of the todo list  --dry-run            Displays a summary of what would happen if the given command line were run.  --silent             Disables diagnostics output  -?, -h, --help       Show help and usage informationFrom the code perspective it looks like this (full version):var runner = BuildCommandLine()   .UseHost(_ =&gt; CreateHostBuilder(args), (builder) =&gt; builder      .UseEnvironment(\"CLI\")      .UseSerilog()      .ConfigureServices((hostContext, services) =&gt;      {         services.AddCustomSerilog();         var configuration = hostContext.Configuration;         services.AddCli(); // Dependencies defined by CLI project (this).         services.AddApplication(); // Register \"Application\" project.         services.AddInfrastructure(configuration); // Register \"Infrastructure\" project.      })      .UseCommandHandler&lt;CreateTodoListCommand, CreateTodoListCommand.Handler&gt;()      .UseCommandHandler&lt;DeleteTodoListCommand, DeleteTodoListCommand.Handler&gt;()      .UseCommandHandler&lt;ListTodosCommand, ListTodosCommand.Handler&gt;()      .UseCommandHandler&lt;GetTodoListCommand, GetTodoListCommand.Handler&gt;()      .UseCommandHandler&lt;SeedTodoItemsCommand, SeedTodoItemsCommand.Handler&gt;()      .UseCommandHandler&lt;MigrateCommand, MigrateCommand.Handler&gt;())         .UseDefaults().Build();static CommandLineBuilder BuildCommandLine(){   var root = new RootCommand();   root.AddCommand(BuildTodoListCommands());   root.AddCommand(BuildTodoItemsCommands());   root.AddCommand(new MigrateCommand());   root.AddGlobalOption(new Option&lt;bool&gt;(\"--silent\", \"Disables diagnostics output\"));   root.Handler = CommandHandler.Create(() =&gt; root.Invoke(\"-h\"));   return new CommandLineBuilder(root);   // omitted for brevity}As result, the project is plugged through DI to console application and all features provided by Application project could be consumed from Console project.The application functionality is added as following:// Application/ServiceCollectionExtensions.cspublic static class ServiceCollectionExtensions{   public static IServiceCollection AddApplication(this IServiceCollection services)   {      services.AddAutoMapper(Assembly.GetExecutingAssembly());      services.AddValidatorsFromAssembly(Assembly.GetExecutingAssembly());      services.AddMediatR(Assembly.GetExecutingAssembly());      services.AddTransient(typeof(IPipelineBehavior&lt;,&gt;), typeof(ValidationBehaviour&lt;,&gt;));      services.AddTransient(typeof(IPipelineBehavior&lt;,&gt;), typeof(PerformanceBehaviour&lt;,&gt;));      return services;   }}// Infrastructure/ServiceCollectionExtensions.cspublic static class ServiceCollectionExtensions{   public static IServiceCollection AddInfrastructure(this IServiceCollection services, IConfiguration configuration)   {      services.AddDbContext&lt;ApplicationDbContext&gt;(options =&gt;            options.UseSqlite(configuration.GetConnectionString(\"DefaultConnection\"),               b =&gt; b.MigrationsAssembly(typeof(ApplicationDbContext).Assembly.FullName)));      services.AddScoped&lt;IApplicationDbContext&gt;(provider =&gt; provider.GetService&lt;ApplicationDbContext&gt;());      services.AddScoped&lt;IDomainEventService, DomainEventService&gt;();      services.AddTransient&lt;IDateTime, DateTimeService&gt;();      return services;   }}Finally, we can add code that does actually something useful.public class CreateTodoListCommand : Command{   public IConsole Console { get; set; }   public CreateTodoListCommand() : base(name: \"create\", \"Creates todo list\")   {      this.AddOption(new Option&lt;string&gt;(new string[] { \"--title\", \"-t\" }, \"Title of the todo list\"));   }   public new class Handler : ICommandHandler   {      private readonly IMediator meditor;      public string Title { get; set; } // Conventional binding      public Handler(IMediator meditor) =&gt;            this.meditor = meditor ?? throw new ArgumentNullException(nameof(meditor));      public async Task&lt;int&gt; InvokeAsync(InvocationContext context)      {            await this.meditor.Send(new CreateTodoListCommand { Title = this.Title });            return 0;      }   }}Underlying implementation from Application project.public class CreateTodoListCommand : IRequest&lt;int&gt;{   public string Title { get; set; }}public class CreateTodoListCommandHandler : IRequestHandler&lt;CreateTodoListCommand, int&gt;{   private readonly IApplicationDbContext context;   public CreateTodoListCommandHandler(IApplicationDbContext context) =&gt;      this.context = context;   public async Task&lt;int&gt; Handle(      CreateTodoListCommand request,      CancellationToken cancellationToken)   {      var entity = new TodoList { Title = request.Title };      this.context.TodoLists.Add(entity);      await this.context.SaveChangesAsync(cancellationToken);      return entity.Id;   }}If you are interested in this approach, I encourage you to investigate the source code on your own.DemoLet‚Äôs run migrate command to create local todo.db and seed initial data.Create todo list:List commands:See todo list details:I hope you find this blog post useful. Take care!Reference  https://github.com/dotnet/command-line-api  https://clig.dev/  https://spectreconsole.net/  https://github.com/jasontaylordev/CleanArchitecture  https://github.com/NikiforovAll/clean-cli-todo-example/tree/v0.1.0 - template-like version."
    },
  
    {
      "id": "71",
      "title": "Publish NuGet packages via Docker Release Container",
      "url": "/dotnet/nuget/docker/2021/05/09/nuget-release-container.html",
      "date": "May 09, 2021",
      "categories": ["dotnet","nuget","docker"],
      "tags": ["dotnet","nuget","docker","cicd"],
      "shortinfo": "Release container is a docker image that is used to distribute and release components.",
      "content": "TL;DRYou can use Docker to push packages to a NuGet feed. This blog post shows how to release a NuGet package to Amazon CodeArtifact via Docker. Source code can be found at https://github.com/NikiforovAll/docker-release-container-sample.GeneralThe idea behind having a release container is pretty straightforward - you can bundle artifacts and tools so the release mechanism is portable and unified because of Docker. Also, another advantage of building NuGet packages in Docker is that you don‚Äôt need any dependencies installed on the build-server itself. I invite you to read Andrew‚Äôs Lock post to get more details about the use case (https://andrewlock.net/pushing-nuget-packages-built-in-docker-by-running-the-container/). This blog post is focused on the practical side, let‚Äôs dive into it by reviewing the Dockerfile:  Base layer is used for publishing. It contains aws-cli and credential provider (AWS.CodeArtifact.NuGet.CredentialProvider) so we can deploy to private NuGet feed as described in here. Please see the excellent guide on how to work with Docker and NuGet feeds https://github.com/dotnet/dotnet-docker/blob/main/documentation/scenarios/nuget-credentials.md.  Build layer is used for building and packing.  Entrypoint defines custom publishing script, essentially, dotnet nuget push is called. Note that, you can specify additional arguments. (e.g: override --source or provide --api-key).FROM mcr.microsoft.com/dotnet/core/sdk:3.1-buster AS baseRUN apt-get update &amp;&amp; apt install unzip &amp;&amp; apt-get install -y curlRUN curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"RUN unzip awscliv2.zip &amp;&amp; ./aws/installWORKDIR /artifactsRUN dotnet new tool-manifest --name manifestRUN dotnet tool install --ignore-failed-sources AWS.CodeArtifact.NuGet.CredentialProviderRUN dotnet codeartifact-creds installFROM mcr.microsoft.com/dotnet/sdk:5.0-buster-slim AS buildARG Configuration=\"Release\"ENV DOTNET_CLI_TELEMETRY_OPTOUT=true \\    DOTNET_SKIP_FIRST_TIME_EXPERIENCE=trueWORKDIR /srcCOPY [\"src/ReleaseContainerSample/ReleaseContainerSample.csproj\", \"src/ReleaseContainerSample/\"]COPY [\"tests/ReleaseContainerSample.Tests/ReleaseContainerSample.Tests.csproj\", \"tests/ReleaseContainerSample.Tests/\"]RUN dotnet restore \"src/ReleaseContainerSample/ReleaseContainerSample.csproj\"COPY . .RUN dotnet build \"src/ReleaseContainerSample\" \\    --configuration $Configuration    # --no-restoreRUN dotnet test \"tests/ReleaseContainerSample.Tests\" \\    --configuration $Configuration \\    --no-buildFROM build AS publishARG Configuration=\"Release\"ARG Version=1.0.0RUN dotnet pack \"src/ReleaseContainerSample\"\\    -p:Version=$Version \\    --configuration $Configuration \\    --output /artifacts \\    --include-symbolsFROM base AS finalWORKDIR /artifactsCOPY --from=publish /artifacts .COPY ./build/publish-nuget.sh ./publish-nuget.shLABEL org.opencontainers.image.title=\"ReleaseContainerSample\" \\    org.opencontainers.image.description=\"\" \\    org.opencontainers.image.documentation=\"https://github.com/NikiforovAll/docker-release-container-sample\" \\    org.opencontainers.image.source=\"https://github.com/NikiforovAll/docker-release-container-sample.git\" \\    org.opencontainers.image.url=\"https://github.com/NikiforovAll/docker-release-container-sample\" \\    org.opencontainers.image.vendor=\"\"ENTRYPOINT [\"./publish-nuget.sh\"]CMD [\"--source\", \"https://api.nuget.org/v3/index.json\"]Before we produce artifact, we need to specify version. Let‚Äôs use GitVersion to get the build version.$ Version=`docker run --rm -v \"$(pwd):/repo\" gittools/gitversion:5.6.6 /repo \\    | tr { '\\n' | tr , '\\n' | tr } '\\n' \\    | grep \"NuGetVersion\" \\    | awk -F'\"' '{print $4}' | head -n1` &amp;&amp; echo $Version# out1.0.1After that, we are ready to build the release container (image)$ docker build -f ./src/ReleaseContainerSample/Dockerfile \\    --build-arg Version=\"$Version\" \\    -t release-container-example .# check the result$ docker image list REPOSITORY                                                          TAG              IMAGE ID       CREATED          SIZErelease-container-example                                           latest           7ca4acd3845b   43 seconds ago   1.12GBYou can peek inside release container by running:$ docker run --rm --entrypoint '/bin/ls' --name release-container-sample release-container-example# outReleaseContainerSample.1.0.1.nupkgReleaseContainerSample.1.0.1.symbols.nupkgpublish-nuget.shüöÄ Publish.docker run --rm \\    -e AWS_ACCESS_KEY_ID=\"\" \\    -e AWS_SECRET_ACCESS_KEY=\"\" \\    -e AWS_DEFAULT_REGION=\"eu-central-1\" \\    --name release-container-sample release-container-example \\    --source \"https://codeartifact.eu-central-1.amazonaws.com/nuget/codeartifact-repository/v3/index.json\"# Alternatively, you can use public  NuGet repository.docker run --rm \\    --name release-container-sample release-container-example \\    --source \"https://api.nuget.org/v3/index.json\" --api-key \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"SummaryIn this blog post, I showed how you can build NuGet packages via Docker, and push them to your NuGet feed when you run the container.Pros:  Easy to release. The solution is portable. It‚Äôs our goal after all.  Extendable approach. You are in charge of how to build NuGet package and can install all required tools and dependencies when you need it.Cons:  Images can be quite sizable. Additional space is required to release containers, so a retention policy should be applied.  Adds unnecessary complexity if you already use dotnet toolchain and you have all dependencies installed on build server.Reference  https://docs.aws.amazon.com/codeartifact/latest/ug/nuget-cli.html  https://github.com/dotnet/dotnet-docker/blob/main/documentation/scenarios/nuget-credentials.md  https://andrewlock.net/pushing-nuget-packages-built-in-docker-by-running-the-container"
    },
  
    {
      "id": "72",
      "title": "ASP.NET Core Endpoints. Add endpoint-enabled middleware by using IEndpointRouteBuilder extension method",
      "url": "/dotnet/aspnetcore/2021/03/23/endpoint-route-builder-extension-pattern.html",
      "date": "March 23, 2021",
      "categories": ["dotnet","aspnetcore"],
      "tags": ["dotnet","aspnetcore"],
      "shortinfo": "An example of how to add middleware for a selected endpoint. EndpointRouteBuilderExtensions pattern.",
      "content": "TL;DRYou can use extension method (e.g. IEndpointConventionBuilder Map{FeatureToMap}(this IEndpointRouteBuilder endpoints) to add a middleware to specific endpoint.MiddlewareMiddleware forms the basic building blocks of the HTTP Pipeline. It is a really good concept to implement cross-cutting concerns and weave a re-usable piece of code to the ASP.NET pipeline. Middleware provides application-level features. For example, you might need Middleware to implement features like: Routing, Cookies, Session, CORS, Authentication, HTTPS Redirection, Caching, Response Compression, Exception Handling. Most of the time, you‚Äôve got out-of-the-box option provided by the framework.To extend ASP.NET Core pipeline we use IApplicationBuilder injected in Startup.cs.Furthermore, since ASP.NET Core 3.0 you could use HttpContext.GetEndpoint to retrieve selected endpoint/metadata, e.g.:// Before routing runs, endpoint is always null hereapp.Use(next =&gt; context =&gt;{    Console.WriteLine($\"Endpoint: {context.GetEndpoint()?.DisplayName ?? \"(null)\"}\");    return next(context);});app.UseRouting();// After routing runs, endpoint will be non-null if routing found a matchapp.Use(next =&gt; context =&gt;{    Console.WriteLine($\"Endpoint: {context.GetEndpoint()?.DisplayName ?? \"(null)\"}\");    return next(context);});Middleware can exist as simple inline methods or as complex, reusable classes. If you don‚Äôt like RequestDelegate notation/form. You can use IApplicationBuilder.UseMiddleware extension method to add middleware as a class and access an endpoint from parameter (HttpContext).// Startup.csapp.UseMiddleware&lt;MyAwesomeMiddleware&gt;();// ...// MyAwesomeMiddleware.cspublic async Task InvokeAsync(HttpContext httpContext){    var endpoint = httpContext.GetEndpoint();}If you need something more fine-grained and, probably, more powerful you can use Endpoint concept.Routing and EndpointsRouting is responsible for matching incoming HTTP requests and dispatching those requests to the app‚Äôs executable endpoints. You can use IApplicationBuilder.UseEndpoints to define pipeline logic based on a selected route. Many of ASP.NET Core features/aspects are implemented with the routing concept in mind. For example, you can IEndpointRouteBuilder.MapControllers or IEndpointRouteBuilder.MapGrpcService or you can build your own framework based on this extension capabilities.app.UseEndpoints(endpoints =&gt;{    // Configure another endpoint, with authorization.    endpoints.MapGet(\"/\", async context =&gt;    {        await context.Response.WriteAsync($\"Hello from {context.GetEndpoint()}!\");    }).RequireAuthorization().WithMetadata(new AuditPolicyAttribute(needsAudit: true));});As you can see, Endpoint contains an EndpointMetadataCollection that holds various data put and managed during pipeline execution./// &lt;summary&gt;/// Represents a logical endpoint in an application./// &lt;/summary&gt;public class Endpoint{    public Endpoint(        RequestDelegate? requestDelegate,        EndpointMetadataCollection? metadata,        string? displayName)    {        RequestDelegate = requestDelegate;        Metadata = metadata ?? EndpointMetadataCollection.Empty;        DisplayName = displayName;    }    /// &lt;summary&gt;    /// Gets the informational display name of this endpoint.    /// &lt;/summary&gt;    public string? DisplayName { get; }    /// &lt;summary&gt;    /// Gets the collection of metadata associated with this endpoint.    /// &lt;/summary&gt;    public EndpointMetadataCollection Metadata { get; }    /// &lt;summary&gt;    /// Gets the delegate used to process requests for the endpoint.    /// &lt;/summary&gt;    public RequestDelegate? RequestDelegate { get; }}The big picture:Practical ExampleRecently, I‚Äôve stumbled upon IConfigurationRoot.GetDebugView. Basically, it allows you to dump configuration values and reason about how configuration settings are resolved. Personally, I find this a really useful and productive way of reading the component configuration. You can enable a special endpoint to check your configuration by adding the following code to your UseEndpoints method (as part of Startup.cs).app.UseEndpoints(endpoints =&gt;{    endpoints.MapGet(\"/config\", async context =&gt;    {        var config = (Configuration as IConfigurationRoot).GetDebugView();        await context.Response.WriteAsync(config);    });});Here is an example output:NikiforovAll.ConfigurationDebugViewEndpointThe example above works and everything, but I took a chance and wrapped this functionality into NuGet package - https://github.com/NikiforovAll/ConfigurationDebugViewEndpoint.It is an example of how to organize code base and apply the technique to make your Startup.cs more readable and composable. I will guide you through the process.Our goal is to have an extension method that will allow us to plug /config endpoint. Something like this:app.UseEndpoints(endpoints =&gt;{    endpoints.MapConfigurationDebugView(\"/config\", (options) =&gt; options.AllowDevelopmentOnly = true);});Let‚Äôs start with the ConfigurationDebugViewMiddleware. We want to write IConfiguration debug view output to Response.public class ConfigurationDebugViewMiddleware{    public ConfigurationDebugViewMiddleware(RequestDelegate next) { }    public async Task InvokeAsync(HttpContext httpContext)    {        var configuration = httpContext.RequestServices.GetService&lt;IConfiguration&gt;();        var config = (configuration as IConfigurationRoot).GetDebugView();        await httpContext.Response.WriteAsync(config);    }}The trick is to create EndpointRouteBuilderExtensions.cs and write a small extension method that will allow us to plug ConfigurationDebugViewMiddleware.Generally, we follow the next approach/signatureIEndpointConventionBuilder Map{FeatureToMap}(this IEndpointRouteBuilder endpoints, string pattern = &lt;defaultPattern&gt;, Action&lt;{FeatureOptions}&gt; configure);The actual implementation is based on the fact that you can create a sub-pipeline using the same abstraction - IApplicationBuilder that you use for your Startup.cs pipeline. IEndpointRouteBuilder.CreateApplicationBuilder creates a IApplicationBuilder that you can use and configure.For example, here is a definition of an imaginary pipeline that handles SOAP requests:var pipeline = endpoints.CreateApplicationBuilder()    .UseMiddleware&lt;LoggingMiddleware&gt;()    .UseMiddleware&lt;CachingMiddleware&gt;()    .UseMiddleware&lt;SOAPEndpointMiddleware&gt;()    .Build();return endpoints.Map(pattern, pipeline)Complete implementation:/// &lt;summary&gt;/// Provides extension methods for &lt;see cref=\"IEndpointRouteBuilder\"/&gt; to add routes./// &lt;/summary&gt;public static class EndpointRouteBuilderExtensions{    /// &lt;summary&gt;    /// Adds a configuration endpoint to the &lt;see cref=\"IEndpointRouteBuilder\"/&gt; with the specified template.    /// &lt;/summary&gt;    /// &lt;param name=\"endpoints\"&gt;The &lt;see cref=\"IEndpointRouteBuilder\"/&gt; to add endpoint to.&lt;/param&gt;    /// &lt;param name=\"pattern\"&gt;The URL pattern of the endpoint.&lt;/param&gt;    /// &lt;param name=\"optionsDelegate\"&gt;&lt;/param&gt;    /// &lt;returns&gt;A route for the endpoint.&lt;/returns&gt;    public static IEndpointConventionBuilder? MapConfigurationDebugView(        this IEndpointRouteBuilder endpoints,        string pattern = \"config\",        Action&lt;ConfigurationDebugViewOptions&gt;? optionsDelegate = default)    {        if (endpoints == null)        {            throw new ArgumentNullException(nameof(endpoints));        }        var options = new ConfigurationDebugViewOptions();        optionsDelegate?.Invoke(options);        return MapConfigurationDebugViewCore(endpoints, pattern, options);    }    private static IEndpointConventionBuilder? MapConfigurationDebugViewCore(IEndpointRouteBuilder endpoints, string pattern, ConfigurationDebugViewOptions options)    {        var environment = endpoints.ServiceProvider.GetRequiredService&lt;IHostEnvironment&gt;();        var builder = endpoints.CreateApplicationBuilder();        if (options.AllowDevelopmentOnly &amp;&amp; !environment.IsDevelopment())        {            return null;        }        var pipeline = builder            .UseMiddleware&lt;ConfigurationDebugViewMiddleware&gt;()            .Build();        return endpoints.Map(pattern, pipeline);    }}SummaryI‚Äôve shared with you a simple but yet useful technique to organize your codebase. It allows you to keep your endpoint-related code nice and clean, developers can easily find extensions by conventions. Do you have other best practices to organize Startup.cs? Feel free to list them in the comments, I would like to hear from you!You might want to try the configuration-debug-view NuGet package. https://github.com/NikiforovAll/ConfigurationDebugViewEndpoint.Reference  ASP.NET Core Middleware  Routing in ASP.NET Core  ASP.NET Core Series: Endpoint Routing  ASP.NET Core Series: Route To Code"
    },
  
    {
      "id": "73",
      "title": "Advance the practical side of your coding skills with CodingStories",
      "url": "/dotnet/coding-stories/2021/03/14/coding-story.html",
      "date": "March 14, 2021",
      "categories": ["dotnet","coding-stories"],
      "tags": ["dotnet","coding-stories"],
      "shortinfo": "In this blog post, I share my experience of writing a coding story and give you instructions on how to get started.",
      "content": "TL;DRIn this blog post, I will share with you what coding story is about and how to write your own coding story.IntroductionRecently, I‚Äôve discovered a new and practical way to hone my programming skills. The approach is called CodingStories. Basically, a story is some sort of journey, a day-to-day task that you might do at your work. Usually, a coding story shows some new feature, coding techniques, or code refactoring. Generally, you want to guide a user from a bad place to a better one. After all, we all want to write robust and maintainable code.If you think about the regular experience of learning something new and practical, you may find that it is hard to find a definitive guide. From time to time, I encounter tutorials and blog posts that resemble something like this:Sometimes it is frustrating to find an interesting article, but not to be able to reproduce it or learn from it. But, It is nobody‚Äôs fault üòâ. It is hard to describe step-by-step code changes and stick to the topic. CodingStories solves the problem by introducing a useful medium to consume coding stories. From a user perspective, you have a nice and handy UI to go through coding stories and experience how to solve a particular task. Every coding story is based on a git repository. It is essential for a coding story to have a meaningful and clean git history. Additionally, you can always open a coding story locally and work with a git repository on your own.Writing my first coding storyBefore starting coding, I investigated coding.stories - author guide. It gave me somewhat understanding of a coding story and the potential ways I could write one.My first coding story was about Dependency Inversion Principle (D from SOLID). I started with not terrible but still hardly testable version and gradually improved it to a more testable and clean solution.Basically, I copied the initial solution and used the copy as a draft version. The draft version was based on the preliminary commit history. It is not final coding story. Anyway, having this draft helped me to understand the solution and project structure better. After that, it is was easy to reproduce the solution one more time.A coding story author should introduce coding story instructions as part of a commit. So every step is essentially code changes and instructions defined in a dedicated file called .story.md.I will not go through the details of the coding story (discount-calculation-story-csharp). Everything you need is there (kinda). I encourage you to try the interactive version, you may find it here:  https://codingstories.io/story/https:%2F%2Fgitlab.com%2FNikiforovAll%2Fdiscount-calculation-story-csharp.Source repository: https://gitlab.com/NikiforovAll/discount-calculation-story-csharp.More coding stories: https://codingstories.io/storiesCoding story writing principlesHere is a list of principle that I quite valuable after writing my first story:  Keep git commit history clean and write concise commits  Every commit should compile and pass unit tests  Do not try to be perfect, focus on the goal of a coding story  Do not hurry to write a coding story from the first try. It is better to figure out how the end solution will look like. Consider writing a coding story at the very end.  Try to follow common project structure and code formatting guidelines. Examples can be found at https://gitlab.com/codingstories.Writing your first coding storyStudy the basicsHere is how to become an author: https://codingstories.io/become-author/guide. üöÄUse a templateI‚Äôve prepared project template for C# and Java. It helps you to get started and promotes consistent coding story structure among other coding stories.You can find it here https://github.com/NikiforovAll/codingstories-template, here is the changelog: https://github.com/NikiforovAll/codingstories-template/blob/main/CHANGELOG.md.All you need to do is to install it via dotnet-sdk. I‚Äôve prepared a docker wrapper for those who don‚Äôt want to install dotnet-sdk, for more details, please see (ghcr.io/nikiforovall/coding-stories-scaffolder).dotnet new --install CodingStories.Template::1.3.0Check that template is installed successfully:$ dotnet new -l | grep storyCoding Stories Template                       story-java           java        Epam/CodingStoriesCoding Stories Template                       story                [C#]        Epam/CodingStoriesScaffold a coding story:dotnet new story -n MyFirstCodingStory # --no-open-todo --no-devcontainerThe output of the command above:$ tree.‚îú‚îÄ‚îÄ global.json‚îú‚îÄ‚îÄ MyFirstCodingStory‚îÇ   ‚îú‚îÄ‚îÄ Class1.cs‚îÇ   ‚îî‚îÄ‚îÄ MyFirstCodingStory.csproj‚îú‚îÄ‚îÄ MyFirstCodingStory.sln‚îú‚îÄ‚îÄ MyFirstCodingStory.Tests‚îÇ   ‚îú‚îÄ‚îÄ MyFirstCodingStory.Tests.csproj‚îÇ   ‚îî‚îÄ‚îÄ UnitTest1.cs‚îú‚îÄ‚îÄ README.md‚îî‚îÄ‚îÄ TODO.htmlWrite a coding storyI‚Äôve decided to write a coding story on how to write a coding story. Don‚Äôt you think it is a reasonable thing to do? ü§îYou can find it here:  https://codingstories.io/story/https%3A%2F%2Fgitlab.com%2FNikiforovAll%2Fcoding-story-coding-story ‚≠êSource repository: https://gitlab.com/NikiforovAll/coding-story-coding-storyGenerate a coding story linkTo check your coding story, you need to prepare the URL that is based on gitlab repository. Note, currently, CodingStories caches coding stories for two hours, so please make sure that you read.Down below you may find the form to generate a valid coding story link from gitlab repository link.                                        Generate CodingStory URL                                            SummaryI hope that I‚Äôve intrigued you enough to write your first coding story. Let me know what you think!"
    },
  
    {
      "id": "74",
      "title": "Task-based asynchronous Pattern and Composition (aka Task Combinators)",
      "url": "/dotnet/async/2021/02/26/tap-composition.html",
      "date": "February 26, 2021",
      "categories": ["dotnet","async"],
      "tags": ["dotnet","async"],
      "shortinfo": "WhenAll, WhenAny, ForEachAsync, Throttling, Process as complete, etc.",
      "content": "TL;DRConcurrency with TPL is fairly simple. In this blog post, I will reiterate over fundamentals and share with you some common patterns that you could use to compose your solutions. This blog post is focused on the concurrent processing of a collection of tasks and possible semantics that could be considered during the design of a solution.Examples and source code can be found here: nikiforovall.blog.examples/tap-compositionPart I: FundamentalsThe Task class provides a life cycle for asynchronous operations, and that cycle is represented by the TaskStatus enumeration. Exceptions occurring during the execution of an asynchronous method are assigned to the returned Task. If the cancellation request is honored such that work is ended prematurely, the Task returned from the TAP method will end in the TaskStatus.Canceled state. Finally, await operator unwraps the result of executing depending on TaskStatus.Built-in task combinators: Task.Run, Task.FromResult, Task.WhenAll, Task.WhenAny, Task.Delay.Task.WhenAll &amp; Task.WhenAnyTask.WhenAll returns a brand-new Task. It represents the final result of batch execution. If a cancellation (or exception) happens during Task.WhenAll the result will be in TaskStatus.Canceled (TaskStatus.Faulted), but other tasks are not terminated.The Task.WhenAny is used to asynchronously wait on multiple asynchronous operations represented as Tasks, asynchronously waiting for just one of them to complete. Unlike Task.WhenAll, which in the case of successful completion of all tasks returns a list of their unwrapped results, Task.WhenAny returns the Task that completed.I suggest you investigate the behavior of Task.WhenAll and Task.WhenAny by exploring Unit Tests, so you understand how it behaves in certain scenarios.Part II: CombinatorsLet‚Äôs compare processing algorithms by the following qualities:  Order of processing. Indicates how tasks are scheduled and executed relative to each other.  Process as Complete. Describes how and when the results are processed.  Throttled. Possibility to control the amount of workload at a given point in time.  Batch Cancellation. Possibility to cancel other tasks in case of cancellation request.  Interleaving. Isolated result processing.Vanilla Task.WhenAllWhenTask.WhenAll is used on a collection of tasks, all tasks are run to completion, i.e.: Task.IsCompleted = true. Results could be unwrapped from Task&lt;TResult[]&gt;.            Name      Order of processing      Process as Complete      Throttled      Batch Cancellation      Interleaving                  Task.WhenAll      Concurrent ‚≠ê      false      false      Pass cancellation token to each task      N/A, results are aggregated and it is up to implementor how to process them      E.g.:[Fact]public async Task WhenAll_Result_Unwrapped(){    var job1 = RunJob(100, 1);    var job2 = RunJob(300, 2);    var whenAllTask = Task.WhenAll(job1, job2);    var results = await whenAllTask;    Assert.Equal(new int[] { 1, 2 }, results);    Assert.True(whenAllTask.IsCompletedSuccessfully);    Assert.Equal(TaskStatus.RanToCompletion, whenAllTask.Status);}Cancellation variation:[Fact]public async Task WhenAll_SharedCancellationToken_AllCancelled(){    var cancellationTokenSource = new CancellationTokenSource(50);    var t = cancellationTokenSource.Token;    var job1 = RunJob(100, 1, t);    var job2 = RunJob(150, 2, t);    var job3 = RunJob(200, 3, t);    List&lt;Task&gt; batch = new() { job1, job2, job3 };    var whenAllTask = Task.WhenAll(batch);    try    { await whenAllTask; }    catch { };    Assert.All(batch, t =&gt; Assert.True(t.IsCanceled));}Sequential ForEachAsyncVery simple implementation, just to demonstrate the different sides of the problem.            Name      Order of processing      Process as Complete      Throttled      Batch Cancellation      Interleaving                  SequentialBlockingForEachAsync      Sequential      false, sequential nature of processing impedes this      true, not possible to control concurrency level      N/A, easy to add on demand      true ‚≠ê      public static class SequentialBlockingForEachAsync{    public static async Task ForEachAsync&lt;TResult, TSource&gt;(        this IEnumerable&lt;TSource&gt; list,        Func&lt;TSource, Task&lt;TResult&gt;&gt; taskSelector,        Action&lt;TSource, TResult&gt; resultProcessor)    {        foreach (var value in list)        {            resultProcessor(value, await taskSelector(value));        }    }}üí° Note, you can change the parameter resultProcessor to run continuation or adjust to your needs.Run the code:private static IEnumerable&lt;int&gt; GenerateData(){    yield return 3;    yield return 2;    yield return 1;}[Benchmark]public async Task SequentialBlocking() =&gt;    await GenerateData().ForEachAsync(async i =&gt;        {            await Task.Delay(i * 100);            return i;        }, Empty);private static void Empty&lt;T&gt;(T source, T result) { }// |             Method |     Mean |   Error |  StdDev |// |------------------- |---------:|--------:|--------:|// | SequentialBlocking | 600.8 ms | 0.19 ms | 0.18 ms |Concurrent Interleaved ForEachAsyncThe goal of this implementation is to achieve the maximum concurrency, but process results separate from each other:Can be described as following: (ref)      For each element in an enumerable, run a function that returns a Task{TResult} to represent the completion of processing that element. All of these functions may run asynchronously concurrently.        As each task completes, run a second processing action over the results.  All of these actions must be run sequentially, but order doesn‚Äôt matter.              Name      Order of processing      Process as Complete      Throttled      Batch Cancellation      Interleaving                  ConcurrentInterleavedForEachAsync      Concurrent ‚≠ê      true      false      N/A, easy to add on demand      true ‚≠ê      public static class ConcurrentIsolatedForEachAsync{    public static Task ForEachAsync&lt;TSource, TResult&gt;(        this IEnumerable&lt;TSource&gt; source,        Func&lt;TSource, Task&lt;TResult&gt;&gt; taskSelector, Action&lt;TSource, TResult&gt; resultProcessor)    {        SemaphoreSlim oneAtATime = new(initialCount: 1, maxCount: 1);        return Task.WhenAll(            from item in source            select ProcessAsync(item, taskSelector, resultProcessor, oneAtATime));    }    private static async Task ProcessAsync&lt;TSource, TResult&gt;(        TSource item,        Func&lt;TSource, Task&lt;TResult&gt;&gt; taskSelector, Action&lt;TSource, TResult&gt; resultProcessor,        SemaphoreSlim oneAtATime)    {        TResult result = await taskSelector(item);        await oneAtATime.WaitAsync();        try        { resultProcessor(item, result); }        finally { oneAtATime.Release(); }    }}üí° Note, you can change the parameter resultProcessor to run continuation or adjust to your needs.Run the code:private static IEnumerable&lt;int&gt; GenerateData(){    yield return 3;    yield return 2;    yield return 1;}[Benchmark]public async Task ConcurrentIsolated() =&gt;    await GenerateData().ForEachAsync(        GenerateData(), async i =&gt;        {            await Task.Delay(i * 100);            return i;        }, Empty);private static void Empty&lt;T&gt;(T source, T result) { }// |             Method |     Mean |   Error |  StdDev |// |------------------- |---------:|--------:|--------:|// | ConcurrentIsolated | 299.1 ms | 1.42 ms | 1.26 ms |Concurrent Interleaved CombinatorFor those who don‚Äôt like working with lambdas and ForEach LINQ operator. Personally, I really like this approach because it produces better stack traces.            Name      Order of processing      Process as Complete      Throttled      Batch Cancellation      Interleaving                  ConcurrentInterleavedCombinator      Concurrent ‚≠ê      true      false      N/A, easy to add on demand      true ‚≠ê      public static class ConcurrentInterleavedCombinator{    public static Task&lt;Task&lt;T&gt;&gt;[] Interleaved&lt;T&gt;(IEnumerable&lt;Task&lt;T&gt;&gt; tasks)    {        var inputTasks = tasks.ToList();        var buckets = new TaskCompletionSource&lt;Task&lt;T&gt;&gt;[inputTasks.Count];        var results = new Task&lt;Task&lt;T&gt;&gt;[buckets.Length];        for (var i = 0; i &lt; buckets.Length; i++)        {            buckets[i] = new TaskCompletionSource&lt;Task&lt;T&gt;&gt;();            results[i] = buckets[i].Task;        }        var nextTaskIndex = -1;        void continuation(Task&lt;T&gt; completed)        {            var bucket = buckets[Interlocked.Increment(ref nextTaskIndex)];            _ = bucket.TrySetResult(completed);        }        foreach (var inputTask in inputTasks)        {            _ = inputTask.ContinueWith(                continuation,                CancellationToken.None,                TaskContinuationOptions.ExecuteSynchronously,                TaskScheduler.Default);        }        return results;    }}Here, we create TaskCompletionSource&lt;Task&lt;T&gt;&gt; instances to represent the buckets, one bucket per each of the tasks that will eventually complete. Then, we hook up a continuation to each input task. This continuation will get the next available bucket and store the newly completed task into it. refRun the code:private static IEnumerable&lt;int&gt; GenerateData(){    yield return 3;    yield return 2;    yield return 1;}[Benchmark]public async Task ConcurrentInterleavedCombinator(){    var tasks = GenerateData().Select(async i =&gt;    {        await Task.Delay(i * 100);        return i;    });    foreach (var bucket in Interleaved(tasks))    {        var t = await bucket;    }}// |                          Method |     Mean |   Error |  StdDev |// |-------------------------------- |---------:|--------:|--------:|// | ConcurrentInterleavedCombinator | 299.4 ms | 1.01 ms | 0.94 ms |Throttled Interleaved WhenAllBefore this, we don‚Äôt really have control over consumed resources. It is a common task to implement throttling, here is how you can do that.            Name      Order of processing      Process as Complete      Throttled      Batch Cancellation      Interleaving                  WhenAllThrottled      Concurrent*      true      true ‚≠ê      N/A      true ‚≠ê      public static class WhenAllThrottledExtensions{    public static async Task WhenAllThrottled(this IEnumerable&lt;Task&gt; source, int throttled)    {        var tasks = new List&lt;Task&gt;();        throttled--;        foreach (var task in source)        {            if (tasks.Count == throttled)            {                var finishedTask = await Task.WhenAny(tasks);                _ = tasks.Remove(finishedTask);            }            tasks.Add(task);        }        await Task.WhenAll(tasks);    }}For more details, please see corresponding Unit Tests: Appendix - RunWhenAllThrottled.üí° This is a simplified version without result processing. Try to convert this snippet to ThrottledForEachAsync version.Run the code:private static IEnumerable&lt;int&gt; GenerateData(){    yield return 2;    yield return 2;    yield return 1;    yield return 1;}[Benchmark]public async Task ThrottledInterleaved() =&gt;    await GenerateData().Select(i =&gt; Task.Delay(i * 100)).WhenAllThrottled(2);// |               Method |     Mean |   Error |  StdDev |// |--------------------- |---------:|--------:|--------:|// | ThrottledInterleaved | 300.6 ms | 0.06 ms | 0.05 ms |Throttled ControlDegreeOfParallelism ForEachAsyncYou don‚Äôt always need interleaving. Let‚Äôs see the approach based on System.Collections.Concurrent.Partitioner.It limits the number of operations that are able to run in parallel. One way to achieve that is to partition the input data set into N partitions, where N is the desired maximum degree of parallelism, and schedule a separate task to begin the execution for each partition. ref.            Name      Order of processing      Process as Complete      Throttled      Batch Cancellation      Interleaving                  ControlParallelismForEachAsync      Concurrent*      true      true ‚≠ê      N/A      false ‚≠ê      public static class ControlParallelismForEachAsyncExtensions{    public static Task ForEachAsync&lt;T&gt;(this IEnumerable&lt;T&gt; source, int dop, Func&lt;T, Task&gt; body)    {        return Task.WhenAll(            from partition in Partitioner.Create(source).GetPartitions(dop)            select Task.Run(async delegate {                using (partition)                    while (partition.MoveNext())                        await body(partition.Current);        }));    }}Run the code:private static IEnumerable&lt;int&gt; GenerateData(){    yield return 2;    yield return 1;    yield return 2;    yield return 1;}[Benchmark]public async Task ControlParallelismForEachAsync() =&gt;    await GenerateData().ForEachAsync(dop: 2, i =&gt; Task.Delay(i * 100));// |                         Method |     Mean |   Error |  StdDev |// |------------------------------- |---------:|--------:|--------:|// | ControlParallelismForEachAsync | 300.5 ms | 0.86 ms | 0.76 ms |SummaryHere is the final comparison table:            Name      Order of processing      Process as Complete      Throttled      Batch Cancellation      Interleaving                  Task.WhenAll      Concurrent ‚≠ê      false      false      true      N/A              SequentialBlockingForEachAsync      Sequential      false      true*      on demand      true ‚≠ê              ConcurrentInterleavedForEachAsync      Concurrent ‚≠ê      true      false      on demand      true ‚≠ê              ConcurrentInterleavedCombinator      Concurrent ‚≠ê      true      false      on demand      true ‚≠ê              WhenAllThrottled      Concurrent*      true      true ‚≠ê      N/A      true ‚≠ê              ControlParallelismForEachAsync      Concurrent*      true      true ‚≠ê      N/A      false ‚≠ê      For now, you have enough knowledge to build more complex combinators to meet your requirements. I suggest you to check Reference for more information.Hope you find this blog useful. Let me know what you think!AppendixAppendix - Example Taskspublic static async Task&lt;int&gt; RunJob(int milliseconds, int taskId, CancellationToken cancellationToken = default){    await Task.Delay(milliseconds, cancellationToken);    return taskId;}public static async Task ThrowJob(int milliseconds = 100){    await Task.Delay(milliseconds);    throw new InvalidOperationException(nameof(ThrowJob));}public static async Task CanceledJob(int milliseconds){    var cancellationTokenSource = new CancellationTokenSource(milliseconds);    await Task.Delay(milliseconds * 2, cancellationTokenSource.Token);}Appendix - Task.WhenAll[Fact]public async Task WhenAll_Result_Unwrapped(){    var job1 = RunJob(100, 1);    var job2 = RunJob(300, 2);    Task&lt;int[]&gt; whenAllTask = Task.WhenAll(job1, job2);    var results = await whenAllTask;    Assert.Equal(new int[] { 1, 2 }, results);    Assert.True(whenAllTask.IsCompletedSuccessfully);    Assert.Equal(TaskStatus.RanToCompletion, whenAllTask.Status);}[Fact]public void WhenAll_ExceptionsThrown_ExceptionsAggregated(){    var job1 = RunJob(100, 1);    var job2 = ThrowJob(100);    var job3 = ThrowJob(100);    var job4 = RunJob(150, 4);    AggregateException captured = default;    var whenAllTask = Task.WhenAll(job1, job2, job3, job4);    try    { whenAllTask.Wait(); }    catch (AggregateException ae) { captured = ae; };    var whenAllException = whenAllTask.Exception;    Assert.True(whenAllTask.IsFaulted);    Assert.Equal(2, captured.Flatten().InnerExceptions.Count);    // this is interesting    Assert.NotSame(whenAllException, captured);    Assert.Equal(whenAllException.Message, captured.Message);}[Fact]public async Task WhenAll_ExceptionsThrown_ExceptionUnwrapped(){    var job1 = RunJob(100, 1);    var job2 = ThrowJob(100);    var job3 = ThrowJob(100);    List&lt;Task&gt; batch = new() { job1, job2, job3 };    InvalidOperationException captured = default;    // t_all = whenall(t1, t2-&gt;throws, t3-&gt;throws)    // t_all.Exception = AggregateException(t2.Exception, t3.Exception)    // await t_all -&gt; unwrap t_all.AggregateException -&gt; select (t2 | t3) as t_selected    // -&gt; unwrap t_selected    var whenAllTask = Task.WhenAll(batch);    try    { await whenAllTask; }    catch (InvalidOperationException e) { captured = e; };    Assert.True(whenAllTask.IsFaulted);    Assert.Equal(TaskStatus.Faulted, whenAllTask.Status);    Assert.Equal(TaskStatus.RanToCompletion, job1.Status);    Assert.Equal(2, whenAllTask.Exception.Flatten().InnerExceptions.Count);    var aggregatedException = Assert.IsType&lt;AggregateException&gt;(job2.Exception);    Assert.Contains(captured, batch.Select(t =&gt; t.Exception?.InnerException));}[Fact]public async Task WhenAll_ExceptionsThrown_UnwrappedChildTask(){    var job1 = RunJob(100, 1);    var job2 = ThrowJob(100);    List&lt;Task&gt; batch = new() { job1, job2 };    InvalidOperationException captured1 = default;    InvalidOperationException captured2 = default;    var whenAllTask = Task.WhenAll(batch);    try    { await whenAllTask; }    catch (InvalidOperationException e) { captured1 = e; };    Assert.Equal(TaskStatus.Faulted, job2.Status);    try    { await job2; }    catch (InvalidOperationException e) { captured2 = e; };    Assert.Same(captured1, captured2);    Assert.Equal(captured1.Message, captured2.Message);}[Fact]public async Task WhenAll_CanceledTask_ExceptionThrown(){    var job1 = RunJob(100, 1);    var job2 = CanceledJob(50);    var job3 = CanceledJob(100);    TaskCanceledException captured = null;    var whenAllTask = Task.WhenAll(job1, job2, job3);    try    { await whenAllTask; }    catch (TaskCanceledException e) { captured = e; };    Assert.True(whenAllTask.IsCompleted &amp;&amp; whenAllTask.IsCanceled);    Assert.Equal(TaskStatus.Canceled, whenAllTask.Status);    Assert.NotNull(captured);    Assert.Null(whenAllTask.Exception);    Assert.Null(job2.Exception);    Assert.Null(job3.Exception);    Assert.Equal(TaskStatus.Canceled, job2.Status);    Assert.Equal(TaskStatus.Canceled, job3.Status);}[Fact]public async Task WhenAll_CanceledAndFaulted_ExceptionOverCancellation(){    var job1 = RunJob(100, 1);    var job2 = CanceledJob(50);    var job3 = ThrowJob(100);    var whenAllTask = Task.WhenAll(job1, job2, job3);    try    { await whenAllTask; }    catch { };    Assert.True(whenAllTask.IsFaulted);    Assert.Equal(TaskStatus.Faulted, whenAllTask.Status);    Assert.Equal(TaskStatus.RanToCompletion, job1.Status);    Assert.Equal(TaskStatus.Canceled, job2.Status);    Assert.Equal(TaskStatus.Faulted, job3.Status);    Assert.IsType&lt;InvalidOperationException&gt;(whenAllTask.Exception.InnerException);    Assert.Null(job2.Exception);}[Fact]public async Task WhenAll_SharedCancellationToken_AllCancelled(){    var cancellationTokenSource = new CancellationTokenSource(50);    var t = cancellationTokenSource.Token;    var job1 = RunJob(100, 1, t);    var job2 = RunJob(150, 2, t);    var job3 = RunJob(200, 3, t);    List&lt;Task&gt; batch = new() { job1, job2, job3 };    var whenAllTask = Task.WhenAll(batch);    try    { await whenAllTask; }    catch { };    Assert.All(batch, t =&gt; Assert.True(t.IsCanceled));}Appendix - Task.WhenAny[Fact]public async Task WhenAny_FirstCanceled(){    var job1 = RunJob(100, 1);    var job2 = CanceledJob(50);    var job3 = ThrowJob(70);    List&lt;Task&gt; batch = new() { job1, job2, job3 };    var waitForAnyTaskTask = Task.WhenAny(batch);    var someTask = await waitForAnyTaskTask;    Assert.Equal(TaskStatus.RanToCompletion, waitForAnyTaskTask.Status);    Assert.Equal(TaskStatus.Canceled, someTask.Status);    var someTask2 = await Task.WhenAny(batch);    Assert.Same(someTask, someTask2);    await Task.Delay(100); // let other tasks to complete    var someTask3 = await Task.WhenAny(batch);    Assert.NotSame(someTask, someTask3);}Appendix - RunWhenAllThrottled[Fact]public async Task WhenAllThrottled_AllGood(){    var batch = Generate();    var whenAllTask = batch.WhenAllThrottled(2);    Assert.Equal(TaskStatus.WaitingForActivation, whenAllTask.Status);    await whenAllTask;    Assert.Equal(TaskStatus.RanToCompletion, whenAllTask.Status);    static IEnumerable&lt;Task&gt; Generate()    {        yield return RunJob(100, 1);        yield return RunJob(100, 2);        yield return RunJob(100, 3);        yield return RunJob(100, 4);    }}[Fact]public async Task WhenAllThrottled_ExceptionPropagated(){    var batch = Generate();    var whenAllTask = batch.WhenAllThrottled(2);    try    { await whenAllTask; }    catch { };    Assert.Equal(TaskStatus.Faulted, whenAllTask.Status);    static IEnumerable&lt;Task&gt; Generate()    {        yield return RunJob(100, 1);        yield return ThrowJob(100);        yield return RunJob(100, 3);        yield return ThrowJob(100);        yield return RunJob(100, 5);    }}[Fact]public async Task WhenAllThrottled_CancellationPropagated(){    var batch = Generate();    var whenAllTask = batch.WhenAllThrottled(2);    try    { await whenAllTask; }    catch { };    Assert.Equal(TaskStatus.Canceled, whenAllTask.Status);    static IEnumerable&lt;Task&gt; Generate()    {        yield return RunJob(100, 1);        yield return CanceledJob(100);        yield return RunJob(100, 3);        yield return CanceledJob(100);    }}"
    },
  
    {
      "id": "75",
      "title": "Awaitable/awaiter pattern and logical micro-threading in C#",
      "url": "/csharp/compcsi/dotnet/2020/10/20/awaitable-pattern.html",
      "date": "October 20, 2020",
      "categories": ["csharp","compcsi","dotnet"],
      "tags": ["csharp","dotnet","async"],
      "shortinfo": "Let's build a state machine based on awaitable/awaiter pattern.",
      "content": "TL;DRWe‚Äôll focus on the extensibility points of the C# compiler to customize the behavior of async methods and task-like types. As result, we will be able to write linear, concise, and composable code with the possibility to have control over task scheduling. ü§ìC# üíú patternsNumerous C# features are based on duck-typing and patterns inferred by the compiler. Basically, it means that a developer doesn‚Äôt need to implement interfaces or inherit classes explicitly to achieve some pluggable behavior. Instead, we could follow some well-known conventions inferred by the compiler to plug or glue some functionality together. Duck-typing is great when strongly-typed approach impedes too many limitations or you want to keep your code clean and tidy. So if you‚Äôve got a pattern right, than, the compiler will do its magic and generate some code for you. Let‚Äôs see examples of such patterns in C#.            Pattern      How to use                  Foreach      Implement GetEnumerator method to make you custom type work with foreach.              LINQ Queries      Implement SelectMany, Select,Where, GroupBy, OrderBy, ThenBy, Join. C# compiler creates a series of method calls. As long as these methods are found, you are good.              Deconstructors      Implement the Deconstruct method with a bunch out parameters. The result of deconstruction could be consumed as ValueTuple or as separately initialized variables.              Dynamic      The C# compiler invokes a whole reflection machinery which by default can invoke methods, properties, and fields. As soon as the object implements IDynamicMetaObjectProvider the object gets runtime control on what is invoked.              Async/Await      The only thing required is a GetAwaiter method which returns an object having an IsComplete and GetResult() method and implementing the INotifyCompletion interface.      ExamplesBefore we delve into the main part, I would like to warm you up and provide some examples of how C# compiler infers patterns in various cases.ForeachUsually, we implement IEnumerable to support collection-like behavior. As result, it is possible to use foreach over user-defined types. But, in order for type to be consumed by foreach you just need to implement GetEnumerator method. Beside that, C# 9 introduces nice little feature. You can iterate over System.Range by implementing extension method. This enables a bunch of new opportunities, e.g.:public static IEnumerator&lt;T&gt; GetEnumerator&lt;T&gt;(this System.Range range) =&gt; enumerator;// Usageforeach (var i in 1..10) {}LINQ QueriesGenerate permutations for a list of words and write LINQ quires over it.See full version on Github: PermutationsSource.cs.// Implementationpublic class PermutationsSource{    //...    public IEnumerable&lt;T&gt; SelectMany&lt;T&gt;(        Func&lt;PermutationsSource, IEnumerable&lt;string&gt;&gt; selector, Func&lt;PermutationsSource, string, T&gt; resultSelector) =&gt;            selector(this).Select(i =&gt; resultSelector(this, i));    public PermutationsSource Select(Func&lt;PermutationsSource, PermutationsSource&gt; selection) =&gt; selection(this);    //...}// Usagevar q = from ps in new PermutationsSource(\"test\", \"test\", \"new\")    from perm in ps.Permute()    where perm.Length &lt; 4    select perm;Debug.Assert(q.Count() == 6);DeconstructorsBesides tuples, you can deconstruct custom types that have Deconstruct method implement as class member or extension method.public class Person{    public string FirstName { get; }    public string LastName { get; }    public int Age { get; }    public Person(string fName, string lName, int age) =&gt;        (FirstName, LastName, Age) = (fName, lName, age);    public void Deconstruct(out string fName, out string lName) =&gt;        (fName, lName) = (FirstName, LastName);    public void Deconstruct(out string fName, out string lName, out int age) =&gt;        (fName, lName, age) = (FirstName, LastName, Age);}// Usagevar p = new Person(\"John\", \"Doe\", 42);(string firstName, _) = p;var (x,_, z) = p;Async/AwaitYou can implement simplistic awaitable/awaiter pattern based on extension methods. Like this:public static TaskAwaiter GetAwaiter(this IEnumerable&lt;Task&gt; tasks) =&gt;    TaskEx.WhenAll(tasks).GetAwaiter();// Usageawait from url in urls select DownloadAsync(url);üöÄ Let‚Äôs see how we can use this in something, that I call logical micro-threading.Problem: Largest series productOccasionally, I use coding platforms to practice my craft. I really like the idea of doing Katas to hone coding skills, because you truly know something when you‚Äôve implemented it and get hands-on experience. Katas is part of the thing. üê±‚Äçüë§  A code kata is an exercise in programming which helps programmers hone their skills through practice and repetition.Let‚Äôs solve this one: exercism.io/tracks/csharp/exercises/largest-series-product.  Given a string of digits, calculate the largest product for a contiguous substring of digits of length n.The problem itself is really straightforward and we could solve it by iterating over the input array and calculating the local maximum every time the window has a length of a given value (span).Here is my first take on it: largest-series-product/solution.var max = 0;var curr = 1;var array = ToDigits(digits);var length = array.Count;int start = 0, end = 0;for (int i = 0; i &lt; length; i++){    end = i;    if (array[i] == 0)    {        start = i + 1;        end = start;        curr = 1;        continue;    }    var full = end - start &gt;= span;    if (full)    {        curr /= array[start];        curr *= array[i];        start++;    }    else        curr *= array[end];    max = curr &gt; max &amp;&amp; (end - start + 1 == span) ? curr : max;}return max;I hate to read code like this ü§Æ. It takes you quite some time and effort to really figure out what is going on. With great confidence, we can describe this code as write-only.  Write-only code code is source code so arcane, complex, or ill-structured that it cannot be reliably modified or even comprehended by anyone with the possible exception of the author.Let‚Äôs formulate how we could tackle the complexity of a relatively simple problems in utterly hyperbolical manner.My goal is to share with you the technique. I deliberately chose a simple problem so we could try to understand the micro-threading approach without delving into details.Micro-threadingC# 5 language made a huge step forward by introducing async/await. Async/await helps to compose tasks and gives the user an ability to use well-known constructs like try/catch, using etc. A regular method has just one entry point and one exit point. But async methods and iterators are different. In the case of async method, a method caller get the (e.g. Task-like type or Task) result immediately and then ‚Äúawait‚Äù the actual result of the method via the resulting task-like type.Async machineryMethods marked by async are undergoing subtle transformations performed by the compiler. To make everything work together, the compiler makes use of:  Generated state machine that acts like a stack frame for an asynchronous method and contains all the logic from the original async method  AsyncTaskMethodBuilder&lt;T&gt; that keeps the completed task (very similar to TaskCompletionSource&lt;T&gt; type) and manages the state transition of the state machine.  TaskAwaiter&lt;T&gt; that wraps a task and schedules continuations of it if needed.  MoveNextRunner that calls IAsyncStateMachine.MoveNextmethod in the correct execution context.The fascinating part is you use extensibility points the C# compiler provides for customizing the behavior of async methods:  Provide your own default async method builder in the System.Runtime.CompilerServices namespace.  Use custom task awaiters.  Define your own task-like types. Starting from C# 7.2.Later, we will see how the last two options could be used in more detail.Let‚Äôs get back to kata problem, shall we?If we take a look from the different angle we can spot different states in which the sliding window can be: ‚Äúreduced‚Äù, ‚Äúexpanded‚Äù, and ‚Äúshifted‚Äù.So, the idea is to move the window from left to right and calculate local product every time the window is ‚Äúexpanded‚Äù or ‚Äúshifted‚Äù.Solution of the largest series product with awaitable/awaiter pattenMy goal is to come up with the solution write highly composable and linear code and at the same time have control of how building blocks are executed and scheduled.The solution consists of next building blocks:$ tree -L 2 liblib‚îú‚îÄ‚îÄ Core‚îÇ   ‚îú‚îÄ‚îÄ AsyncTicker.cs‚îÇ   ‚îú‚îÄ‚îÄ IStrategy.cs‚îÇ   ‚îú‚îÄ‚îÄ StrategyAwaiter.cs‚îÇ   ‚îú‚îÄ‚îÄ StrategyBuilder.cs‚îÇ   ‚îú‚îÄ‚îÄ Strategy.cs‚îÇ   ‚îú‚îÄ‚îÄ StrategyExtensions.cs‚îÇ   ‚îú‚îÄ‚îÄ StrategyResult.cs‚îÇ   ‚îú‚îÄ‚îÄ StrategyStatus.cs‚îÇ   ‚îî‚îÄ‚îÄ StrategyTask.cs‚îú‚îÄ‚îÄ LargestSeriesProductV2.cs‚îú‚îÄ‚îÄ lib.csproj‚îî‚îÄ‚îÄ StrategyImplementations    ‚îú‚îÄ‚îÄ ChaosWindowStrategy.cs    ‚îú‚îÄ‚îÄ ExpandStrategy.cs    ‚îú‚îÄ‚îÄ MultipleSlidingWindowsStrategy.cs    ‚îú‚îÄ‚îÄ ShiftStrategy.cs    ‚îú‚îÄ‚îÄ SkipStrategy.cs    ‚îú‚îÄ‚îÄ SlideStrategy.cs    ‚îú‚îÄ‚îÄ SlidingWindowStrategy.cs    ‚îî‚îÄ‚îÄ Solver.cs2 directories, 19 filesFull source code: /exercism-playground/csharp/largest-series-productCoreIn order to use async/await for task-like types we need to introduce a bunch of extensibility points.A strategy represents a discrete and composable algorithm:public interface IStrategy{    StrategyStatus Status { get; }    IStrategy[] Tick();}public class StrategyResult{    public StrategyStatus Status { get; set; }    public IStrategy[] Strategies { get; set; }}public enum StrategyStatus { InProgress, Done, Failed }We want to make it awaitable and you do it like this await myAwesomeStrategyStrategyExtensions.cs:public static class AwaitableExtensions{    public static StrategyAwaiter GetAwaiter(this IStrategy strategy) =&gt;        new StrategyAwaiter(new[] { strategy });}As mentioned before, C# compiler could do its magic if we implement awaitable/awaiter patter.public class StrategyAwaiter : INotifyCompletion{    public StrategyAwaiter(IStrategy[] strategies)    {        Strategies = strategies;    }    public IStrategy[] Strategies { get; }    public bool IsCompleted { get; private set; }    public bool GetResult() =&gt;        Strategies == null || Strategies.All(x =&gt; x.Status == StrategyStatus.Done);    public void OnCompleted(Action continuation)    {        IsCompleted = true;        continuation();    }}public class StrategyTask{    private readonly IStrategy[] strategies;    public StrategyAwaiter GetAwaiter() =&gt;        return new StrategyAwaiter(strategies);}[AsyncMethodBuilder(typeof(StrategyTaskBuilder&lt;&gt;))]public class StrategyTask&lt;T&gt;{    public IStrategy[] Strategies { get; set; }    public T Result { get; set; }    public bool IsComplete { get; set; }    public Action Continue { get; set; }}public class StrategyTaskBuilder&lt;T&gt;{  // skipped for brevity}That‚Äôs a lot, I know. But, bare with me, I will show how it works:  Driver do the .Tick()  AsyncTicker runs a fresh strategy/task. (run())  Code executes normally until first await  GetAwaiter unwraps task-like type  AsyncMethodBuilder initializes state machine  AsyncTicker runs continuation (continue())  stateMachine.MoveNext() is invoked  If StrategyTask is completed, the result is returned and control flow is resumedI suggest you to spin up the debugger and investigate on your own üòâ.Let‚Äôs say we want to define a top-level strategy that is based on sliding windows giving us a valid span for every type we await a sub-strategy to slide:// SlidingWindowStrategy.csprotected override async StrategyTask&lt;long&gt; Run(){    bool success;    long max = long.MinValue;    Range scan = ..;    var mem = Digits;    do    {        var slide = new SlideStrategy(mem, scan, span);        success = await slide &amp;&amp; slide.Scanned.End.Value != 0;        if (success)        {            var mul = MultiplyDigits(mem[slide.Scanned]);            max = Math.Max(max, mul);        }        scan = slide.Scanned;        mem = mem[scan.Start.Value..];    } while (success);    Result = max;    return max;}A valid slide could look something like this:// SlideStrategy.csprotected override async StrategyTask&lt;bool&gt; Run(){    var skipped = false;    var newSource = Scanned;    if (source.Span[0] == 0)    {        var skip = new SkipStrategy(source);        skipped = await skip;        Scanned = skip.Scanned;        if (skipped &amp;&amp; newSource.End.Value == 0 &amp;&amp; !newSource.End.IsFromEnd)            return false;    }    if (Scanned.End.Value - Scanned.Start.Value != span)    {        var expand = new ExpandStrategy(source[newSource.Start.Value..], span);        var expanded = await expand;        Scanned = expand.Scanned;        return expanded;    }    else    {        var shift = new ShiftStrategy(source, span);        var shifted = await shift;        Scanned = shift.Scanned;        return shifted;    }}And the building blocks:// ExpandStrategy.csprotected override async StrategyTask&lt;bool&gt; Run(){    var end = 0;    var start = 0;    while (end &lt; source.Length &amp;&amp; (end - start) &lt; span)    {        if (source.Span[end] == 0)        {            var skip = new SkipStrategy(source);            var skipped = await skip;            start = skip.Scanned.Start.Value;            if (!skipped)                return false;        }        Scanned = start..(end + 1);        end++;    }    return true;}// ShiftStrategy.csprotected override async StrategyTask&lt;bool&gt; Run(){    if (span &gt;= source.Length)    {        Scanned = default;        return false;    }    if (source.Span[span] == 0)    {        var expand = new ExpandStrategy(source[(span + 1)..], span);        var expanded = await expand;        var shiftedRange = (expand.Scanned.Start.Value + span)..(expand.Scanned.End.Value + span);        Scanned = shiftedRange;        return expanded;    }    Scanned = 1..(span + 1);    return true;}// SkipStrategy.csprotected override async StrategyTask&lt;bool&gt; Run(){    var cnt = 0;    var wasSkipped = false;    while (source.Span[cnt] == 0)    {        wasSkipped = true;        Scanned = ++cnt..;    }    return wasSkipped;}And now, when we have the solution assembled, we can write a ‚ÄúDriver‚Äù code to trigger task machinery. Control flow is returned to the driver every time we await something. This means, that we can change the way strategies are scheduled without modification of the actual algorithm. We may say that scheduling is moved out to a separate layer and adheres Open-Closed Principle (OCP) in this regard.Having this layer we can enhance the solution with:  Interception logic  Fine-grained control over strategy scheduling  Cancellation logic without the necessity to throw an Exceptionpublic class Solver  {    private void SequentialScheduling&lt;T, K&gt;(T strategy) where T : Strategy&lt;K&gt;    {        RecursiveTick(strategy: strategy, indentation: 0);    }    // The simplest approach is to make use of thread pool to introduce concurrency, but if we wanted to    // we would implement a scheduler that has complete supervision over control flow    private void ThreadpoolScheduling&lt;T, K&gt;(params T[] strategies) where T : Strategy&lt;K&gt;    {        var tasks = strategies.Select(t =&gt; Task.Run(() =&gt; RecursiveTick(t, 0)));        Task.WaitAll(tasks.ToArray());    }    public bool RunDriver&lt;T, K&gt;(params T[] strategies) where T : Strategy&lt;K&gt;    {        SequentialScheduling&lt;T, K&gt;(strategies[0]);        return new StrategyTask(strategies).GetAwaiter().GetResult();    }  }If we run we can see the output based on simple interceptor logic://entry pointvar input = \"290010345\";Solution.GetLargestProduct&lt;SlideStrategy&gt;(input, span: 2, Console.Out);// GetLargestProductvar strategy = new SlidingWindowStrategy(input, span);solver.RunDriver&lt;SlidingWindowStrategy, long&gt;(strategy);return strategy.Result;Let‚Äôs parallelize our solution by adding new strategy that will make use of existing building blocks://entry pointvar input = \"290010345\";Solution.GetLargestProduct&lt;MultipleSlidingWindowsStrategy&gt;(input, span: 2, Console.Out);// GetLargestProductvar strategy = new MultipleSlidingWindowsStrategy(input, span);solver.RunDriver&lt;MultipleSlidingWindowsStrategy, long&gt;(strategy);return strategy.Result;// MultipleSlidingWindowsStrategy.csprotected override async StrategyTask&lt;long&gt; Run(){    var mid = Digits.Length / 2;    var strat1 = new SlidingWindowStrategy(Digits[..(mid + span)], 2);    var start2 = new SlidingWindowStrategy(Digits[(mid - span)..], 2);    await WhenAll(strat1, start2);    var res = Math.Max(strat1.Result, start2.Result);    Result = res;    return res;}protected StrategyTask WhenAll(params IStrategy[] strategies){    return new StrategyTask(strategies);}We could take another route and parallelize based on ‚ÄúDriver‚Äù//entry pointvar input = \"290010345\";Solution.GetLargestProduct&lt;ChaosWindowStrategy&gt;(input, span: 2, Console.Out);// GetLargestProductvar mid = source.Length / 2;var strategies = new SlidingWindowStrategy[]{    new SlidingWindowStrategy(source[..(mid + span)], span),    new SlidingWindowStrategy(source[(mid - span)..], span),};solver.RunDriver&lt;ChaosWindowStrategy, long&gt;(strategies);return strategies.Select(s =&gt; s.Result).Max();The beautiful thing is that we don‚Äôt need to change the actual implementation of the algorithm to have control over execution and scheduling. Which you may or may not find useful, but I find the approach interesting.ConclusionThis is a tremendous over-engineering for such a simple problem. But I hope you‚Äôve got this idea and how it could used to build custom async machinery."
    },
  
    {
      "id": "76",
      "title": "Publish images to GitHub Container Registry (ghcr)",
      "url": "/docker/2020/09/19/publish-package-to-ghcr.html",
      "date": "September 19, 2020",
      "categories": ["docker"],
      "tags": ["docker"],
      "shortinfo": "Succinct post on how to get things going in ghcr.",
      "content": "In this blog post, I would like to show you how easy is to publish your docker images to GitHub Container Registry. This topic is relevant because Docker Hub has changed retention limits, so might want to consider other players on the market.TL;DRTo publish an image to ghcr:  Create a Personal Access Token  Log-in to the container registry  Push an image to ghcr.io/GITHUB_USERNAME/IMAGE_NAME:VERSIONTo access GitHub container registry you need to create Personal Access Token (PAT) on GitHub:‚ÄúSettings &gt; Developer Settings &gt; Personal access tokens‚Äù and create token with permissions related to ‚Äúpackages‚Äù (or https://github.com/settings/tokens/new).After that, you can login export CR_PAT=YOUR_TOKEN ; echo $CR_PAT | docker login ghcr.io -u USERNAME --password-stdin.Now, you want to tag your local images:docker tag SOURCE_IMAGE_NAME:VERSION ghcr.io/TARGET_OWNER/TARGET_IMAGE_NAME:VERSIONPush re-tagged imaged to the container registry (ghcr.io):docker push ghcr.io/OWNER/IMAGE_NAME:VERSIONExampleI‚Äôve pushed a containerized image of dotnet-script to GitHub Registry, it allows you to run REPL-style prompt for C#.For now, GitHub doesn‚Äôt provide search and discovery capabilities for images, but you already can find some UI to see image details from UI, if you know the name.You can find packaged dotnet-script here:https://github.com/users/NikiforovAll/packages/container/package/dotnet-script    üöÄ  Let‚Äôs download and run it:docker image pull ghcr.io/nikiforovall/dotnet-script:latestdocker container run --it --rm ghcr.io/nikiforovall/dotnet-script"
    },
  
    {
      "id": "77",
      "title": "Bringing \"surround with\" functionality to vscode. Check out this extension.",
      "url": "/productivity/csharp/vscode/2020/08/21/surround-with-csharp.html",
      "date": "August 21, 2020",
      "categories": ["productivity","csharp","vscode"],
      "tags": ["vscode","productivity"],
      "shortinfo": "Quick overview of surround-with-csharp vscode extension.",
      "content": "Quite often, in my day-to-day workflow, I want to wrap up some code section in a well-known code snippet. I hate these moments. Usually, you want to cut and paste some code, and after that formatting goes wrong and all of that üò±. So‚Ä¶I would like to share with you extension for vscode that I wrote for personal use. (marketplace.visualstudio.com/surround-with-csharp)The goal of this extension is to provide the capability to wrap up selected text in a C# code snippet.This extension supports different concepts to trigger functionally to surround your code.Source code: https://github.com/NikiforovAll/surround-with-csharpCompletionProviderYou can just simply hit CTRL + SPACE and if you have some code selected, you will get completion items that you can choose from by using arrow keys:CommandsIf your favorite command-pallette. You can invoke command ‚ÄúC#: Surround With‚Äù (or CTRL+SHIFT+S, CTRL+SHIFT+S) and you will be prompted for a code snippet.Customize KeybindingsHere is a list of keybindings provided by extension for quicker access to commands to use a particular snippet. You can assign your own keybindings if you want to.            Snippet      Keybinding                  surround.with.namespace      ctrl+shift+S N              surround.with.for      ctrl+shift+S F              surround.with.foreach      ctrl+shift+S ctrl+F              surround.with.do      ctrl+shift+S D              surround.with.while      ctrl+shift+S W              surround.with.if      ctrl+shift+S I              surround.with.else      ctrl+shift+S E              surround.with.try      ctrl+shift+S T              surround.with.tryf      ctrl+shift+S ctrl+T              surround.with.lock      ctrl+shift+S L      Some commands are not bound at all. So if you are ‚ÄúI want to wrap all my code in regions‚Äù kind of guy, you can go to ‚ÄúKeyboard Shortcuts‚Äù and find all subcommands. Usually, the name starts with ‚Äúsurround.with‚Äù.Hope you find this useful, have fun! üéâ"
    },
  
    {
      "id": "78",
      "title": "How to organize learning process with mindmaps, spreadsheets and .NET Core",
      "url": "/productivity/2020/05/03/how-to-organize-your-learning-process-with-xmind.html",
      "date": "May 03, 2020",
      "categories": ["productivity"],
      "tags": ["dotnet","productivity"],
      "shortinfo": "Visualization of current learning progress as mindmap. Solution is based on XMindCsharp.",
      "content": "I prefer reading books from cover to cover and taking notes quite frequently. At the same, I tend to have multiple items on my reading list. It feels frustrating to get back to an awesome book and realize I need to revision it all over again. So I decided to organize it a little bit. I think the best way to organize the learning process is to envision the end goal and have sensible timelines.For the sake of simplicity, I use google spreadsheets as storage and XMind as a visualization tool. XMind is a great tool to visualize and brainstorm some ideas.üí° My take on it  Determine the list of books and materials to learn for the next sprint (e.g. quarter, half of a year)  Set priority and categorize  Add corresponding notes to Evernote for each book, course, etc. It is quite easy to generate internal link in Evernote and access it from spreadsheet directly.  Generate mindmap and play with it so it is possible to estimate and create a plan to take further actions.  Manage progress and perform retrospective when you feel you need it. üîÅBooks for .NET DeveloperHere is the the list of really good books to consider:            Book      Author                      C# in Depth (4th) - 2019      Jon Skeet              C# 8.0 and .NET Core 3.0      Mark J. Price              Programming C# 8.0 - Build Cloud, Web, and Desktop Applications      Ian Griffiths              .NET Core in Action      Dustin Metzgar              Unit Testing Principles, Practices, and Patterns      Vladimir Khorikov              Concurrency in C# Cookbook: Asynchronous, Parallel, and Multithreaded Programming      Stephen Cleary              Under the Hood of .NET Memory Management      Chris Farrell              Designing Data-Intensive Applications      Martin Kleppmann              .NET Microservices: Architecture for Containerized .NET Applications      Cesar de la Torre              Designing Distributed Systems      Brendan Burns              Agile Principles, Patterns, and Practices in C#      Robert C. Martin              The Pragmatic Programmer, From Journeyman To Master      Andrew Hunt      Build mindmaps programmaticallyI‚Äôve created library XMindCSharp for .NET to build mindmaps that you can open with (XMind).You can work with it like this:var book = new XMindConfiguration()                .WithFileWriter(\"./output\", zip: true)                .CreateWorkBook(workbookName: \"test.xmind\");var rootTopic = book.GetPrimarySheet()    .GetRootTopic();rootTopic.SetTitle(\"Scope\");rootTopic.Add(epicTopic);//...Also, I‚Äôve developed simple CLI application edu-scope-to-mindmap to create mindmaps from excel spreadsheet I‚Äôve mentioned above.To use it, run next command from project directory:$ dotnet run --path ./ouput --name test.xmind --source-path input/input.xlsxSometimes it is hard to pick a bite that I can chew on üòÜ.SummaryI‚Äôve introduced lightweight approach to organize your learning process. Personally, I find it useful because learning is essential part of my craft and you better do it well üòâ.If you want to organize your learning process the way I do, please feel free to use: template.xlsx + edu-scope-to-mindmap"
    },
  
    {
      "id": "79",
      "title": "Catch up with .NET Core - System.Text.JSON",
      "url": "/dotnet-core/2020/04/27/catch-dotnetcore-json.html",
      "date": "April 27, 2020",
      "categories": ["dotnet-core"],
      "tags": ["dotnet","json"],
      "shortinfo": "Introduction to System.Text.Json.",
      "content": "TL;DRIntroduction to System.Text.Json. Basically, the article is a projection of documentation from microsoft docs, you might want to skip this one üòõ. It was created due to the habit of learning new stuff that I practice.For a very long time .NET Framework didn‚Äôt have a unanimous and centralized built-in API to work with JSON. Also, the way we work with JSON has shaped and evolved over time. So .NET team has added brand-new NuGet package System.Text.Json with support of serialization of object to JSON text, deserialization to JSON text to objects, reading/writing JSON text encoded as UTF-8 and creation of in-memory JSON (DOM) objects. By default, strings .NET are represented by UTF-16 encoding. System.Text.Json avoids unnecessary transcoding by making use of ref struct and working with UTF-8 in place.Also, System.Text.Json eliminates dependency on 3rd party library for ASP .NET Core. Application developers still could easily use Json.NET, but this time there is not dependency between underlying platform and application code.Examples. JSON ‚Üî POCOExcessive description how to serialize could be found at:  https://docs.microsoft.com/en-us/dotnet/standard/serialization/system-text-json-how-to#serialization-behavior  https://docs.microsoft.com/en-us/dotnet/standard/serialization/system-text-json-how-to#deserialization-behavior.private static void SerializePrettyPrint(){    var payload = new DataPayload()    {        Id = 1,        Type = Type.Root,        Descendants = new DataPayload[]{            new DataPayload(){Id = 2, Type = Type.Standard}        }    };    var options = new JsonSerializerOptions    {        WriteIndented = true,        AllowTrailingCommas = true,        PropertyNamingPolicy = JsonNamingPolicy.CamelCase,        IgnoreNullValues = true,        Converters = { new JsonStringEnumConverter(JsonNamingPolicy.CamelCase) }    };    string payloadAsText = JsonSerializer.Serialize(payload, options);    Trace.TraceInformation(payloadAsText);    var payload2 = JsonSerializer.Deserialize&lt;DataPayload&gt;(payloadAsText, options);    Debug.Assert(payload2.Id == payload.Id, \"same id\");    Debug.Assert(payload2.Type == Type.Root, \"same id\");}Examples. Utf8JsonReader, Utf8JsonWriter  https://docs.microsoft.com/en-us/dotnet/standard/serialization/system-text-json-how-to#use-utf8jsonreader  https://docs.microsoft.com/en-us/dotnet/standard/serialization/system-text-json-how-to#use-utf8jsonwriter  https://docs.microsoft.com/en-us/dotnet/standard/serialization/system-text-json-how-to#filter-data-using-utf8jsonreaderExamples. JsonDocument  https://docs.microsoft.com/en-us/dotnet/standard/serialization/system-text-json-how-to#use-jsondocument-for-access-to-data  https://docs.microsoft.com/en-us/dotnet/standard/serialization/system-text-json-how-to#use-jsondocument-to-write-jsonSee full code at GitHub: NikiforovAll/TextJsonDemo"
    },
  
    {
      "id": "80",
      "title": "Catch up with .NET Core - System.IO.Pipelines",
      "url": "/dotnet-core/2020/04/24/catch-dotnetcore-piepelines.html",
      "date": "April 24, 2020",
      "categories": ["dotnet-core"],
      "tags": ["dotnet","pipelines"],
      "shortinfo": "Introduction to System.IO.Pipelines.",
      "content": "TL;DRIntroduction to System.IO.Pipelines. See Reference section for stuff you might find useful to delve in.System.IO.Pipelines is a library that is designed to make it easier to do high performance IO shipped in .NET Standard 2.1 as a first class BCL API.Provides programming model to perform high performance parsing of streaming data. Developers are free from having to manage buffers. A Pipeline is like a Stream that pushes data to you. Networking is the primary use case for this API, the whole thing emerged from Kestrel as implementation detail.There are no explicit buffers allocated. All buffer management is delegated to the PipeReader and PipeWriter implementations. Delegating buffer management makes it easier for consuming code to focus solely on the business logic.Example from docs.microsoft: https://docs.microsoft.com/en-us/dotnet/standard/io/pipelines#pipe-basic-usageExample: reading of buffered integersOn the high level it looks something like snippet below. FillPipeAsync is data source that uses writer object to manage buffer.static async Task Main(string[] args){    var pipe = new Pipe(new PipeOptions(pauseWriterThreshold: 12, resumeWriterThreshold: 9));    Task writing = FillPipeAsync(5, pipe.Writer);    Task reading = ReadPipeAsync(pipe.Reader);    await Task.WhenAll(reading, writing);}There are no explicit buffers allocated. All buffer management is delegated to the PipeReader and PipeWriter implementations. Delegating buffer management makes it easier for consuming code to focus solely on the business logic.private static async Task FillPipeAsync(    int iterations, PipeWriter writer, CancellationToken token = default){    const int minimumBufferSize = 4;    var random = new Random();    for (int i = 0; i &lt; iterations; i++)    {        Memory&lt;byte&gt; memory = writer.GetMemory(minimumBufferSize);        var numberToWrite = random.Next(int.MaxValue / 2, int.MaxValue);        Console.WriteLine(\"Writing...\");        BitConverter.TryWriteBytes(memory.Span, numberToWrite);        writer.Advance(minimumBufferSize); // Tell writer how much data were written        FlushResult result = await writer.FlushAsync(); // Make the data available to the PipeReader.    }    await writer.CompleteAsync();}private static async IAsyncEnumerable&lt;ReadOnlySequence&lt;byte&gt;&gt; GetReaderResult(    PipeReader reader, [EnumeratorCancellation] CancellationToken token = default){    while (true &amp;&amp; !token.IsCancellationRequested)    {        ReadResult result = await reader.ReadAsync(); // Await reading        ReadOnlySequence&lt;byte&gt; buffer = result.Buffer;        if (buffer.Length &lt; 4)            yield break;        var position = buffer.GetPosition(sizeof(int));        Console.WriteLine(\"Reading...\");        yield return buffer.Slice(0, position);        buffer = buffer.Slice(position);        reader.AdvanceTo(buffer.Start, position); // Tell the PipeReader how much of the buffer has been consumed.        if (result.IsCompleted) // Stop reading if there's no more data coming.            break;    }    await reader.CompleteAsync(); // Mark the PipeReader as complete.}private static async Task ReadPipeAsync(PipeReader reader){    await foreach (var bytesReceived in GetReaderResult(reader))    {        foreach (var i in bytesReceived.ToArray&lt;byte&gt;())        {            Console.Write($\"{i:x3}.\");        }        Console.WriteLine(\"!\");    }}Output:$ dotnet runWriting...Writing...Writing...Reading...00c.04d.04d.078.!Reading...Writing...0eb.0d2.0f6.05f.!Reading...Writing...06e.0ca.029.050.!Reading...085.0d0.0bd.043.!Reading...01d.074.08c.07b.!See full example at GitHub: blog-examples/pipelinesStreams and pipesSystem.IO.Pipelines was designed to make writing of high performance parsing of streaming data. The vast majority of API working with IO are written based on System.IO.Stream. To simplify this, BCL provides out of the box ways to converts between this types.Stream ‚Üí PipeWhen working with a stream it is possible to create wrappers/adapters around Stream. UsePipeWriter.Create / PipeReader.CreateExample: https://github.com/davidfowl/TcpEchoPipe ‚Üí StreamWhen reading or writing stream data, you typically read data using a de-serializer and write data using a serializer. Most of these read and write stream APIs have a Stream parameter. To make it easier PipeReader and PipeWriter expose an AsStream. AsStream returns a Stream implementation around the PipeReader or PipeWriter."
    },
  
    {
      "id": "81",
      "title": "Sorting algorithms with C# 8, Span{T} and Try .NET",
      "url": "/algorithms/2020/04/15/dotnet-try-and-sorting-algorithms.html",
      "date": "April 15, 2020",
      "categories": ["algorithms"],
      "tags": ["dotnet","algorithms"],
      "shortinfo": "I would like to share with you something fun. The right way to write interactive documentation - Try .NET. It is an awesome project maintained by Microsoft. It allows you...",
      "content": "I would like to share with you something fun. The right way to write interactive documentation - Try .NET. It is an awesome project maintained by Microsoft. It allows you to embed C# code inside your markdown documentation and run it locally. The absolute best way to learn it is to use it. So I encourage you to fork https://github.com/NikiforovAll/intro-to-algorithms and play with it.All you need to do is to navigate to root directory and run:dotnet tool install -g dotnet-trydotnet tryOn the demo below you can find implementation of standard sorting algorithms with C# 8 and Span&lt;T&gt;.Non-interactive version: https://github.com/NikiforovAll/intro-to-algorithms/blob/master/docs/Sorting.md"
    },
  
    {
      "id": "82",
      "title": "Development environment based on Windows Terminal + WSL + ZSH + dotfiles",
      "url": "/productivity/2019/11/30/nikiforovall-setup.html",
      "date": "November 30, 2019",
      "categories": ["productivity"],
      "tags": ["windows-terminal","dotfiles","wsl","deaac"],
      "shortinfo": "I would like to share with you approach on how to set up your working environment lightning fast. This post is based on my dotfiles repository - nikiforovall/dotifles",
      "content": "IntroductionI would like to share with you how to implement dotfiles concept for your working environment lightning fast. This post is based on my dotfiles repository - nikiforovall/dotifles.It is a good time to be a .NET developer. It is not only mature and enterprise-ready ecosystem but vibrant and active community. I feel like guys from Microsoft doing a lot to support and encourage .NET community. In particular, that is why we have something like Windows Subsystem For Linux, Windows Terminal and many more.So I use Windows Terminal. It is a modern terminal emulator for windows. I switched from a really nice terminal emulator cmder to Windows Terminal (Preview). And I don‚Äôt regret it, especially after the release of Windows Terminal Preview v0.7 Release. This release includes PowerLine fonts for Cascadia Fonts, tab reordering splitting of tab windows into panes. Sweat!Windows Terminal is highly configurable, so you could adjust keybindings, color schemes, and profiles.You can install Windows Terminal from Microsoft Store or by using chocolatey windows package manager.choco install microsoft-windows-terminal -yAlso, see my Windows Terminal profiles.json https://github.com/NikiforovAll/dotfiles/tree/master/artifacts/wt_profile.json.I use Visual Studio Code as the main editor and development environment. Although, it is possible to use dotfiles for synchronization concept for vscode I encourage you to use Settings Sync extension. It uses GitHub as storage and makes use of API key to authorize and manage your configurations. You could install it like this:choco install vscode -ycode --intall-extension Shan.code-settings-syncAnd now, here where the fun begins. The end goal is to bootstrap and configure working environment with the minimum effort. Basically, we want to run bash one-liner and have everything arranged nice and clean. Something like:bash -c \"$(wget -qO - https://raw.github.com/nikiforovall/dotfiles/master/src/wsl/os/install.sh)\"WSL SetupBut first, let‚Äôs see how we set up brand new WSL instance. Here I list available Linux distros and unregister ubuntu-18.04Let‚Äôs register new user in Ubuntu-18.04 LTS.And set default distro ‚ÄúUbuntu-18.04‚Äù# $wsl -h# -setdefault, -s &lt;DistributionName&gt;wsl -s \"Ubuntu-18.04\"We can login to WSL via Windows Terminal by specifying \"source\": \"Windows.Terminal.Wsl\" in corresponding Windows Terminal profile.Installationüîß If it is a brand-new WSL instance you might want to run apt upgrade. In the demo below I‚Äôve already had everything.Let‚Äôs see how the bash one-liner works:bash -c \"$(wget -qO - https://raw.github.com/nikiforovall/dotfiles/master/src/wsl/os/install.sh)\"It installs a bunch of essentials for development environment: utils, tools, and dotnet sdk. Also, it symlinks git and shell configs. So after reloading your shell, it is familiar and responsive working environment. The goal of dotfiles technique is to provide the way to manage various .* configuration files. The basic idea is to have git repository responsible for installing and managing configurations on a working machine so you don‚Äôt need to worry about losing favorite aliases, programs, and configurations. Perfect! üöÄDemoAt the demonstration below I create a folder for the new dotnet console project. I slightly modify Program.cs and run the application inside WSL. There are a lot of details that make the process of development easier and promote productivity. I just mention a few of them: simplicity of setup, syntax and commands highlighting and autocompletion.SummaryIt was a brief overview of my dotfiles setup. I encourage you to investigate the internals of my dotfiles repository for mode details. Feel free to fork and work on dream setup of your own. Let me know what you think about it!"
    },
  
    {
      "id": "83",
      "title": "Design Patterns. Behavioral Patterns. Part I",
      "url": "/design-patterns/2019/06/09/design-patterns-first-part.html",
      "date": "June 09, 2019",
      "categories": ["design-patterns"],
      "tags": ["dotnet","csharp","design-patterns"],
      "shortinfo": "Overview of behavioral patterns with Try.NET",
      "content": "Yes, you might say that blogging about design patterns is boring. But what about interactive version with live examples and  Try .NET? Let‚Äôs delve into it. TOC   Strategy  Template Method  Visitor  State  Mediator  Observer  Memento  Iterator  Interpreter  Command  ChainOfResponsibility  Wrapping upStrategyDefines a family of algorithms, encapsulate each one, and make them interchangeable. Strategy lets the algorithm vary independently from clients that use it. Strategy adapts behavior at runtime to requirements that are not known in advance.In general case Pattern Strategy doesn‚Äôt restrict the number of operations in defined interface. So it could be one operation (Sort in ISortable) or family of operations (Encode/Decode in IMessageProcessor). In case when it is one operation, it is possible to use .NET specific implementation that relies on delegate Funct&lt;T1, T2,‚Ä¶&gt; or Action&lt;T1, T2,‚Ä¶&gt;.Use case  When you want to encapsulate some behavior or part of algorithm.  When you want to change behavior of runtime.Tradeoff: Flexibility vs complexityFrequency of use: HighDiagramExamples in .NET  Many extension methods in LINQ accepts strategy methods. For example IComparer&lt;T&gt; or IEqualityComparer&lt;T&gt;.  WCF contains a lot of examples of strategy pattern: IErrorHandler, IDispatchMessageFormatter, MessageFilter.Source code: StrategyExample: Try .NETpublic class Program{    public static void Main()    {            ILogReader logReader;            // log storage number one            logReader = new LogFileReader();            List&lt;LogEntry&gt; logEntryList1 = logReader.Read();            Assert.Equal(2, logEntryList1.Count);            Assert.Contains(\"LogFileReader\", logEntryList1[0].Message);            // log storage number two            logReader = new WindowsEventLogReader();            List&lt;LogEntry&gt; logEntryList2 = logReader.Read();            Assert.Single(logEntryList2);            Assert.Contains(\"WindowsEventLogReader\", logEntryList2[0].Message);            TestRunner.Print();    }}Template MethodDefines the skeleton of an algorithm in an operation, deferring some steps to subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm‚Äôs structure. In some cases this pattern is better when you have plenty of functionality to reuse or you have complicated hierarchy of behaviors to implement so you could use sliding version of template method.It is possible to use .NET specific implementation that relies on extension methods and namespaces. So template method is defined by public extension methods, implementation is determined by compile-time.Use case  When you want to encapsulate some behavior or part of algorithm.  When you want to define strict contract for developers to reuse some functionality by using inheritance over composition.Frequency of use: MediumDiagramExamples in .NET  WCF contains a lot of examples of template method pattern: CommunicationObject, ServiceHostBase, MessageHeader, ChannelBase.  System.Runtime.InteropServices.SafeHandle represents a wrapper class for operating system handles.Source code: Template Method.Example: Try .NETpublic abstract class AbstractClass{    // TemplateMethod    public int Process(IEnumerable&lt;int&gt; items)    {        return items            .Where&lt;int&gt;(this.Filter)            .Select(this.ProcessUnit)            .Aggregate((item, acc) =&gt;                {                    acc += item;                    return acc;                });    }    protected abstract bool Filter(int item);    protected abstract int ProcessUnit(int i);}public class Program{    public static void Main()    {        var items = new [] { 1, 2, 3, 4, 5 };        // define behavior via subclassing        AbstractClass processor = new ConcreteProcessor1();        int result1 = processor.Process(items);        Assert.Equal(24, result1);        processor = new ConcreteProcessor2();        int result2 = processor.Process(items);        Assert.Equal(15, result2);        TestRunner.Print();    }}VisitorRepresent an operation to be performed on the elements of an object structure. Visitor lets you define a new operation without changing the classes of the elements on which it operates.Use case  When you want to split data structures and commands. Visitor pattern encourages to adhere OCP (S_O_LID).  When you want to be able to add operations to stable hierarchy more easily. In OOP world it is easier to arrange object into hierarchy rather than adding new operation for existing hierarchy of objects. Adding new operation is considered as risky move since it is very likely to design fragile solution, Visitor allows to tackle this problem.Frequency of use: LowDiagramExamples in .NET  ExpressionTreeVisitor for working with Expression Trees in .NET.  DbExpressionVisitor in EntityFramework in .NET.Source code: VisitorExample: Try .NET// Visitor implpublic class LogEntryVisitor: ILogEntryVisitor{    public string State { get; set; }    public void ProcessLogEntry(LogEntry logEntry)    {        logEntry.Accept(this);    }    void ILogEntryVisitor.Visit(ExceptionLogEntry exceptionLogEntry)    {        State = $\"LogEntryVisitor[{nameof(exceptionLogEntry)}]\";    }    void ILogEntryVisitor.Visit(SimpleLogEntry simpleLogEntry)    {        State = $\"LogEntryVisitor[{nameof(simpleLogEntry)}]\";    }}// Programpublic class Program {    public static void Main () {        var baseEntry = new SimpleLogEntry () {            Created = DateTime.Now,            Message = \"test\"        };        var simpleEntry = new SimpleLogEntry (baseEntry) {            AdditionalInfo = \"additional\"        };        var exceptionEntry = new ExceptionLogEntry (baseEntry) {            ExceptionCode = 111        };        //it is possible to implement multiple visitors for particular hierarchy of objects        // for example: PersistentLogEntryVisitors that allows to process LogEntries and save them to storage        var visitor = new LogEntryVisitor ();        visitor.ProcessLogEntry (simpleEntry);        Assert.Contains (\"simple\", visitor.State);        visitor.ProcessLogEntry (exceptionEntry);        Assert.Contains (\"exception\", visitor.State);        TestRunner.Print ();    }}StateAllow an object to alter its behavior when its internal state changes. StateClient aggregates state internally and acts in according with current State.Use case  When you have a state machine to implement and you feel like the behavior will be changed at some point, so you want to have distinct point of extension.Frequency of use: MediumDiagramExample in .NET  CommunicationObject in WCF provides a common base implementation for the basic state machine common to all communication-oriented objects.  Task implements state machine to move between statuses: Created, WaitingForActivation, WaitingToRun, Running, RunToCompletion, Canceled, Faulted.Source code: State.Example: Try .NET (+ INotifyPropertyChanged to inspect how state changes intrinsically.)public class StateOne : BaseState {    public StateOne (StateClient client) {        Client = client;    }    protected override void HandleContext () {        //in general, state is changed based on rules defined within concrete implementation of state (same for StateTwo)        if (base.IsNeedToChangeState ()) {            Client.State = new StateTwo (this);        }    }}public abstract class BaseState {    // ...    public int Value { get; protected set; }    public StateClient Client { get; set; }    protected int Threshold { get; set; } = 1;    public virtual bool IsNeedToChangeState () {        return Value &gt;= Threshold;    }    // ...}// Programpublic class Program {    public static void Main () {        var ctx = new StateClient ();        ctx.State = new StateOne (ctx);        Assert.Equal (\"StateOne\", ctx.State.GetType ().Name);        //Threshold is reached, lets change to StateTwo        ctx.Process ();        Assert.Equal (\"StateTwo\", ctx.State.GetType ().Name);        ctx.Process ();        ctx.Process ();        //Threshold is reached, lets change to StateTwo.        //Once again, rules could be defined within state (e.g. different value for threshold or something like that)        Assert.Equal (\"StateOne\", ctx.State.GetType ().Name);        TestRunner.Print ();    }}MediatorDefine an object that encapsulates how a set of objects interact. Mediator promotes loose coupling by keeping objects from referring to each other explicitly, and it lets you vary their interaction independently.Mediator could be used at different level of application. It could be class as mediator, component as mediator, application-layer as mediator. For example, components could be connected in Composition Root by applying Mediator pattern.Use case  When you want to describe interaction between autonomous components in loose-coupled way. Implicit mediator vs explicit mediator.Frequency of use: MediumDiagramExamples in .NET  EventAggregator class in WPF.  In MVC, Controller is actually a mediator.Source code: Mediator.Example: Try .NETpublic class Chatroom : AbstractChatroom {    private Dictionary&lt;string, User&gt; _users = new Dictionary&lt;string, User&gt; ();    public override void Register (User user) {        if (!_users.ContainsValue (user)) {            _users[user.Name] = user;        }        user.Chatroom = this;    }    public override void Send (string from, string to, string message) {        User participant = _users[to];        if (participant != null) {            participant.Receive (from, message);        } else {            throw new KeyNotFoundException (\"User not found\");        }    }}ObserverDefine a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically. Implementation could be based on push model, when clients is notified with all data needed or pull model, when clients are just notified and request data on demand.It is hard to find classic interface based implementation of Observer in .NET because of presence of delegates (EventHandler&lt;T&gt;), events, IObserver/IObserverable and Reactive programming libs.Use case  Consider Observer when you have one-to-many dependency and you want to facilitate lose-coupling.  When there is a complicated communication between collection of objects you could implement Mediator based on Observer.Frequency of use: HighDiagramExamples in .NET  IObserver/IObserverable in .NET.  AppDomainSetup.AppDomainInitializer, HttpConfiguration.Initializer as a point of extension of the app during initialization.Source code: Observer.Example: Try .NETpublic class Program{    public static void Main()    {            var subject = new ConcreteSubject();            var observers = new[] { new ConcreteObserver(), new ConcreteObserver(), new ConcreteObserver() };            foreach (var observer in observers)            {                subject.Attach(observer);            }            subject.SubjectState = \"init\";            subject.Notify();            Assert.All(observers, (el =&gt; \"init\".Equals(el.State)));            subject.SubjectState = \"changed\";            subject.Notify();            Assert.All(observers, (el =&gt; \"changed\".Equals(el.State)));            TestRunner.Print();    }}MementoWithout violating encapsulation, capture and externalize an object‚Äôs internal state so that the object can be restored to this state later.Use case  When you have distinct list of parameters that you want to manage and externalize for clients of your code.Frequency of use: Low.DiagramSource code: Memento.Example: Try .NETpublic class Program {    public static void Main () {        (string, int) [] states = {            (\"1\", 0),            (\"2\", 10),            (\"3\", 20),        };        Originator originator = new Originator ();        var caretaker = new Caretaker ();        originator.State = states[0];        caretaker.Push (originator.CreateMemento ());        originator.State = states[1];        caretaker.Push (originator.CreateMemento ());        originator.State = states[2];        originator.SetMemento (caretaker.Pop ());        Assert.Equal (states[1], originator.State);        originator.SetMemento (caretaker.Pop ());        Assert.Equal (states[0], originator.State);        Assert.Empty (caretaker);    }}IteratorProvide a way to access the elements of an aggregate object sequentially without exposing its underlying representation.Iterators in .NET are unidirectional and lazy, every time you invoke GetEnumerator() new instance is returned. Also, iterators are mutable by default, so it is better to use struct iterators and save yourself from potential issues.Use case  Use iterator when you want to provide uniform way to perform custom and stateful iteration of a collection of objects.Frequency of use: HighDiagramExamples in .NET  IEnumerable/IEnumerator - collections in .NET.  Any object that has GetEnumerator method could play a role of Iterator in .NET.  yield return construction is used to implement generators in .NET.Source code: Iterator.Example: Try .NETpublic class Program {    public static void Main () {        var collection = new CustomCollection&lt;int&gt; { 1, 2, 3, 4, 5 };        var iterator = collection.CreateIterator ();        Assert.Equal (new int[] { 1, 3, 5 }, iterator.getCollectionFromIterator ().ToList ());        TestRunner.Print ();    }}// part of iterator implementation//Step = 2;public override T Next () {    T result = default (T);    if (!IsDone ()) {        result = _collection[_currentIndex];        _currentIndex += Step;    }    return result;}InterpreterGiven a language, define a representation for its grammar along with an interpreter that uses the representation to interpret sentences in the language.Use case:  When you work with syntax trees that you want to evaluate in some way.Frequency of use: LowDiagramSource code: Interpreter.Example: Try .NET//Programpublic class Program{    public static void Main()    {        var context = new Context();        context.TerminalExpressionArgs = new TerminalExpression[] {            new TerminalExpression(1),            new TerminalExpression(2)        };        AbstractInterpreter[] interpreters = new AbstractInterpreter[]{            new TerminalExpression(3),            new NonTerminalExpression(){BinaryOperation = (a, b)=&gt; a * b }        };        // terminal expression is multiplied based on default value from context        // 3 * 2        Assert.Equal(6, interpreters[0].Interpret(context));        // (1 * 2) * (2 * 2)        Assert.Equal(8, interpreters[1].Interpret(context));        TestRunner.Print();    }}CommandEncapsulate a request as an object, thereby letting you parameterize clients with different requests, queue or log requests, and support undoable operations.Use case  When you want to encapsulate operation and detach it from originator.  When you want to define granular contract between components and encourage ISP.Frequency of use: Medium HighDiagramExamples in .NET  ICommand in WPF.  IDbCommand in ADO.NET.  Task&lt;T&gt; accepts Func&lt;T&gt;  so it could be interpreted as command.Source code: Command.Example: Try .NETpublic class Program {    public static void Main () {        var operations = new List &lt; (char @operator, int operand) &gt; {            ('+', 10),            ('-', 5),            ('*', 3)        };        CalculatorClient client = new CalculatorClient ();        foreach (var operation in operations) {            client.Compute (operation.@operator, operation.operand);        }        Assert.Equal (15, client.Result);        client.Undo ();        Assert.Equal (5, client.Result);    }}ChainOfResponsibilityAvoid coupling between the sender of a request to its receiver by giving more than one object a chance to handle the request. Chain the receiving objects and pass the request along the chain until an object handles it.Use case  When you have generalized request/event/operation to be processed by interested parties not know in advance. It is possible to change processing behavior at runtime.Frequency of use: MediumDiagramExamples in .NET  Event Closing in WinForms, so it possible to prevent form from closing by setting Cancel  argument in CancelEventArgs.Source code: ChainOfResponsibility.Example: Try .NET// Example of processor.public class Director : Approver{    public Director(string id) :base (id)    {    }    public override (bool, ProcessingLevel) ProcessRequest(Purchase purchase) =&gt;        purchase.Amount &lt; 10 ? (true, ProcessingLevel.Accepted) : successor.ProcessRequest(purchase);}// Programpublic static void Main () {    List&lt;Approver&gt; approvers = new List&lt;Approver&gt; {        new Director (\"#1\"),        new VicePresident (\"#2\"),        new President (\"#3\")    };    ChainOfResponsibilityManager manager = new ChainOfResponsibilityManager (        new Dictionary&lt;string, string&gt; {            [\"#1\"] = \"#2\",            [\"#2\"] = \"#3\"        }    );    var baseApprover = manager.CreateChainOfApprovers (approvers);    Purchase purchase1 = (Purchase) (5.0, 1, \"p1\");    Purchase purchase2 = (Purchase) (11.0, 1, \"p1\");    Purchase purchase3 = (Purchase) (16.0, 1, \"p1\");    Purchase purchase4 = (Purchase) (21.0, 1, \"p1\");    // process is initiated by first node, conditions is not met so it stops.    // Consider to externalize ChainOfResponsibility to client interface and use node registry to build chain.    // In this case chain execution started from first node.    var result1 = approvers[0].ProcessRequest (purchase1);    Assert.Equal (ProcessingLevel.Accepted, result1.Item2);    // process is initiated by first node. Control is passed to successor.    var result2 = approvers[0].ProcessRequest (purchase2);    Assert.Equal (ProcessingLevel.EscalationLevelOne, result2.Item2);    // process is initiated by first node. Control is passed all the way to last node and processed successfully.    var result3 = approvers[0].ProcessRequest (purchase3);    Assert.Equal (ProcessingLevel.EscalationLevelTwo, result3.Item2);    // end of ChainOfResponsibility    var result4 = approvers[0].ProcessRequest (purchase4);    Assert.Equal (false, result4.Item1);    Assert.Equal (ProcessingLevel.Rejected, result4.Item2);    TestRunner.Print ();}Wrapping upAnd that‚Äôs it. Let me know what you think and feel free to contribute to https://github.com/NikiforovAll/design-patterns-playground."
    }
  
]
